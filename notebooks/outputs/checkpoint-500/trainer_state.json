{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 46.51162790697674,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09,
      "grad_norm": 0.2204626500606537,
      "learning_rate": 0.002,
      "loss": 0.6248,
      "step": 1
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.11693359166383743,
      "learning_rate": 0.001997997997997998,
      "loss": 0.6299,
      "step": 2
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.38200682401657104,
      "learning_rate": 0.001995995995995996,
      "loss": 0.596,
      "step": 3
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.9068372249603271,
      "learning_rate": 0.0019939939939939938,
      "loss": 0.7228,
      "step": 4
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.4440826177597046,
      "learning_rate": 0.001991991991991992,
      "loss": 0.6225,
      "step": 5
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.29924121499061584,
      "learning_rate": 0.00198998998998999,
      "loss": 0.5211,
      "step": 6
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.5933983325958252,
      "learning_rate": 0.001987987987987988,
      "loss": 0.5388,
      "step": 7
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.2977130115032196,
      "learning_rate": 0.001985985985985986,
      "loss": 0.5192,
      "step": 8
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.4721057713031769,
      "learning_rate": 0.001983983983983984,
      "loss": 0.5086,
      "step": 9
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.57562255859375,
      "learning_rate": 0.001981981981981982,
      "loss": 0.5412,
      "step": 10
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.5603854656219482,
      "learning_rate": 0.00197997997997998,
      "loss": 0.4778,
      "step": 11
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.3974635601043701,
      "learning_rate": 0.0019779779779779782,
      "loss": 0.4624,
      "step": 12
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.45064789056777954,
      "learning_rate": 0.0019759759759759763,
      "loss": 0.4669,
      "step": 13
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.6731711626052856,
      "learning_rate": 0.001973973973973974,
      "loss": 0.4701,
      "step": 14
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.18675543367862701,
      "learning_rate": 0.001971971971971972,
      "loss": 0.4395,
      "step": 15
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.16011585295200348,
      "learning_rate": 0.00196996996996997,
      "loss": 0.416,
      "step": 16
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.10524789243936539,
      "learning_rate": 0.001967967967967968,
      "loss": 0.4102,
      "step": 17
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.20033268630504608,
      "learning_rate": 0.001965965965965966,
      "loss": 0.4121,
      "step": 18
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.11050759255886078,
      "learning_rate": 0.0019639639639639638,
      "loss": 0.4064,
      "step": 19
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.13043314218521118,
      "learning_rate": 0.001961961961961962,
      "loss": 0.3862,
      "step": 20
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.14499731361865997,
      "learning_rate": 0.00195995995995996,
      "loss": 0.4014,
      "step": 21
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.1966066062450409,
      "learning_rate": 0.001957957957957958,
      "loss": 0.3537,
      "step": 22
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.21345072984695435,
      "learning_rate": 0.001955955955955956,
      "loss": 0.3725,
      "step": 23
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.3094753324985504,
      "learning_rate": 0.001953953953953954,
      "loss": 0.3845,
      "step": 24
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.31922733783721924,
      "learning_rate": 0.001951951951951952,
      "loss": 0.376,
      "step": 25
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.40778297185897827,
      "learning_rate": 0.00194994994994995,
      "loss": 0.3701,
      "step": 26
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.19390186667442322,
      "learning_rate": 0.001947947947947948,
      "loss": 0.327,
      "step": 27
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.24013637006282806,
      "learning_rate": 0.001945945945945946,
      "loss": 0.3183,
      "step": 28
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.16304755210876465,
      "learning_rate": 0.001943943943943944,
      "loss": 0.329,
      "step": 29
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.16777722537517548,
      "learning_rate": 0.001941941941941942,
      "loss": 0.338,
      "step": 30
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.17272473871707916,
      "learning_rate": 0.00193993993993994,
      "loss": 0.3246,
      "step": 31
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.21011485159397125,
      "learning_rate": 0.001937937937937938,
      "loss": 0.3276,
      "step": 32
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.18070094287395477,
      "learning_rate": 0.001935935935935936,
      "loss": 0.2986,
      "step": 33
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.1892654299736023,
      "learning_rate": 0.001933933933933934,
      "loss": 0.2888,
      "step": 34
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.14621806144714355,
      "learning_rate": 0.001931931931931932,
      "loss": 0.2671,
      "step": 35
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.1926691234111786,
      "learning_rate": 0.00192992992992993,
      "loss": 0.286,
      "step": 36
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.1583983302116394,
      "learning_rate": 0.0019279279279279281,
      "loss": 0.2561,
      "step": 37
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.22915111482143402,
      "learning_rate": 0.0019259259259259258,
      "loss": 0.2626,
      "step": 38
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.17812588810920715,
      "learning_rate": 0.0019239239239239238,
      "loss": 0.2524,
      "step": 39
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.13349206745624542,
      "learning_rate": 0.0019219219219219219,
      "loss": 0.2265,
      "step": 40
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.2258288711309433,
      "learning_rate": 0.00191991991991992,
      "loss": 0.2535,
      "step": 41
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.173696830868721,
      "learning_rate": 0.001917917917917918,
      "loss": 0.2423,
      "step": 42
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.23605255782604218,
      "learning_rate": 0.0019159159159159158,
      "loss": 0.2315,
      "step": 43
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.20791642367839813,
      "learning_rate": 0.0019139139139139139,
      "loss": 0.1886,
      "step": 44
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.23653602600097656,
      "learning_rate": 0.001911911911911912,
      "loss": 0.1805,
      "step": 45
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.2770611643791199,
      "learning_rate": 0.00190990990990991,
      "loss": 0.1898,
      "step": 46
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.26397013664245605,
      "learning_rate": 0.001907907907907908,
      "loss": 0.1717,
      "step": 47
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.26032182574272156,
      "learning_rate": 0.001905905905905906,
      "loss": 0.1726,
      "step": 48
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.20846165716648102,
      "learning_rate": 0.001903903903903904,
      "loss": 0.1479,
      "step": 49
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.24298708140850067,
      "learning_rate": 0.001901901901901902,
      "loss": 0.1616,
      "step": 50
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.2579248249530792,
      "learning_rate": 0.0018998998998999,
      "loss": 0.1445,
      "step": 51
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.19726793467998505,
      "learning_rate": 0.001897897897897898,
      "loss": 0.1425,
      "step": 52
    },
    {
      "epoch": 4.93,
      "grad_norm": 0.26564180850982666,
      "learning_rate": 0.0018958958958958958,
      "loss": 0.1565,
      "step": 53
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.19641900062561035,
      "learning_rate": 0.0018938938938938938,
      "loss": 0.1209,
      "step": 54
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.22598569095134735,
      "learning_rate": 0.0018918918918918919,
      "loss": 0.1164,
      "step": 55
    },
    {
      "epoch": 5.21,
      "grad_norm": 0.251605749130249,
      "learning_rate": 0.00188988988988989,
      "loss": 0.1108,
      "step": 56
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.23231488466262817,
      "learning_rate": 0.001887887887887888,
      "loss": 0.0952,
      "step": 57
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.29818660020828247,
      "learning_rate": 0.0018858858858858858,
      "loss": 0.1099,
      "step": 58
    },
    {
      "epoch": 5.49,
      "grad_norm": 0.24504368007183075,
      "learning_rate": 0.0018838838838838839,
      "loss": 0.101,
      "step": 59
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.21923938393592834,
      "learning_rate": 0.001881881881881882,
      "loss": 0.1039,
      "step": 60
    },
    {
      "epoch": 5.67,
      "grad_norm": 0.24487842619419098,
      "learning_rate": 0.00187987987987988,
      "loss": 0.1001,
      "step": 61
    },
    {
      "epoch": 5.77,
      "grad_norm": 0.261868417263031,
      "learning_rate": 0.001877877877877878,
      "loss": 0.0911,
      "step": 62
    },
    {
      "epoch": 5.86,
      "grad_norm": 0.26674044132232666,
      "learning_rate": 0.0018758758758758759,
      "loss": 0.1053,
      "step": 63
    },
    {
      "epoch": 5.95,
      "grad_norm": 0.1992938369512558,
      "learning_rate": 0.001873873873873874,
      "loss": 0.0883,
      "step": 64
    },
    {
      "epoch": 6.05,
      "grad_norm": 0.5339002013206482,
      "learning_rate": 0.001871871871871872,
      "loss": 0.0867,
      "step": 65
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.1707654446363449,
      "learning_rate": 0.00186986986986987,
      "loss": 0.0775,
      "step": 66
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.8775418400764465,
      "learning_rate": 0.001867867867867868,
      "loss": 0.0801,
      "step": 67
    },
    {
      "epoch": 6.33,
      "grad_norm": 0.4073001444339752,
      "learning_rate": 0.0018658658658658657,
      "loss": 0.1034,
      "step": 68
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.29436206817626953,
      "learning_rate": 0.0018638638638638638,
      "loss": 0.1003,
      "step": 69
    },
    {
      "epoch": 6.51,
      "grad_norm": 0.290439635515213,
      "learning_rate": 0.0018618618618618619,
      "loss": 0.0956,
      "step": 70
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.21431350708007812,
      "learning_rate": 0.00185985985985986,
      "loss": 0.0852,
      "step": 71
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.24671588838100433,
      "learning_rate": 0.001857857857857858,
      "loss": 0.0848,
      "step": 72
    },
    {
      "epoch": 6.79,
      "grad_norm": 0.2750566601753235,
      "learning_rate": 0.0018558558558558558,
      "loss": 0.0784,
      "step": 73
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.2785981297492981,
      "learning_rate": 0.0018538538538538539,
      "loss": 0.0853,
      "step": 74
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.2566787004470825,
      "learning_rate": 0.001851851851851852,
      "loss": 0.0802,
      "step": 75
    },
    {
      "epoch": 7.07,
      "grad_norm": 0.18839526176452637,
      "learning_rate": 0.00184984984984985,
      "loss": 0.0635,
      "step": 76
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.20859406888484955,
      "learning_rate": 0.001847847847847848,
      "loss": 0.0669,
      "step": 77
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.3039081394672394,
      "learning_rate": 0.0018458458458458459,
      "loss": 0.0644,
      "step": 78
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.2052895873785019,
      "learning_rate": 0.001843843843843844,
      "loss": 0.0628,
      "step": 79
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.19869670271873474,
      "learning_rate": 0.001841841841841842,
      "loss": 0.0676,
      "step": 80
    },
    {
      "epoch": 7.53,
      "grad_norm": 0.22746023535728455,
      "learning_rate": 0.0018398398398398398,
      "loss": 0.0623,
      "step": 81
    },
    {
      "epoch": 7.63,
      "grad_norm": 0.19103148579597473,
      "learning_rate": 0.0018378378378378379,
      "loss": 0.0648,
      "step": 82
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.18535712361335754,
      "learning_rate": 0.0018358358358358357,
      "loss": 0.0625,
      "step": 83
    },
    {
      "epoch": 7.81,
      "grad_norm": 0.21970614790916443,
      "learning_rate": 0.0018338338338338338,
      "loss": 0.0735,
      "step": 84
    },
    {
      "epoch": 7.91,
      "grad_norm": 0.21595558524131775,
      "learning_rate": 0.0018318318318318318,
      "loss": 0.0728,
      "step": 85
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.2423926591873169,
      "learning_rate": 0.00182982982982983,
      "loss": 0.0693,
      "step": 86
    },
    {
      "epoch": 8.09,
      "grad_norm": 0.17347559332847595,
      "learning_rate": 0.001827827827827828,
      "loss": 0.041,
      "step": 87
    },
    {
      "epoch": 8.19,
      "grad_norm": 0.19068543612957,
      "learning_rate": 0.0018258258258258258,
      "loss": 0.05,
      "step": 88
    },
    {
      "epoch": 8.28,
      "grad_norm": 0.15929880738258362,
      "learning_rate": 0.0018238238238238239,
      "loss": 0.0395,
      "step": 89
    },
    {
      "epoch": 8.37,
      "grad_norm": 0.2608363628387451,
      "learning_rate": 0.001821821821821822,
      "loss": 0.0446,
      "step": 90
    },
    {
      "epoch": 8.47,
      "grad_norm": 0.18586759269237518,
      "learning_rate": 0.00181981981981982,
      "loss": 0.0509,
      "step": 91
    },
    {
      "epoch": 8.56,
      "grad_norm": 0.16114765405654907,
      "learning_rate": 0.001817817817817818,
      "loss": 0.0421,
      "step": 92
    },
    {
      "epoch": 8.65,
      "grad_norm": 0.16853126883506775,
      "learning_rate": 0.0018158158158158159,
      "loss": 0.0462,
      "step": 93
    },
    {
      "epoch": 8.74,
      "grad_norm": 0.17938560247421265,
      "learning_rate": 0.001813813813813814,
      "loss": 0.0459,
      "step": 94
    },
    {
      "epoch": 8.84,
      "grad_norm": 0.19151407480239868,
      "learning_rate": 0.001811811811811812,
      "loss": 0.0494,
      "step": 95
    },
    {
      "epoch": 8.93,
      "grad_norm": 0.14819422364234924,
      "learning_rate": 0.0018098098098098098,
      "loss": 0.0421,
      "step": 96
    },
    {
      "epoch": 9.02,
      "grad_norm": 0.14905127882957458,
      "learning_rate": 0.0018078078078078077,
      "loss": 0.0381,
      "step": 97
    },
    {
      "epoch": 9.12,
      "grad_norm": 0.12670212984085083,
      "learning_rate": 0.0018058058058058057,
      "loss": 0.0287,
      "step": 98
    },
    {
      "epoch": 9.21,
      "grad_norm": 0.32756730914115906,
      "learning_rate": 0.0018038038038038038,
      "loss": 0.0306,
      "step": 99
    },
    {
      "epoch": 9.3,
      "grad_norm": 0.19094567000865936,
      "learning_rate": 0.0018018018018018018,
      "loss": 0.0413,
      "step": 100
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.18511539697647095,
      "learning_rate": 0.0017997997997997999,
      "loss": 0.0407,
      "step": 101
    },
    {
      "epoch": 9.49,
      "grad_norm": 0.16668114066123962,
      "learning_rate": 0.0017977977977977977,
      "loss": 0.0326,
      "step": 102
    },
    {
      "epoch": 9.58,
      "grad_norm": 0.15231497585773468,
      "learning_rate": 0.0017957957957957958,
      "loss": 0.0299,
      "step": 103
    },
    {
      "epoch": 9.67,
      "grad_norm": 0.19253884255886078,
      "learning_rate": 0.0017937937937937938,
      "loss": 0.0425,
      "step": 104
    },
    {
      "epoch": 9.77,
      "grad_norm": 0.161960169672966,
      "learning_rate": 0.001791791791791792,
      "loss": 0.0385,
      "step": 105
    },
    {
      "epoch": 9.86,
      "grad_norm": 0.15865224599838257,
      "learning_rate": 0.00178978978978979,
      "loss": 0.0358,
      "step": 106
    },
    {
      "epoch": 9.95,
      "grad_norm": 0.16319987177848816,
      "learning_rate": 0.0017877877877877878,
      "loss": 0.0367,
      "step": 107
    },
    {
      "epoch": 10.05,
      "grad_norm": 0.1199568435549736,
      "learning_rate": 0.0017857857857857859,
      "loss": 0.0279,
      "step": 108
    },
    {
      "epoch": 10.14,
      "grad_norm": 0.12916956841945648,
      "learning_rate": 0.001783783783783784,
      "loss": 0.0234,
      "step": 109
    },
    {
      "epoch": 10.23,
      "grad_norm": 0.14657966792583466,
      "learning_rate": 0.0017817817817817817,
      "loss": 0.0263,
      "step": 110
    },
    {
      "epoch": 10.33,
      "grad_norm": 0.1660681962966919,
      "learning_rate": 0.0017797797797797798,
      "loss": 0.0303,
      "step": 111
    },
    {
      "epoch": 10.42,
      "grad_norm": 0.1787068247795105,
      "learning_rate": 0.0017777777777777776,
      "loss": 0.0325,
      "step": 112
    },
    {
      "epoch": 10.51,
      "grad_norm": 0.36577415466308594,
      "learning_rate": 0.0017757757757757757,
      "loss": 0.0293,
      "step": 113
    },
    {
      "epoch": 10.6,
      "grad_norm": 0.15158431231975555,
      "learning_rate": 0.0017737737737737738,
      "loss": 0.035,
      "step": 114
    },
    {
      "epoch": 10.7,
      "grad_norm": 0.14038600027561188,
      "learning_rate": 0.0017717717717717718,
      "loss": 0.0359,
      "step": 115
    },
    {
      "epoch": 10.79,
      "grad_norm": 0.16171874105930328,
      "learning_rate": 0.0017697697697697699,
      "loss": 0.0309,
      "step": 116
    },
    {
      "epoch": 10.88,
      "grad_norm": 0.12541256844997406,
      "learning_rate": 0.0017677677677677677,
      "loss": 0.0272,
      "step": 117
    },
    {
      "epoch": 10.98,
      "grad_norm": 0.28332453966140747,
      "learning_rate": 0.0017657657657657658,
      "loss": 0.0356,
      "step": 118
    },
    {
      "epoch": 11.07,
      "grad_norm": 0.20303605496883392,
      "learning_rate": 0.0017637637637637638,
      "loss": 0.03,
      "step": 119
    },
    {
      "epoch": 11.16,
      "grad_norm": 0.1583126187324524,
      "learning_rate": 0.0017617617617617619,
      "loss": 0.0237,
      "step": 120
    },
    {
      "epoch": 11.26,
      "grad_norm": 0.1663227677345276,
      "learning_rate": 0.00175975975975976,
      "loss": 0.0242,
      "step": 121
    },
    {
      "epoch": 11.35,
      "grad_norm": 0.1336936503648758,
      "learning_rate": 0.0017577577577577578,
      "loss": 0.0203,
      "step": 122
    },
    {
      "epoch": 11.44,
      "grad_norm": 0.22252091765403748,
      "learning_rate": 0.0017557557557557558,
      "loss": 0.0375,
      "step": 123
    },
    {
      "epoch": 11.53,
      "grad_norm": 0.12601596117019653,
      "learning_rate": 0.001753753753753754,
      "loss": 0.0208,
      "step": 124
    },
    {
      "epoch": 11.63,
      "grad_norm": 0.19643957912921906,
      "learning_rate": 0.0017517517517517517,
      "loss": 0.0393,
      "step": 125
    },
    {
      "epoch": 11.72,
      "grad_norm": 0.1497407853603363,
      "learning_rate": 0.0017497497497497498,
      "loss": 0.0299,
      "step": 126
    },
    {
      "epoch": 11.81,
      "grad_norm": 0.1200670525431633,
      "learning_rate": 0.0017477477477477476,
      "loss": 0.027,
      "step": 127
    },
    {
      "epoch": 11.91,
      "grad_norm": 0.17628063261508942,
      "learning_rate": 0.0017457457457457457,
      "loss": 0.0414,
      "step": 128
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.1418675035238266,
      "learning_rate": 0.0017437437437437437,
      "loss": 0.0273,
      "step": 129
    },
    {
      "epoch": 12.09,
      "grad_norm": 0.1458865851163864,
      "learning_rate": 0.0017417417417417418,
      "loss": 0.022,
      "step": 130
    },
    {
      "epoch": 12.19,
      "grad_norm": 0.11567732691764832,
      "learning_rate": 0.0017397397397397399,
      "loss": 0.0167,
      "step": 131
    },
    {
      "epoch": 12.28,
      "grad_norm": 0.44207289814949036,
      "learning_rate": 0.0017377377377377377,
      "loss": 0.0262,
      "step": 132
    },
    {
      "epoch": 12.37,
      "grad_norm": 0.14352640509605408,
      "learning_rate": 0.0017357357357357358,
      "loss": 0.0236,
      "step": 133
    },
    {
      "epoch": 12.47,
      "grad_norm": 0.19560161232948303,
      "learning_rate": 0.0017337337337337338,
      "loss": 0.0339,
      "step": 134
    },
    {
      "epoch": 12.56,
      "grad_norm": 0.17569731175899506,
      "learning_rate": 0.0017317317317317319,
      "loss": 0.0332,
      "step": 135
    },
    {
      "epoch": 12.65,
      "grad_norm": 0.2208559513092041,
      "learning_rate": 0.00172972972972973,
      "loss": 0.036,
      "step": 136
    },
    {
      "epoch": 12.74,
      "grad_norm": 0.17576010525226593,
      "learning_rate": 0.0017277277277277278,
      "loss": 0.0293,
      "step": 137
    },
    {
      "epoch": 12.84,
      "grad_norm": 0.17175643146038055,
      "learning_rate": 0.0017257257257257258,
      "loss": 0.0305,
      "step": 138
    },
    {
      "epoch": 12.93,
      "grad_norm": 0.1681094914674759,
      "learning_rate": 0.0017237237237237237,
      "loss": 0.0311,
      "step": 139
    },
    {
      "epoch": 13.02,
      "grad_norm": 0.25889313220977783,
      "learning_rate": 0.0017217217217217217,
      "loss": 0.0332,
      "step": 140
    },
    {
      "epoch": 13.12,
      "grad_norm": 0.13084806501865387,
      "learning_rate": 0.0017197197197197198,
      "loss": 0.0215,
      "step": 141
    },
    {
      "epoch": 13.21,
      "grad_norm": 0.12187618762254715,
      "learning_rate": 0.0017177177177177176,
      "loss": 0.0192,
      "step": 142
    },
    {
      "epoch": 13.3,
      "grad_norm": 0.12164618819952011,
      "learning_rate": 0.0017157157157157157,
      "loss": 0.0195,
      "step": 143
    },
    {
      "epoch": 13.4,
      "grad_norm": 0.12924732267856598,
      "learning_rate": 0.0017137137137137137,
      "loss": 0.0199,
      "step": 144
    },
    {
      "epoch": 13.49,
      "grad_norm": 0.15081335604190826,
      "learning_rate": 0.0017117117117117118,
      "loss": 0.0244,
      "step": 145
    },
    {
      "epoch": 13.58,
      "grad_norm": 0.20312143862247467,
      "learning_rate": 0.0017097097097097098,
      "loss": 0.0299,
      "step": 146
    },
    {
      "epoch": 13.67,
      "grad_norm": 0.17748410999774933,
      "learning_rate": 0.0017077077077077077,
      "loss": 0.0275,
      "step": 147
    },
    {
      "epoch": 13.77,
      "grad_norm": 0.16115255653858185,
      "learning_rate": 0.0017057057057057057,
      "loss": 0.0289,
      "step": 148
    },
    {
      "epoch": 13.86,
      "grad_norm": 0.14800146222114563,
      "learning_rate": 0.0017037037037037038,
      "loss": 0.0255,
      "step": 149
    },
    {
      "epoch": 13.95,
      "grad_norm": 0.13397763669490814,
      "learning_rate": 0.0017017017017017019,
      "loss": 0.0222,
      "step": 150
    },
    {
      "epoch": 14.05,
      "grad_norm": 0.15193189680576324,
      "learning_rate": 0.0016996996996997,
      "loss": 0.0249,
      "step": 151
    },
    {
      "epoch": 14.14,
      "grad_norm": 0.11898165196180344,
      "learning_rate": 0.0016976976976976978,
      "loss": 0.0166,
      "step": 152
    },
    {
      "epoch": 14.23,
      "grad_norm": 0.13656771183013916,
      "learning_rate": 0.0016956956956956956,
      "loss": 0.0223,
      "step": 153
    },
    {
      "epoch": 14.33,
      "grad_norm": 0.15442809462547302,
      "learning_rate": 0.0016936936936936937,
      "loss": 0.0263,
      "step": 154
    },
    {
      "epoch": 14.42,
      "grad_norm": 0.35817015171051025,
      "learning_rate": 0.0016916916916916917,
      "loss": 0.0182,
      "step": 155
    },
    {
      "epoch": 14.51,
      "grad_norm": 0.14386148750782013,
      "learning_rate": 0.0016896896896896898,
      "loss": 0.0234,
      "step": 156
    },
    {
      "epoch": 14.6,
      "grad_norm": 0.15172500908374786,
      "learning_rate": 0.0016876876876876876,
      "loss": 0.0234,
      "step": 157
    },
    {
      "epoch": 14.7,
      "grad_norm": 0.1171698123216629,
      "learning_rate": 0.0016856856856856857,
      "loss": 0.0196,
      "step": 158
    },
    {
      "epoch": 14.79,
      "grad_norm": 0.14162251353263855,
      "learning_rate": 0.0016836836836836837,
      "loss": 0.0222,
      "step": 159
    },
    {
      "epoch": 14.88,
      "grad_norm": 0.16906990110874176,
      "learning_rate": 0.0016816816816816818,
      "loss": 0.0208,
      "step": 160
    },
    {
      "epoch": 14.98,
      "grad_norm": 0.13473807275295258,
      "learning_rate": 0.0016796796796796796,
      "loss": 0.0198,
      "step": 161
    },
    {
      "epoch": 15.07,
      "grad_norm": 0.7500114440917969,
      "learning_rate": 0.0016776776776776777,
      "loss": 0.0237,
      "step": 162
    },
    {
      "epoch": 15.16,
      "grad_norm": 0.17404961585998535,
      "learning_rate": 0.0016756756756756757,
      "loss": 0.022,
      "step": 163
    },
    {
      "epoch": 15.26,
      "grad_norm": 0.20667695999145508,
      "learning_rate": 0.0016736736736736738,
      "loss": 0.0317,
      "step": 164
    },
    {
      "epoch": 15.35,
      "grad_norm": 0.18450109660625458,
      "learning_rate": 0.0016716716716716718,
      "loss": 0.0324,
      "step": 165
    },
    {
      "epoch": 15.44,
      "grad_norm": 0.21405695378780365,
      "learning_rate": 0.0016696696696696697,
      "loss": 0.0354,
      "step": 166
    },
    {
      "epoch": 15.53,
      "grad_norm": 0.17413051426410675,
      "learning_rate": 0.0016676676676676677,
      "loss": 0.0282,
      "step": 167
    },
    {
      "epoch": 15.63,
      "grad_norm": 0.474658340215683,
      "learning_rate": 0.0016656656656656656,
      "loss": 0.0292,
      "step": 168
    },
    {
      "epoch": 15.72,
      "grad_norm": 0.216481015086174,
      "learning_rate": 0.0016636636636636636,
      "loss": 0.0324,
      "step": 169
    },
    {
      "epoch": 15.81,
      "grad_norm": 0.22553342580795288,
      "learning_rate": 0.0016616616616616617,
      "loss": 0.0319,
      "step": 170
    },
    {
      "epoch": 15.91,
      "grad_norm": 0.21478119492530823,
      "learning_rate": 0.0016596596596596595,
      "loss": 0.0325,
      "step": 171
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.2862038016319275,
      "learning_rate": 0.0016576576576576576,
      "loss": 0.0429,
      "step": 172
    },
    {
      "epoch": 16.09,
      "grad_norm": 0.15950839221477509,
      "learning_rate": 0.0016556556556556557,
      "loss": 0.0243,
      "step": 173
    },
    {
      "epoch": 16.19,
      "grad_norm": 0.31929463148117065,
      "learning_rate": 0.0016536536536536537,
      "loss": 0.0271,
      "step": 174
    },
    {
      "epoch": 16.28,
      "grad_norm": 0.1751406043767929,
      "learning_rate": 0.0016516516516516518,
      "loss": 0.0245,
      "step": 175
    },
    {
      "epoch": 16.37,
      "grad_norm": 0.16707609593868256,
      "learning_rate": 0.0016496496496496496,
      "loss": 0.0242,
      "step": 176
    },
    {
      "epoch": 16.47,
      "grad_norm": 0.7891533970832825,
      "learning_rate": 0.0016476476476476477,
      "loss": 0.0321,
      "step": 177
    },
    {
      "epoch": 16.56,
      "grad_norm": 0.18779712915420532,
      "learning_rate": 0.0016456456456456457,
      "loss": 0.0252,
      "step": 178
    },
    {
      "epoch": 16.65,
      "grad_norm": 0.1881711632013321,
      "learning_rate": 0.0016436436436436438,
      "loss": 0.0282,
      "step": 179
    },
    {
      "epoch": 16.74,
      "grad_norm": 0.203497052192688,
      "learning_rate": 0.0016416416416416418,
      "loss": 0.0269,
      "step": 180
    },
    {
      "epoch": 16.84,
      "grad_norm": 0.46536093950271606,
      "learning_rate": 0.0016396396396396397,
      "loss": 0.0327,
      "step": 181
    },
    {
      "epoch": 16.93,
      "grad_norm": 0.17711864411830902,
      "learning_rate": 0.0016376376376376375,
      "loss": 0.0267,
      "step": 182
    },
    {
      "epoch": 17.02,
      "grad_norm": 0.2085958570241928,
      "learning_rate": 0.0016356356356356356,
      "loss": 0.0351,
      "step": 183
    },
    {
      "epoch": 17.12,
      "grad_norm": 0.16959838569164276,
      "learning_rate": 0.0016336336336336336,
      "loss": 0.0245,
      "step": 184
    },
    {
      "epoch": 17.21,
      "grad_norm": 0.21090783178806305,
      "learning_rate": 0.0016316316316316317,
      "loss": 0.0317,
      "step": 185
    },
    {
      "epoch": 17.3,
      "grad_norm": 0.24964235723018646,
      "learning_rate": 0.0016296296296296295,
      "loss": 0.0237,
      "step": 186
    },
    {
      "epoch": 17.4,
      "grad_norm": 0.1871645301580429,
      "learning_rate": 0.0016276276276276276,
      "loss": 0.0279,
      "step": 187
    },
    {
      "epoch": 17.49,
      "grad_norm": 0.17427071928977966,
      "learning_rate": 0.0016256256256256256,
      "loss": 0.0246,
      "step": 188
    },
    {
      "epoch": 17.58,
      "grad_norm": 0.2249002605676651,
      "learning_rate": 0.0016236236236236237,
      "loss": 0.0342,
      "step": 189
    },
    {
      "epoch": 17.67,
      "grad_norm": 0.14843669533729553,
      "learning_rate": 0.0016216216216216218,
      "loss": 0.0237,
      "step": 190
    },
    {
      "epoch": 17.77,
      "grad_norm": 0.19408485293388367,
      "learning_rate": 0.0016196196196196196,
      "loss": 0.0312,
      "step": 191
    },
    {
      "epoch": 17.86,
      "grad_norm": 0.1466081440448761,
      "learning_rate": 0.0016176176176176177,
      "loss": 0.0232,
      "step": 192
    },
    {
      "epoch": 17.95,
      "grad_norm": 0.7054815888404846,
      "learning_rate": 0.0016156156156156157,
      "loss": 0.031,
      "step": 193
    },
    {
      "epoch": 18.05,
      "grad_norm": 0.23588448762893677,
      "learning_rate": 0.0016136136136136138,
      "loss": 0.032,
      "step": 194
    },
    {
      "epoch": 18.14,
      "grad_norm": 0.25636205077171326,
      "learning_rate": 0.0016116116116116118,
      "loss": 0.0304,
      "step": 195
    },
    {
      "epoch": 18.23,
      "grad_norm": 0.1628827303647995,
      "learning_rate": 0.0016096096096096097,
      "loss": 0.0183,
      "step": 196
    },
    {
      "epoch": 18.33,
      "grad_norm": 0.6799976229667664,
      "learning_rate": 0.0016076076076076075,
      "loss": 0.0383,
      "step": 197
    },
    {
      "epoch": 18.42,
      "grad_norm": 0.24623911082744598,
      "learning_rate": 0.0016056056056056056,
      "loss": 0.0301,
      "step": 198
    },
    {
      "epoch": 18.51,
      "grad_norm": 0.26263728737831116,
      "learning_rate": 0.0016036036036036036,
      "loss": 0.0384,
      "step": 199
    },
    {
      "epoch": 18.6,
      "grad_norm": 0.24216783046722412,
      "learning_rate": 0.0016016016016016017,
      "loss": 0.0377,
      "step": 200
    },
    {
      "epoch": 18.7,
      "grad_norm": 0.21968820691108704,
      "learning_rate": 0.0015995995995995995,
      "loss": 0.0388,
      "step": 201
    },
    {
      "epoch": 18.79,
      "grad_norm": 0.23277999460697174,
      "learning_rate": 0.0015975975975975976,
      "loss": 0.0335,
      "step": 202
    },
    {
      "epoch": 18.88,
      "grad_norm": 0.7189760804176331,
      "learning_rate": 0.0015955955955955956,
      "loss": 0.0448,
      "step": 203
    },
    {
      "epoch": 18.98,
      "grad_norm": 0.5283987522125244,
      "learning_rate": 0.0015935935935935937,
      "loss": 0.0479,
      "step": 204
    },
    {
      "epoch": 19.07,
      "grad_norm": 0.1992095708847046,
      "learning_rate": 0.0015915915915915917,
      "loss": 0.0259,
      "step": 205
    },
    {
      "epoch": 19.16,
      "grad_norm": 0.2722899317741394,
      "learning_rate": 0.0015895895895895896,
      "loss": 0.035,
      "step": 206
    },
    {
      "epoch": 19.26,
      "grad_norm": 0.16028214991092682,
      "learning_rate": 0.0015875875875875876,
      "loss": 0.0252,
      "step": 207
    },
    {
      "epoch": 19.35,
      "grad_norm": 0.1479153335094452,
      "learning_rate": 0.0015855855855855857,
      "loss": 0.0186,
      "step": 208
    },
    {
      "epoch": 19.44,
      "grad_norm": 0.18401648104190826,
      "learning_rate": 0.0015835835835835838,
      "loss": 0.0273,
      "step": 209
    },
    {
      "epoch": 19.53,
      "grad_norm": 0.45847856998443604,
      "learning_rate": 0.0015815815815815818,
      "loss": 0.0219,
      "step": 210
    },
    {
      "epoch": 19.63,
      "grad_norm": 0.172963485121727,
      "learning_rate": 0.0015795795795795794,
      "loss": 0.0212,
      "step": 211
    },
    {
      "epoch": 19.72,
      "grad_norm": 0.18817926943302155,
      "learning_rate": 0.0015775775775775775,
      "loss": 0.0292,
      "step": 212
    },
    {
      "epoch": 19.81,
      "grad_norm": 0.20419757068157196,
      "learning_rate": 0.0015755755755755755,
      "loss": 0.0303,
      "step": 213
    },
    {
      "epoch": 19.91,
      "grad_norm": 0.19756686687469482,
      "learning_rate": 0.0015735735735735736,
      "loss": 0.0271,
      "step": 214
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.14679427444934845,
      "learning_rate": 0.0015715715715715717,
      "loss": 0.0214,
      "step": 215
    },
    {
      "epoch": 20.09,
      "grad_norm": 0.14583350718021393,
      "learning_rate": 0.0015695695695695695,
      "loss": 0.0191,
      "step": 216
    },
    {
      "epoch": 20.19,
      "grad_norm": 0.15297341346740723,
      "learning_rate": 0.0015675675675675676,
      "loss": 0.0176,
      "step": 217
    },
    {
      "epoch": 20.28,
      "grad_norm": 0.18278124928474426,
      "learning_rate": 0.0015655655655655656,
      "loss": 0.0239,
      "step": 218
    },
    {
      "epoch": 20.37,
      "grad_norm": 0.1664392501115799,
      "learning_rate": 0.0015635635635635637,
      "loss": 0.0206,
      "step": 219
    },
    {
      "epoch": 20.47,
      "grad_norm": 0.49318164587020874,
      "learning_rate": 0.0015615615615615615,
      "loss": 0.0229,
      "step": 220
    },
    {
      "epoch": 20.56,
      "grad_norm": 0.13801375031471252,
      "learning_rate": 0.0015595595595595596,
      "loss": 0.0158,
      "step": 221
    },
    {
      "epoch": 20.65,
      "grad_norm": 0.154294952750206,
      "learning_rate": 0.0015575575575575576,
      "loss": 0.0205,
      "step": 222
    },
    {
      "epoch": 20.74,
      "grad_norm": 0.14349927008152008,
      "learning_rate": 0.0015555555555555557,
      "loss": 0.0218,
      "step": 223
    },
    {
      "epoch": 20.84,
      "grad_norm": 0.14842985570430756,
      "learning_rate": 0.0015535535535535537,
      "loss": 0.0228,
      "step": 224
    },
    {
      "epoch": 20.93,
      "grad_norm": 0.14195027947425842,
      "learning_rate": 0.0015515515515515516,
      "loss": 0.0218,
      "step": 225
    },
    {
      "epoch": 21.02,
      "grad_norm": 0.1998073309659958,
      "learning_rate": 0.0015495495495495494,
      "loss": 0.0323,
      "step": 226
    },
    {
      "epoch": 21.12,
      "grad_norm": 0.09599553048610687,
      "learning_rate": 0.0015475475475475475,
      "loss": 0.0128,
      "step": 227
    },
    {
      "epoch": 21.21,
      "grad_norm": 0.15941940248012543,
      "learning_rate": 0.0015455455455455455,
      "loss": 0.0193,
      "step": 228
    },
    {
      "epoch": 21.3,
      "grad_norm": 0.18800006806850433,
      "learning_rate": 0.0015435435435435436,
      "loss": 0.0202,
      "step": 229
    },
    {
      "epoch": 21.4,
      "grad_norm": 0.10920831561088562,
      "learning_rate": 0.0015415415415415414,
      "loss": 0.0111,
      "step": 230
    },
    {
      "epoch": 21.49,
      "grad_norm": 0.2068585604429245,
      "learning_rate": 0.0015395395395395395,
      "loss": 0.0272,
      "step": 231
    },
    {
      "epoch": 21.58,
      "grad_norm": 0.13729514181613922,
      "learning_rate": 0.0015375375375375375,
      "loss": 0.0188,
      "step": 232
    },
    {
      "epoch": 21.67,
      "grad_norm": 0.11570963263511658,
      "learning_rate": 0.0015355355355355356,
      "loss": 0.0162,
      "step": 233
    },
    {
      "epoch": 21.77,
      "grad_norm": 0.11246766149997711,
      "learning_rate": 0.0015335335335335337,
      "loss": 0.0158,
      "step": 234
    },
    {
      "epoch": 21.86,
      "grad_norm": 0.13074816763401031,
      "learning_rate": 0.0015315315315315315,
      "loss": 0.0146,
      "step": 235
    },
    {
      "epoch": 21.95,
      "grad_norm": 0.132390558719635,
      "learning_rate": 0.0015295295295295296,
      "loss": 0.0154,
      "step": 236
    },
    {
      "epoch": 22.05,
      "grad_norm": 0.09564812481403351,
      "learning_rate": 0.0015275275275275276,
      "loss": 0.0119,
      "step": 237
    },
    {
      "epoch": 22.14,
      "grad_norm": 0.10601690411567688,
      "learning_rate": 0.0015255255255255257,
      "loss": 0.0158,
      "step": 238
    },
    {
      "epoch": 22.23,
      "grad_norm": 0.11311355233192444,
      "learning_rate": 0.0015235235235235237,
      "loss": 0.0122,
      "step": 239
    },
    {
      "epoch": 22.33,
      "grad_norm": 0.12214581668376923,
      "learning_rate": 0.0015215215215215214,
      "loss": 0.0132,
      "step": 240
    },
    {
      "epoch": 22.42,
      "grad_norm": 0.1116672083735466,
      "learning_rate": 0.0015195195195195194,
      "loss": 0.0102,
      "step": 241
    },
    {
      "epoch": 22.51,
      "grad_norm": 0.08826189488172531,
      "learning_rate": 0.0015175175175175175,
      "loss": 0.0075,
      "step": 242
    },
    {
      "epoch": 22.6,
      "grad_norm": 0.10075847059488297,
      "learning_rate": 0.0015155155155155155,
      "loss": 0.0119,
      "step": 243
    },
    {
      "epoch": 22.7,
      "grad_norm": 0.11353474110364914,
      "learning_rate": 0.0015135135135135136,
      "loss": 0.0139,
      "step": 244
    },
    {
      "epoch": 22.79,
      "grad_norm": 0.10257426649332047,
      "learning_rate": 0.0015115115115115114,
      "loss": 0.0093,
      "step": 245
    },
    {
      "epoch": 22.88,
      "grad_norm": 0.09495458006858826,
      "learning_rate": 0.0015095095095095095,
      "loss": 0.0108,
      "step": 246
    },
    {
      "epoch": 22.98,
      "grad_norm": 0.5463971495628357,
      "learning_rate": 0.0015075075075075075,
      "loss": 0.0148,
      "step": 247
    },
    {
      "epoch": 23.07,
      "grad_norm": 0.10208092629909515,
      "learning_rate": 0.0015055055055055056,
      "loss": 0.0102,
      "step": 248
    },
    {
      "epoch": 23.16,
      "grad_norm": 0.18766802549362183,
      "learning_rate": 0.0015035035035035036,
      "loss": 0.019,
      "step": 249
    },
    {
      "epoch": 23.26,
      "grad_norm": 0.12946560978889465,
      "learning_rate": 0.0015015015015015015,
      "loss": 0.0138,
      "step": 250
    },
    {
      "epoch": 23.35,
      "grad_norm": 0.16246852278709412,
      "learning_rate": 0.0014994994994994995,
      "loss": 0.0158,
      "step": 251
    },
    {
      "epoch": 23.44,
      "grad_norm": 0.12050007283687592,
      "learning_rate": 0.0014974974974974976,
      "loss": 0.0094,
      "step": 252
    },
    {
      "epoch": 23.53,
      "grad_norm": 0.20695464313030243,
      "learning_rate": 0.0014954954954954957,
      "loss": 0.02,
      "step": 253
    },
    {
      "epoch": 23.63,
      "grad_norm": 0.12067891657352448,
      "learning_rate": 0.0014934934934934937,
      "loss": 0.0123,
      "step": 254
    },
    {
      "epoch": 23.72,
      "grad_norm": 0.1243496686220169,
      "learning_rate": 0.0014914914914914913,
      "loss": 0.016,
      "step": 255
    },
    {
      "epoch": 23.81,
      "grad_norm": 0.11034452170133591,
      "learning_rate": 0.0014894894894894894,
      "loss": 0.0141,
      "step": 256
    },
    {
      "epoch": 23.91,
      "grad_norm": 0.10018245875835419,
      "learning_rate": 0.0014874874874874875,
      "loss": 0.0119,
      "step": 257
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.10271686315536499,
      "learning_rate": 0.0014854854854854855,
      "loss": 0.0113,
      "step": 258
    },
    {
      "epoch": 24.09,
      "grad_norm": 0.0902242660522461,
      "learning_rate": 0.0014834834834834836,
      "loss": 0.0088,
      "step": 259
    },
    {
      "epoch": 24.19,
      "grad_norm": 0.0898832306265831,
      "learning_rate": 0.0014814814814814814,
      "loss": 0.0068,
      "step": 260
    },
    {
      "epoch": 24.28,
      "grad_norm": 0.10742070525884628,
      "learning_rate": 0.0014794794794794795,
      "loss": 0.008,
      "step": 261
    },
    {
      "epoch": 24.37,
      "grad_norm": 0.3394037187099457,
      "learning_rate": 0.0014774774774774775,
      "loss": 0.012,
      "step": 262
    },
    {
      "epoch": 24.47,
      "grad_norm": 0.1366187185049057,
      "learning_rate": 0.0014754754754754756,
      "loss": 0.0128,
      "step": 263
    },
    {
      "epoch": 24.56,
      "grad_norm": 0.16941972076892853,
      "learning_rate": 0.0014734734734734736,
      "loss": 0.0144,
      "step": 264
    },
    {
      "epoch": 24.65,
      "grad_norm": 0.13689333200454712,
      "learning_rate": 0.0014714714714714715,
      "loss": 0.0131,
      "step": 265
    },
    {
      "epoch": 24.74,
      "grad_norm": 0.12805940210819244,
      "learning_rate": 0.0014694694694694695,
      "loss": 0.0135,
      "step": 266
    },
    {
      "epoch": 24.84,
      "grad_norm": 0.15572786331176758,
      "learning_rate": 0.0014674674674674676,
      "loss": 0.0181,
      "step": 267
    },
    {
      "epoch": 24.93,
      "grad_norm": 0.11057024449110031,
      "learning_rate": 0.0014654654654654656,
      "loss": 0.0119,
      "step": 268
    },
    {
      "epoch": 25.02,
      "grad_norm": 0.15199409425258636,
      "learning_rate": 0.0014634634634634637,
      "loss": 0.017,
      "step": 269
    },
    {
      "epoch": 25.12,
      "grad_norm": 0.11110897362232208,
      "learning_rate": 0.0014614614614614613,
      "loss": 0.0099,
      "step": 270
    },
    {
      "epoch": 25.21,
      "grad_norm": 0.0694834515452385,
      "learning_rate": 0.0014594594594594594,
      "loss": 0.0065,
      "step": 271
    },
    {
      "epoch": 25.3,
      "grad_norm": 0.12897028028964996,
      "learning_rate": 0.0014574574574574574,
      "loss": 0.0126,
      "step": 272
    },
    {
      "epoch": 25.4,
      "grad_norm": 0.10043667256832123,
      "learning_rate": 0.0014554554554554555,
      "loss": 0.0112,
      "step": 273
    },
    {
      "epoch": 25.49,
      "grad_norm": 0.10153573751449585,
      "learning_rate": 0.0014534534534534536,
      "loss": 0.0099,
      "step": 274
    },
    {
      "epoch": 25.58,
      "grad_norm": 0.4327797591686249,
      "learning_rate": 0.0014514514514514514,
      "loss": 0.0086,
      "step": 275
    },
    {
      "epoch": 25.67,
      "grad_norm": 0.12818408012390137,
      "learning_rate": 0.0014494494494494495,
      "loss": 0.0142,
      "step": 276
    },
    {
      "epoch": 25.77,
      "grad_norm": 0.13305599987506866,
      "learning_rate": 0.0014474474474474475,
      "loss": 0.0144,
      "step": 277
    },
    {
      "epoch": 25.86,
      "grad_norm": 0.16424229741096497,
      "learning_rate": 0.0014454454454454456,
      "loss": 0.0168,
      "step": 278
    },
    {
      "epoch": 25.95,
      "grad_norm": 0.10903183370828629,
      "learning_rate": 0.0014434434434434436,
      "loss": 0.0105,
      "step": 279
    },
    {
      "epoch": 26.05,
      "grad_norm": 0.14578936994075775,
      "learning_rate": 0.0014414414414414415,
      "loss": 0.0126,
      "step": 280
    },
    {
      "epoch": 26.14,
      "grad_norm": 0.12450461089611053,
      "learning_rate": 0.0014394394394394395,
      "loss": 0.0116,
      "step": 281
    },
    {
      "epoch": 26.23,
      "grad_norm": 0.1154106929898262,
      "learning_rate": 0.0014374374374374376,
      "loss": 0.0094,
      "step": 282
    },
    {
      "epoch": 26.33,
      "grad_norm": 0.12836499512195587,
      "learning_rate": 0.0014354354354354356,
      "loss": 0.0105,
      "step": 283
    },
    {
      "epoch": 26.42,
      "grad_norm": 0.11856179684400558,
      "learning_rate": 0.0014334334334334333,
      "loss": 0.0091,
      "step": 284
    },
    {
      "epoch": 26.51,
      "grad_norm": 0.12042251974344254,
      "learning_rate": 0.0014314314314314313,
      "loss": 0.0134,
      "step": 285
    },
    {
      "epoch": 26.6,
      "grad_norm": 0.13722313940525055,
      "learning_rate": 0.0014294294294294294,
      "loss": 0.0162,
      "step": 286
    },
    {
      "epoch": 26.7,
      "grad_norm": 0.11666831374168396,
      "learning_rate": 0.0014274274274274274,
      "loss": 0.0101,
      "step": 287
    },
    {
      "epoch": 26.79,
      "grad_norm": 0.11098901182413101,
      "learning_rate": 0.0014254254254254255,
      "loss": 0.0107,
      "step": 288
    },
    {
      "epoch": 26.88,
      "grad_norm": 0.1570785492658615,
      "learning_rate": 0.0014234234234234233,
      "loss": 0.0182,
      "step": 289
    },
    {
      "epoch": 26.98,
      "grad_norm": 0.11240728944540024,
      "learning_rate": 0.0014214214214214214,
      "loss": 0.0102,
      "step": 290
    },
    {
      "epoch": 27.07,
      "grad_norm": 0.11979077011346817,
      "learning_rate": 0.0014194194194194194,
      "loss": 0.0107,
      "step": 291
    },
    {
      "epoch": 27.16,
      "grad_norm": 0.06227131932973862,
      "learning_rate": 0.0014174174174174175,
      "loss": 0.0058,
      "step": 292
    },
    {
      "epoch": 27.26,
      "grad_norm": 0.10084498673677444,
      "learning_rate": 0.0014154154154154156,
      "loss": 0.0082,
      "step": 293
    },
    {
      "epoch": 27.35,
      "grad_norm": 0.09121516346931458,
      "learning_rate": 0.0014134134134134134,
      "loss": 0.0086,
      "step": 294
    },
    {
      "epoch": 27.44,
      "grad_norm": 0.08678147196769714,
      "learning_rate": 0.0014114114114114115,
      "loss": 0.0076,
      "step": 295
    },
    {
      "epoch": 27.53,
      "grad_norm": 0.10272347927093506,
      "learning_rate": 0.0014094094094094095,
      "loss": 0.0106,
      "step": 296
    },
    {
      "epoch": 27.63,
      "grad_norm": 0.07840856909751892,
      "learning_rate": 0.0014074074074074076,
      "loss": 0.0071,
      "step": 297
    },
    {
      "epoch": 27.72,
      "grad_norm": 0.10716904699802399,
      "learning_rate": 0.0014054054054054054,
      "loss": 0.009,
      "step": 298
    },
    {
      "epoch": 27.81,
      "grad_norm": 0.08777103573083878,
      "learning_rate": 0.0014034034034034032,
      "loss": 0.0077,
      "step": 299
    },
    {
      "epoch": 27.91,
      "grad_norm": 0.1003369688987732,
      "learning_rate": 0.0014014014014014013,
      "loss": 0.0074,
      "step": 300
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.07700402289628983,
      "learning_rate": 0.0013993993993993994,
      "loss": 0.006,
      "step": 301
    },
    {
      "epoch": 28.09,
      "grad_norm": 0.05425987020134926,
      "learning_rate": 0.0013973973973973974,
      "loss": 0.0035,
      "step": 302
    },
    {
      "epoch": 28.19,
      "grad_norm": 0.08867619186639786,
      "learning_rate": 0.0013953953953953955,
      "loss": 0.0074,
      "step": 303
    },
    {
      "epoch": 28.28,
      "grad_norm": 0.06515760719776154,
      "learning_rate": 0.0013933933933933933,
      "loss": 0.0047,
      "step": 304
    },
    {
      "epoch": 28.37,
      "grad_norm": 0.08021624386310577,
      "learning_rate": 0.0013913913913913914,
      "loss": 0.0061,
      "step": 305
    },
    {
      "epoch": 28.47,
      "grad_norm": 0.1081114262342453,
      "learning_rate": 0.0013893893893893894,
      "loss": 0.0103,
      "step": 306
    },
    {
      "epoch": 28.56,
      "grad_norm": 0.0835670754313469,
      "learning_rate": 0.0013873873873873875,
      "loss": 0.0055,
      "step": 307
    },
    {
      "epoch": 28.65,
      "grad_norm": 0.08508322387933731,
      "learning_rate": 0.0013853853853853855,
      "loss": 0.0073,
      "step": 308
    },
    {
      "epoch": 28.74,
      "grad_norm": 0.07496494799852371,
      "learning_rate": 0.0013833833833833834,
      "loss": 0.0053,
      "step": 309
    },
    {
      "epoch": 28.84,
      "grad_norm": 0.07663434743881226,
      "learning_rate": 0.0013813813813813814,
      "loss": 0.0067,
      "step": 310
    },
    {
      "epoch": 28.93,
      "grad_norm": 0.08244332671165466,
      "learning_rate": 0.0013793793793793795,
      "loss": 0.0058,
      "step": 311
    },
    {
      "epoch": 29.02,
      "grad_norm": 0.0868007019162178,
      "learning_rate": 0.0013773773773773776,
      "loss": 0.0069,
      "step": 312
    },
    {
      "epoch": 29.12,
      "grad_norm": 0.4052487015724182,
      "learning_rate": 0.0013753753753753754,
      "loss": 0.0052,
      "step": 313
    },
    {
      "epoch": 29.21,
      "grad_norm": 0.06757152825593948,
      "learning_rate": 0.0013733733733733732,
      "loss": 0.0038,
      "step": 314
    },
    {
      "epoch": 29.3,
      "grad_norm": 0.07421130686998367,
      "learning_rate": 0.0013713713713713713,
      "loss": 0.0061,
      "step": 315
    },
    {
      "epoch": 29.4,
      "grad_norm": 0.12837092578411102,
      "learning_rate": 0.0013693693693693693,
      "loss": 0.0085,
      "step": 316
    },
    {
      "epoch": 29.49,
      "grad_norm": 0.09021154791116714,
      "learning_rate": 0.0013673673673673674,
      "loss": 0.0085,
      "step": 317
    },
    {
      "epoch": 29.58,
      "grad_norm": 0.07567004859447479,
      "learning_rate": 0.0013653653653653655,
      "loss": 0.007,
      "step": 318
    },
    {
      "epoch": 29.67,
      "grad_norm": 0.12329138815402985,
      "learning_rate": 0.0013633633633633633,
      "loss": 0.0077,
      "step": 319
    },
    {
      "epoch": 29.77,
      "grad_norm": 0.0948009118437767,
      "learning_rate": 0.0013613613613613614,
      "loss": 0.0061,
      "step": 320
    },
    {
      "epoch": 29.86,
      "grad_norm": 0.07781707495450974,
      "learning_rate": 0.0013593593593593594,
      "loss": 0.0051,
      "step": 321
    },
    {
      "epoch": 29.95,
      "grad_norm": 0.10603152215480804,
      "learning_rate": 0.0013573573573573575,
      "loss": 0.0083,
      "step": 322
    },
    {
      "epoch": 30.05,
      "grad_norm": 0.11571641266345978,
      "learning_rate": 0.0013553553553553555,
      "loss": 0.0064,
      "step": 323
    },
    {
      "epoch": 30.14,
      "grad_norm": 0.07329671829938889,
      "learning_rate": 0.0013533533533533534,
      "loss": 0.0045,
      "step": 324
    },
    {
      "epoch": 30.23,
      "grad_norm": 0.09423427283763885,
      "learning_rate": 0.0013513513513513514,
      "loss": 0.0067,
      "step": 325
    },
    {
      "epoch": 30.33,
      "grad_norm": 0.10144127160310745,
      "learning_rate": 0.0013493493493493495,
      "loss": 0.0067,
      "step": 326
    },
    {
      "epoch": 30.42,
      "grad_norm": 0.07445558905601501,
      "learning_rate": 0.0013473473473473473,
      "loss": 0.0051,
      "step": 327
    },
    {
      "epoch": 30.51,
      "grad_norm": 0.07344485819339752,
      "learning_rate": 0.0013453453453453454,
      "loss": 0.0058,
      "step": 328
    },
    {
      "epoch": 30.6,
      "grad_norm": 0.07438346743583679,
      "learning_rate": 0.0013433433433433432,
      "loss": 0.0061,
      "step": 329
    },
    {
      "epoch": 30.7,
      "grad_norm": 0.07676655054092407,
      "learning_rate": 0.0013413413413413413,
      "loss": 0.0052,
      "step": 330
    },
    {
      "epoch": 30.79,
      "grad_norm": 0.0767897292971611,
      "learning_rate": 0.0013393393393393393,
      "loss": 0.0064,
      "step": 331
    },
    {
      "epoch": 30.88,
      "grad_norm": 0.07645553350448608,
      "learning_rate": 0.0013373373373373374,
      "loss": 0.0062,
      "step": 332
    },
    {
      "epoch": 30.98,
      "grad_norm": 0.09462464600801468,
      "learning_rate": 0.0013353353353353354,
      "loss": 0.0077,
      "step": 333
    },
    {
      "epoch": 31.07,
      "grad_norm": 0.049924131482839584,
      "learning_rate": 0.0013333333333333333,
      "loss": 0.0036,
      "step": 334
    },
    {
      "epoch": 31.16,
      "grad_norm": 0.07813724875450134,
      "learning_rate": 0.0013313313313313313,
      "loss": 0.0047,
      "step": 335
    },
    {
      "epoch": 31.26,
      "grad_norm": 0.09082891792058945,
      "learning_rate": 0.0013293293293293294,
      "loss": 0.0046,
      "step": 336
    },
    {
      "epoch": 31.35,
      "grad_norm": 0.0659075453877449,
      "learning_rate": 0.0013273273273273275,
      "loss": 0.0047,
      "step": 337
    },
    {
      "epoch": 31.44,
      "grad_norm": 0.04118886962532997,
      "learning_rate": 0.0013253253253253255,
      "loss": 0.0027,
      "step": 338
    },
    {
      "epoch": 31.53,
      "grad_norm": 0.06225672736763954,
      "learning_rate": 0.0013233233233233234,
      "loss": 0.0039,
      "step": 339
    },
    {
      "epoch": 31.63,
      "grad_norm": 0.05968526005744934,
      "learning_rate": 0.0013213213213213214,
      "loss": 0.0043,
      "step": 340
    },
    {
      "epoch": 31.72,
      "grad_norm": 0.06615372747182846,
      "learning_rate": 0.0013193193193193195,
      "loss": 0.0047,
      "step": 341
    },
    {
      "epoch": 31.81,
      "grad_norm": 0.06672164797782898,
      "learning_rate": 0.0013173173173173173,
      "loss": 0.0053,
      "step": 342
    },
    {
      "epoch": 31.91,
      "grad_norm": 0.05841416120529175,
      "learning_rate": 0.0013153153153153154,
      "loss": 0.0035,
      "step": 343
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.07656476646661758,
      "learning_rate": 0.0013133133133133132,
      "loss": 0.0058,
      "step": 344
    },
    {
      "epoch": 32.09,
      "grad_norm": 0.04063386842608452,
      "learning_rate": 0.0013113113113113113,
      "loss": 0.0024,
      "step": 345
    },
    {
      "epoch": 32.19,
      "grad_norm": 0.04993503540754318,
      "learning_rate": 0.0013093093093093093,
      "loss": 0.0034,
      "step": 346
    },
    {
      "epoch": 32.28,
      "grad_norm": 0.05899167060852051,
      "learning_rate": 0.0013073073073073074,
      "loss": 0.003,
      "step": 347
    },
    {
      "epoch": 32.37,
      "grad_norm": 0.03621702641248703,
      "learning_rate": 0.0013053053053053052,
      "loss": 0.0021,
      "step": 348
    },
    {
      "epoch": 32.47,
      "grad_norm": 0.04735443741083145,
      "learning_rate": 0.0013033033033033033,
      "loss": 0.0038,
      "step": 349
    },
    {
      "epoch": 32.56,
      "grad_norm": 0.047074075788259506,
      "learning_rate": 0.0013013013013013013,
      "loss": 0.0031,
      "step": 350
    },
    {
      "epoch": 32.65,
      "grad_norm": 0.04885280877351761,
      "learning_rate": 0.0012992992992992994,
      "loss": 0.0047,
      "step": 351
    },
    {
      "epoch": 32.74,
      "grad_norm": 0.06170407682657242,
      "learning_rate": 0.0012972972972972974,
      "loss": 0.0042,
      "step": 352
    },
    {
      "epoch": 32.84,
      "grad_norm": 0.06180822476744652,
      "learning_rate": 0.0012952952952952953,
      "loss": 0.0037,
      "step": 353
    },
    {
      "epoch": 32.93,
      "grad_norm": 0.04855296388268471,
      "learning_rate": 0.0012932932932932933,
      "loss": 0.0032,
      "step": 354
    },
    {
      "epoch": 33.02,
      "grad_norm": 0.05172926187515259,
      "learning_rate": 0.0012912912912912914,
      "loss": 0.0039,
      "step": 355
    },
    {
      "epoch": 33.12,
      "grad_norm": 0.03115912713110447,
      "learning_rate": 0.0012892892892892892,
      "loss": 0.0017,
      "step": 356
    },
    {
      "epoch": 33.21,
      "grad_norm": 0.03118099458515644,
      "learning_rate": 0.0012872872872872873,
      "loss": 0.0017,
      "step": 357
    },
    {
      "epoch": 33.3,
      "grad_norm": 0.09290407598018646,
      "learning_rate": 0.0012852852852852851,
      "loss": 0.0027,
      "step": 358
    },
    {
      "epoch": 33.4,
      "grad_norm": 0.062028996646404266,
      "learning_rate": 0.0012832832832832832,
      "loss": 0.003,
      "step": 359
    },
    {
      "epoch": 33.49,
      "grad_norm": 0.06242119520902634,
      "learning_rate": 0.0012812812812812813,
      "loss": 0.0037,
      "step": 360
    },
    {
      "epoch": 33.58,
      "grad_norm": 0.04311477020382881,
      "learning_rate": 0.0012792792792792793,
      "loss": 0.0024,
      "step": 361
    },
    {
      "epoch": 33.67,
      "grad_norm": 0.04930288344621658,
      "learning_rate": 0.0012772772772772774,
      "loss": 0.0025,
      "step": 362
    },
    {
      "epoch": 33.77,
      "grad_norm": 0.03412387892603874,
      "learning_rate": 0.0012752752752752752,
      "loss": 0.0026,
      "step": 363
    },
    {
      "epoch": 33.86,
      "grad_norm": 0.08664079010486603,
      "learning_rate": 0.0012732732732732733,
      "loss": 0.0066,
      "step": 364
    },
    {
      "epoch": 33.95,
      "grad_norm": 0.8719766736030579,
      "learning_rate": 0.0012712712712712713,
      "loss": 0.0064,
      "step": 365
    },
    {
      "epoch": 34.05,
      "grad_norm": 0.07285726815462112,
      "learning_rate": 0.0012692692692692694,
      "loss": 0.0034,
      "step": 366
    },
    {
      "epoch": 34.14,
      "grad_norm": 0.16661174595355988,
      "learning_rate": 0.0012672672672672674,
      "loss": 0.0131,
      "step": 367
    },
    {
      "epoch": 34.23,
      "grad_norm": 0.21597643196582794,
      "learning_rate": 0.0012652652652652653,
      "loss": 0.016,
      "step": 368
    },
    {
      "epoch": 34.33,
      "grad_norm": 0.19439759850502014,
      "learning_rate": 0.0012632632632632633,
      "loss": 0.0158,
      "step": 369
    },
    {
      "epoch": 34.42,
      "grad_norm": 0.11315418034791946,
      "learning_rate": 0.0012612612612612614,
      "loss": 0.0062,
      "step": 370
    },
    {
      "epoch": 34.51,
      "grad_norm": 0.10819867253303528,
      "learning_rate": 0.0012592592592592592,
      "loss": 0.0072,
      "step": 371
    },
    {
      "epoch": 34.6,
      "grad_norm": 0.11476534605026245,
      "learning_rate": 0.0012572572572572573,
      "loss": 0.008,
      "step": 372
    },
    {
      "epoch": 34.7,
      "grad_norm": 0.13075686991214752,
      "learning_rate": 0.0012552552552552551,
      "loss": 0.0091,
      "step": 373
    },
    {
      "epoch": 34.79,
      "grad_norm": 1.8338048458099365,
      "learning_rate": 0.0012532532532532532,
      "loss": 0.0338,
      "step": 374
    },
    {
      "epoch": 34.88,
      "grad_norm": 0.18288426101207733,
      "learning_rate": 0.0012512512512512512,
      "loss": 0.0177,
      "step": 375
    },
    {
      "epoch": 34.98,
      "grad_norm": 0.2959078252315521,
      "learning_rate": 0.0012492492492492493,
      "loss": 0.0319,
      "step": 376
    },
    {
      "epoch": 35.07,
      "grad_norm": 0.2056169956922531,
      "learning_rate": 0.0012472472472472474,
      "loss": 0.0208,
      "step": 377
    },
    {
      "epoch": 35.16,
      "grad_norm": 0.21074797213077545,
      "learning_rate": 0.0012452452452452452,
      "loss": 0.0233,
      "step": 378
    },
    {
      "epoch": 35.26,
      "grad_norm": 0.15737591683864594,
      "learning_rate": 0.0012432432432432433,
      "loss": 0.0172,
      "step": 379
    },
    {
      "epoch": 35.35,
      "grad_norm": 0.17381712794303894,
      "learning_rate": 0.0012412412412412413,
      "loss": 0.0199,
      "step": 380
    },
    {
      "epoch": 35.44,
      "grad_norm": 0.17823565006256104,
      "learning_rate": 0.0012392392392392394,
      "loss": 0.0206,
      "step": 381
    },
    {
      "epoch": 35.53,
      "grad_norm": 0.15753379464149475,
      "learning_rate": 0.0012372372372372374,
      "loss": 0.0186,
      "step": 382
    },
    {
      "epoch": 35.63,
      "grad_norm": 0.11811896413564682,
      "learning_rate": 0.0012352352352352353,
      "loss": 0.0135,
      "step": 383
    },
    {
      "epoch": 35.72,
      "grad_norm": 0.15179665386676788,
      "learning_rate": 0.0012332332332332333,
      "loss": 0.0156,
      "step": 384
    },
    {
      "epoch": 35.81,
      "grad_norm": 0.156512051820755,
      "learning_rate": 0.0012312312312312312,
      "loss": 0.0117,
      "step": 385
    },
    {
      "epoch": 35.91,
      "grad_norm": 0.18240876495838165,
      "learning_rate": 0.0012292292292292292,
      "loss": 0.0184,
      "step": 386
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.12744146585464478,
      "learning_rate": 0.0012272272272272273,
      "loss": 0.0104,
      "step": 387
    },
    {
      "epoch": 36.09,
      "grad_norm": 0.11735134571790695,
      "learning_rate": 0.0012252252252252251,
      "loss": 0.0088,
      "step": 388
    },
    {
      "epoch": 36.19,
      "grad_norm": 0.10435677319765091,
      "learning_rate": 0.0012232232232232232,
      "loss": 0.0089,
      "step": 389
    },
    {
      "epoch": 36.28,
      "grad_norm": 0.1069398745894432,
      "learning_rate": 0.0012212212212212212,
      "loss": 0.0074,
      "step": 390
    },
    {
      "epoch": 36.37,
      "grad_norm": 0.20668792724609375,
      "learning_rate": 0.0012192192192192193,
      "loss": 0.0113,
      "step": 391
    },
    {
      "epoch": 36.47,
      "grad_norm": 0.12832938134670258,
      "learning_rate": 0.0012172172172172173,
      "loss": 0.013,
      "step": 392
    },
    {
      "epoch": 36.56,
      "grad_norm": 0.13163897395133972,
      "learning_rate": 0.0012152152152152152,
      "loss": 0.0102,
      "step": 393
    },
    {
      "epoch": 36.65,
      "grad_norm": 0.14289605617523193,
      "learning_rate": 0.0012132132132132132,
      "loss": 0.0135,
      "step": 394
    },
    {
      "epoch": 36.74,
      "grad_norm": 0.1352647840976715,
      "learning_rate": 0.0012112112112112113,
      "loss": 0.0131,
      "step": 395
    },
    {
      "epoch": 36.84,
      "grad_norm": 0.1026504635810852,
      "learning_rate": 0.0012092092092092094,
      "loss": 0.0091,
      "step": 396
    },
    {
      "epoch": 36.93,
      "grad_norm": 0.13999910652637482,
      "learning_rate": 0.0012072072072072074,
      "loss": 0.012,
      "step": 397
    },
    {
      "epoch": 37.02,
      "grad_norm": 0.13585957884788513,
      "learning_rate": 0.0012052052052052053,
      "loss": 0.0086,
      "step": 398
    },
    {
      "epoch": 37.12,
      "grad_norm": 0.06898730993270874,
      "learning_rate": 0.0012032032032032033,
      "loss": 0.0044,
      "step": 399
    },
    {
      "epoch": 37.21,
      "grad_norm": 0.06994560360908508,
      "learning_rate": 0.0012012012012012011,
      "loss": 0.0051,
      "step": 400
    },
    {
      "epoch": 37.3,
      "grad_norm": 0.1387774795293808,
      "learning_rate": 0.0011991991991991992,
      "loss": 0.0112,
      "step": 401
    },
    {
      "epoch": 37.4,
      "grad_norm": 0.08667416870594025,
      "learning_rate": 0.0011971971971971973,
      "loss": 0.0088,
      "step": 402
    },
    {
      "epoch": 37.49,
      "grad_norm": 0.08598697185516357,
      "learning_rate": 0.001195195195195195,
      "loss": 0.008,
      "step": 403
    },
    {
      "epoch": 37.58,
      "grad_norm": 0.0907205268740654,
      "learning_rate": 0.0011931931931931932,
      "loss": 0.0062,
      "step": 404
    },
    {
      "epoch": 37.67,
      "grad_norm": 0.08570162951946259,
      "learning_rate": 0.0011911911911911912,
      "loss": 0.0084,
      "step": 405
    },
    {
      "epoch": 37.77,
      "grad_norm": 0.10218426585197449,
      "learning_rate": 0.0011891891891891893,
      "loss": 0.0076,
      "step": 406
    },
    {
      "epoch": 37.86,
      "grad_norm": 0.10739883780479431,
      "learning_rate": 0.0011871871871871871,
      "loss": 0.0098,
      "step": 407
    },
    {
      "epoch": 37.95,
      "grad_norm": 0.11295542865991592,
      "learning_rate": 0.0011851851851851852,
      "loss": 0.0096,
      "step": 408
    },
    {
      "epoch": 38.05,
      "grad_norm": 0.0802343562245369,
      "learning_rate": 0.0011831831831831832,
      "loss": 0.0056,
      "step": 409
    },
    {
      "epoch": 38.14,
      "grad_norm": 0.09044903516769409,
      "learning_rate": 0.0011811811811811813,
      "loss": 0.0061,
      "step": 410
    },
    {
      "epoch": 38.23,
      "grad_norm": 0.08450707793235779,
      "learning_rate": 0.0011791791791791793,
      "loss": 0.006,
      "step": 411
    },
    {
      "epoch": 38.33,
      "grad_norm": 0.05670813471078873,
      "learning_rate": 0.0011771771771771772,
      "loss": 0.0041,
      "step": 412
    },
    {
      "epoch": 38.42,
      "grad_norm": 0.0767657682299614,
      "learning_rate": 0.0011751751751751752,
      "loss": 0.0059,
      "step": 413
    },
    {
      "epoch": 38.51,
      "grad_norm": 0.08444706350564957,
      "learning_rate": 0.001173173173173173,
      "loss": 0.006,
      "step": 414
    },
    {
      "epoch": 38.6,
      "grad_norm": 0.08652391284704208,
      "learning_rate": 0.0011711711711711711,
      "loss": 0.006,
      "step": 415
    },
    {
      "epoch": 38.7,
      "grad_norm": 0.09136863797903061,
      "learning_rate": 0.0011691691691691692,
      "loss": 0.0057,
      "step": 416
    },
    {
      "epoch": 38.79,
      "grad_norm": 0.083625428378582,
      "learning_rate": 0.001167167167167167,
      "loss": 0.0076,
      "step": 417
    },
    {
      "epoch": 38.88,
      "grad_norm": 0.07047466188669205,
      "learning_rate": 0.001165165165165165,
      "loss": 0.0042,
      "step": 418
    },
    {
      "epoch": 38.98,
      "grad_norm": 0.07656373828649521,
      "learning_rate": 0.0011631631631631631,
      "loss": 0.0057,
      "step": 419
    },
    {
      "epoch": 39.07,
      "grad_norm": 0.06967911869287491,
      "learning_rate": 0.0011611611611611612,
      "loss": 0.0036,
      "step": 420
    },
    {
      "epoch": 39.16,
      "grad_norm": 0.051445458084344864,
      "learning_rate": 0.0011591591591591593,
      "loss": 0.0028,
      "step": 421
    },
    {
      "epoch": 39.26,
      "grad_norm": 0.0559643991291523,
      "learning_rate": 0.001157157157157157,
      "loss": 0.0032,
      "step": 422
    },
    {
      "epoch": 39.35,
      "grad_norm": 0.03927305340766907,
      "learning_rate": 0.0011551551551551552,
      "loss": 0.0023,
      "step": 423
    },
    {
      "epoch": 39.44,
      "grad_norm": 0.057706110179424286,
      "learning_rate": 0.0011531531531531532,
      "loss": 0.0031,
      "step": 424
    },
    {
      "epoch": 39.53,
      "grad_norm": 0.06381837278604507,
      "learning_rate": 0.0011511511511511513,
      "loss": 0.0032,
      "step": 425
    },
    {
      "epoch": 39.63,
      "grad_norm": 0.05114300921559334,
      "learning_rate": 0.0011491491491491493,
      "loss": 0.0021,
      "step": 426
    },
    {
      "epoch": 39.72,
      "grad_norm": 0.07307816296815872,
      "learning_rate": 0.0011471471471471472,
      "loss": 0.0039,
      "step": 427
    },
    {
      "epoch": 39.81,
      "grad_norm": 0.05135943368077278,
      "learning_rate": 0.0011451451451451452,
      "loss": 0.0039,
      "step": 428
    },
    {
      "epoch": 39.91,
      "grad_norm": 0.0514879934489727,
      "learning_rate": 0.001143143143143143,
      "loss": 0.0026,
      "step": 429
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.08439819514751434,
      "learning_rate": 0.0011411411411411411,
      "loss": 0.0038,
      "step": 430
    },
    {
      "epoch": 40.09,
      "grad_norm": 0.036230411380529404,
      "learning_rate": 0.0011391391391391392,
      "loss": 0.0016,
      "step": 431
    },
    {
      "epoch": 40.19,
      "grad_norm": 0.06861860305070877,
      "learning_rate": 0.001137137137137137,
      "loss": 0.0031,
      "step": 432
    },
    {
      "epoch": 40.28,
      "grad_norm": 0.03462895378470421,
      "learning_rate": 0.001135135135135135,
      "loss": 0.0015,
      "step": 433
    },
    {
      "epoch": 40.37,
      "grad_norm": 0.033459682017564774,
      "learning_rate": 0.0011331331331331331,
      "loss": 0.0016,
      "step": 434
    },
    {
      "epoch": 40.47,
      "grad_norm": 0.05087810382246971,
      "learning_rate": 0.0011311311311311312,
      "loss": 0.003,
      "step": 435
    },
    {
      "epoch": 40.56,
      "grad_norm": 0.06251133978366852,
      "learning_rate": 0.0011291291291291292,
      "loss": 0.0033,
      "step": 436
    },
    {
      "epoch": 40.65,
      "grad_norm": 0.057716287672519684,
      "learning_rate": 0.001127127127127127,
      "loss": 0.003,
      "step": 437
    },
    {
      "epoch": 40.74,
      "grad_norm": 0.04524217173457146,
      "learning_rate": 0.0011251251251251251,
      "loss": 0.0021,
      "step": 438
    },
    {
      "epoch": 40.84,
      "grad_norm": 0.05485425889492035,
      "learning_rate": 0.0011231231231231232,
      "loss": 0.0023,
      "step": 439
    },
    {
      "epoch": 40.93,
      "grad_norm": 0.04494103044271469,
      "learning_rate": 0.0011211211211211213,
      "loss": 0.002,
      "step": 440
    },
    {
      "epoch": 41.02,
      "grad_norm": 0.06474560499191284,
      "learning_rate": 0.0011191191191191193,
      "loss": 0.0031,
      "step": 441
    },
    {
      "epoch": 41.12,
      "grad_norm": 0.032112184911966324,
      "learning_rate": 0.0011171171171171172,
      "loss": 0.0015,
      "step": 442
    },
    {
      "epoch": 41.21,
      "grad_norm": 0.07855093479156494,
      "learning_rate": 0.001115115115115115,
      "loss": 0.003,
      "step": 443
    },
    {
      "epoch": 41.3,
      "grad_norm": 0.05911662057042122,
      "learning_rate": 0.001113113113113113,
      "loss": 0.0024,
      "step": 444
    },
    {
      "epoch": 41.4,
      "grad_norm": 0.07458988577127457,
      "learning_rate": 0.0011111111111111111,
      "loss": 0.0019,
      "step": 445
    },
    {
      "epoch": 41.49,
      "grad_norm": 0.06002940610051155,
      "learning_rate": 0.0011091091091091092,
      "loss": 0.0029,
      "step": 446
    },
    {
      "epoch": 41.58,
      "grad_norm": 0.04031607136130333,
      "learning_rate": 0.001107107107107107,
      "loss": 0.0016,
      "step": 447
    },
    {
      "epoch": 41.67,
      "grad_norm": 0.05871531367301941,
      "learning_rate": 0.001105105105105105,
      "loss": 0.0029,
      "step": 448
    },
    {
      "epoch": 41.77,
      "grad_norm": 0.06386204808950424,
      "learning_rate": 0.0011031031031031031,
      "loss": 0.0022,
      "step": 449
    },
    {
      "epoch": 41.86,
      "grad_norm": 0.031453412026166916,
      "learning_rate": 0.0011011011011011012,
      "loss": 0.0017,
      "step": 450
    },
    {
      "epoch": 41.95,
      "grad_norm": 0.0686318650841713,
      "learning_rate": 0.0010990990990990992,
      "loss": 0.005,
      "step": 451
    },
    {
      "epoch": 42.05,
      "grad_norm": 0.044364456087350845,
      "learning_rate": 0.001097097097097097,
      "loss": 0.0025,
      "step": 452
    },
    {
      "epoch": 42.14,
      "grad_norm": 0.07827119529247284,
      "learning_rate": 0.0010950950950950951,
      "loss": 0.004,
      "step": 453
    },
    {
      "epoch": 42.23,
      "grad_norm": 0.01674993522465229,
      "learning_rate": 0.0010930930930930932,
      "loss": 0.0013,
      "step": 454
    },
    {
      "epoch": 42.33,
      "grad_norm": 0.041024889796972275,
      "learning_rate": 0.0010910910910910912,
      "loss": 0.002,
      "step": 455
    },
    {
      "epoch": 42.42,
      "grad_norm": 0.04098690673708916,
      "learning_rate": 0.0010890890890890893,
      "loss": 0.0023,
      "step": 456
    },
    {
      "epoch": 42.51,
      "grad_norm": 0.05196992680430412,
      "learning_rate": 0.001087087087087087,
      "loss": 0.0024,
      "step": 457
    },
    {
      "epoch": 42.6,
      "grad_norm": 0.038588881492614746,
      "learning_rate": 0.001085085085085085,
      "loss": 0.0017,
      "step": 458
    },
    {
      "epoch": 42.7,
      "grad_norm": 0.044969286769628525,
      "learning_rate": 0.001083083083083083,
      "loss": 0.0023,
      "step": 459
    },
    {
      "epoch": 42.79,
      "grad_norm": 0.07375585287809372,
      "learning_rate": 0.001081081081081081,
      "loss": 0.0026,
      "step": 460
    },
    {
      "epoch": 42.88,
      "grad_norm": 0.0510486401617527,
      "learning_rate": 0.0010790790790790792,
      "loss": 0.003,
      "step": 461
    },
    {
      "epoch": 42.98,
      "grad_norm": 0.04110394045710564,
      "learning_rate": 0.001077077077077077,
      "loss": 0.0032,
      "step": 462
    },
    {
      "epoch": 43.07,
      "grad_norm": 0.024054836481809616,
      "learning_rate": 0.001075075075075075,
      "loss": 0.002,
      "step": 463
    },
    {
      "epoch": 43.16,
      "grad_norm": 0.03442402184009552,
      "learning_rate": 0.0010730730730730731,
      "loss": 0.0015,
      "step": 464
    },
    {
      "epoch": 43.26,
      "grad_norm": 0.03139084577560425,
      "learning_rate": 0.0010710710710710712,
      "loss": 0.0018,
      "step": 465
    },
    {
      "epoch": 43.35,
      "grad_norm": 0.04958639666438103,
      "learning_rate": 0.0010690690690690692,
      "loss": 0.0026,
      "step": 466
    },
    {
      "epoch": 43.44,
      "grad_norm": 0.03323910012841225,
      "learning_rate": 0.001067067067067067,
      "loss": 0.0016,
      "step": 467
    },
    {
      "epoch": 43.53,
      "grad_norm": 0.038213685154914856,
      "learning_rate": 0.0010650650650650651,
      "loss": 0.0019,
      "step": 468
    },
    {
      "epoch": 43.63,
      "grad_norm": 0.07056744396686554,
      "learning_rate": 0.0010630630630630632,
      "loss": 0.0022,
      "step": 469
    },
    {
      "epoch": 43.72,
      "grad_norm": 0.03680761158466339,
      "learning_rate": 0.0010610610610610612,
      "loss": 0.0014,
      "step": 470
    },
    {
      "epoch": 43.81,
      "grad_norm": 0.05797829478979111,
      "learning_rate": 0.001059059059059059,
      "loss": 0.0031,
      "step": 471
    },
    {
      "epoch": 43.91,
      "grad_norm": 0.03306601941585541,
      "learning_rate": 0.001057057057057057,
      "loss": 0.0023,
      "step": 472
    },
    {
      "epoch": 44.0,
      "grad_norm": 0.03531021997332573,
      "learning_rate": 0.001055055055055055,
      "loss": 0.0029,
      "step": 473
    },
    {
      "epoch": 44.09,
      "grad_norm": 0.03529014438390732,
      "learning_rate": 0.001053053053053053,
      "loss": 0.0018,
      "step": 474
    },
    {
      "epoch": 44.19,
      "grad_norm": 0.029707856476306915,
      "learning_rate": 0.001051051051051051,
      "loss": 0.0013,
      "step": 475
    },
    {
      "epoch": 44.28,
      "grad_norm": 0.038505978882312775,
      "learning_rate": 0.001049049049049049,
      "loss": 0.0019,
      "step": 476
    },
    {
      "epoch": 44.37,
      "grad_norm": 0.03348147124052048,
      "learning_rate": 0.001047047047047047,
      "loss": 0.0016,
      "step": 477
    },
    {
      "epoch": 44.47,
      "grad_norm": 0.03438611328601837,
      "learning_rate": 0.001045045045045045,
      "loss": 0.0018,
      "step": 478
    },
    {
      "epoch": 44.56,
      "grad_norm": 0.041374336928129196,
      "learning_rate": 0.001043043043043043,
      "loss": 0.0023,
      "step": 479
    },
    {
      "epoch": 44.65,
      "grad_norm": 0.03722222149372101,
      "learning_rate": 0.0010410410410410412,
      "loss": 0.0021,
      "step": 480
    },
    {
      "epoch": 44.74,
      "grad_norm": 0.027286631986498833,
      "learning_rate": 0.001039039039039039,
      "loss": 0.0016,
      "step": 481
    },
    {
      "epoch": 44.84,
      "grad_norm": 0.016527727246284485,
      "learning_rate": 0.001037037037037037,
      "loss": 0.0012,
      "step": 482
    },
    {
      "epoch": 44.93,
      "grad_norm": 0.06069435924291611,
      "learning_rate": 0.001035035035035035,
      "loss": 0.0033,
      "step": 483
    },
    {
      "epoch": 45.02,
      "grad_norm": 0.04517413303256035,
      "learning_rate": 0.0010330330330330332,
      "loss": 0.0024,
      "step": 484
    },
    {
      "epoch": 45.12,
      "grad_norm": 0.024984322488307953,
      "learning_rate": 0.0010310310310310312,
      "loss": 0.0013,
      "step": 485
    },
    {
      "epoch": 45.21,
      "grad_norm": 0.03412657603621483,
      "learning_rate": 0.0010290290290290288,
      "loss": 0.0015,
      "step": 486
    },
    {
      "epoch": 45.3,
      "grad_norm": 0.022605758160352707,
      "learning_rate": 0.001027027027027027,
      "loss": 0.0015,
      "step": 487
    },
    {
      "epoch": 45.4,
      "grad_norm": 0.034697774797677994,
      "learning_rate": 0.001025025025025025,
      "loss": 0.0019,
      "step": 488
    },
    {
      "epoch": 45.49,
      "grad_norm": 0.031802624464035034,
      "learning_rate": 0.001023023023023023,
      "loss": 0.0017,
      "step": 489
    },
    {
      "epoch": 45.58,
      "grad_norm": 0.04652908816933632,
      "learning_rate": 0.001021021021021021,
      "loss": 0.0015,
      "step": 490
    },
    {
      "epoch": 45.67,
      "grad_norm": 0.011993377469480038,
      "learning_rate": 0.001019019019019019,
      "loss": 0.0012,
      "step": 491
    },
    {
      "epoch": 45.77,
      "grad_norm": 0.062344640493392944,
      "learning_rate": 0.001017017017017017,
      "loss": 0.0019,
      "step": 492
    },
    {
      "epoch": 45.86,
      "grad_norm": 0.030106155201792717,
      "learning_rate": 0.001015015015015015,
      "loss": 0.0016,
      "step": 493
    },
    {
      "epoch": 45.95,
      "grad_norm": 0.04093213379383087,
      "learning_rate": 0.001013013013013013,
      "loss": 0.0018,
      "step": 494
    },
    {
      "epoch": 46.05,
      "grad_norm": 0.02282726764678955,
      "learning_rate": 0.0010110110110110111,
      "loss": 0.0015,
      "step": 495
    },
    {
      "epoch": 46.14,
      "grad_norm": 0.04156970977783203,
      "learning_rate": 0.001009009009009009,
      "loss": 0.0015,
      "step": 496
    },
    {
      "epoch": 46.23,
      "grad_norm": 0.030880684033036232,
      "learning_rate": 0.001007007007007007,
      "loss": 0.0027,
      "step": 497
    },
    {
      "epoch": 46.33,
      "grad_norm": 0.042682062834501266,
      "learning_rate": 0.001005005005005005,
      "loss": 0.0025,
      "step": 498
    },
    {
      "epoch": 46.42,
      "grad_norm": 0.009067803621292114,
      "learning_rate": 0.0010030030030030032,
      "loss": 0.0012,
      "step": 499
    },
    {
      "epoch": 46.51,
      "grad_norm": 0.014194684103131294,
      "learning_rate": 0.0010010010010010012,
      "loss": 0.0011,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 500,
  "total_flos": 1.3026458020244275e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
