{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 93.02325581395348,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09,
      "grad_norm": 0.2204626500606537,
      "learning_rate": 0.002,
      "loss": 0.6248,
      "step": 1
    },
    {
      "epoch": 0.19,
      "grad_norm": 0.11693359166383743,
      "learning_rate": 0.001997997997997998,
      "loss": 0.6299,
      "step": 2
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.38200682401657104,
      "learning_rate": 0.001995995995995996,
      "loss": 0.596,
      "step": 3
    },
    {
      "epoch": 0.37,
      "grad_norm": 0.9068372249603271,
      "learning_rate": 0.0019939939939939938,
      "loss": 0.7228,
      "step": 4
    },
    {
      "epoch": 0.47,
      "grad_norm": 0.4440826177597046,
      "learning_rate": 0.001991991991991992,
      "loss": 0.6225,
      "step": 5
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.29924121499061584,
      "learning_rate": 0.00198998998998999,
      "loss": 0.5211,
      "step": 6
    },
    {
      "epoch": 0.65,
      "grad_norm": 0.5933983325958252,
      "learning_rate": 0.001987987987987988,
      "loss": 0.5388,
      "step": 7
    },
    {
      "epoch": 0.74,
      "grad_norm": 0.2977130115032196,
      "learning_rate": 0.001985985985985986,
      "loss": 0.5192,
      "step": 8
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.4721057713031769,
      "learning_rate": 0.001983983983983984,
      "loss": 0.5086,
      "step": 9
    },
    {
      "epoch": 0.93,
      "grad_norm": 0.57562255859375,
      "learning_rate": 0.001981981981981982,
      "loss": 0.5412,
      "step": 10
    },
    {
      "epoch": 1.02,
      "grad_norm": 0.5603854656219482,
      "learning_rate": 0.00197997997997998,
      "loss": 0.4778,
      "step": 11
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.3974635601043701,
      "learning_rate": 0.0019779779779779782,
      "loss": 0.4624,
      "step": 12
    },
    {
      "epoch": 1.21,
      "grad_norm": 0.45064789056777954,
      "learning_rate": 0.0019759759759759763,
      "loss": 0.4669,
      "step": 13
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.6731711626052856,
      "learning_rate": 0.001973973973973974,
      "loss": 0.4701,
      "step": 14
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.18675543367862701,
      "learning_rate": 0.001971971971971972,
      "loss": 0.4395,
      "step": 15
    },
    {
      "epoch": 1.49,
      "grad_norm": 0.16011585295200348,
      "learning_rate": 0.00196996996996997,
      "loss": 0.416,
      "step": 16
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.10524789243936539,
      "learning_rate": 0.001967967967967968,
      "loss": 0.4102,
      "step": 17
    },
    {
      "epoch": 1.67,
      "grad_norm": 0.20033268630504608,
      "learning_rate": 0.001965965965965966,
      "loss": 0.4121,
      "step": 18
    },
    {
      "epoch": 1.77,
      "grad_norm": 0.11050759255886078,
      "learning_rate": 0.0019639639639639638,
      "loss": 0.4064,
      "step": 19
    },
    {
      "epoch": 1.86,
      "grad_norm": 0.13043314218521118,
      "learning_rate": 0.001961961961961962,
      "loss": 0.3862,
      "step": 20
    },
    {
      "epoch": 1.95,
      "grad_norm": 0.14499731361865997,
      "learning_rate": 0.00195995995995996,
      "loss": 0.4014,
      "step": 21
    },
    {
      "epoch": 2.05,
      "grad_norm": 0.1966066062450409,
      "learning_rate": 0.001957957957957958,
      "loss": 0.3537,
      "step": 22
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.21345072984695435,
      "learning_rate": 0.001955955955955956,
      "loss": 0.3725,
      "step": 23
    },
    {
      "epoch": 2.23,
      "grad_norm": 0.3094753324985504,
      "learning_rate": 0.001953953953953954,
      "loss": 0.3845,
      "step": 24
    },
    {
      "epoch": 2.33,
      "grad_norm": 0.31922733783721924,
      "learning_rate": 0.001951951951951952,
      "loss": 0.376,
      "step": 25
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.40778297185897827,
      "learning_rate": 0.00194994994994995,
      "loss": 0.3701,
      "step": 26
    },
    {
      "epoch": 2.51,
      "grad_norm": 0.19390186667442322,
      "learning_rate": 0.001947947947947948,
      "loss": 0.327,
      "step": 27
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.24013637006282806,
      "learning_rate": 0.001945945945945946,
      "loss": 0.3183,
      "step": 28
    },
    {
      "epoch": 2.7,
      "grad_norm": 0.16304755210876465,
      "learning_rate": 0.001943943943943944,
      "loss": 0.329,
      "step": 29
    },
    {
      "epoch": 2.79,
      "grad_norm": 0.16777722537517548,
      "learning_rate": 0.001941941941941942,
      "loss": 0.338,
      "step": 30
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.17272473871707916,
      "learning_rate": 0.00193993993993994,
      "loss": 0.3246,
      "step": 31
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.21011485159397125,
      "learning_rate": 0.001937937937937938,
      "loss": 0.3276,
      "step": 32
    },
    {
      "epoch": 3.07,
      "grad_norm": 0.18070094287395477,
      "learning_rate": 0.001935935935935936,
      "loss": 0.2986,
      "step": 33
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.1892654299736023,
      "learning_rate": 0.001933933933933934,
      "loss": 0.2888,
      "step": 34
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.14621806144714355,
      "learning_rate": 0.001931931931931932,
      "loss": 0.2671,
      "step": 35
    },
    {
      "epoch": 3.35,
      "grad_norm": 0.1926691234111786,
      "learning_rate": 0.00192992992992993,
      "loss": 0.286,
      "step": 36
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.1583983302116394,
      "learning_rate": 0.0019279279279279281,
      "loss": 0.2561,
      "step": 37
    },
    {
      "epoch": 3.53,
      "grad_norm": 0.22915111482143402,
      "learning_rate": 0.0019259259259259258,
      "loss": 0.2626,
      "step": 38
    },
    {
      "epoch": 3.63,
      "grad_norm": 0.17812588810920715,
      "learning_rate": 0.0019239239239239238,
      "loss": 0.2524,
      "step": 39
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.13349206745624542,
      "learning_rate": 0.0019219219219219219,
      "loss": 0.2265,
      "step": 40
    },
    {
      "epoch": 3.81,
      "grad_norm": 0.2258288711309433,
      "learning_rate": 0.00191991991991992,
      "loss": 0.2535,
      "step": 41
    },
    {
      "epoch": 3.91,
      "grad_norm": 0.173696830868721,
      "learning_rate": 0.001917917917917918,
      "loss": 0.2423,
      "step": 42
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.23605255782604218,
      "learning_rate": 0.0019159159159159158,
      "loss": 0.2315,
      "step": 43
    },
    {
      "epoch": 4.09,
      "grad_norm": 0.20791642367839813,
      "learning_rate": 0.0019139139139139139,
      "loss": 0.1886,
      "step": 44
    },
    {
      "epoch": 4.19,
      "grad_norm": 0.23653602600097656,
      "learning_rate": 0.001911911911911912,
      "loss": 0.1805,
      "step": 45
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.2770611643791199,
      "learning_rate": 0.00190990990990991,
      "loss": 0.1898,
      "step": 46
    },
    {
      "epoch": 4.37,
      "grad_norm": 0.26397013664245605,
      "learning_rate": 0.001907907907907908,
      "loss": 0.1717,
      "step": 47
    },
    {
      "epoch": 4.47,
      "grad_norm": 0.26032182574272156,
      "learning_rate": 0.001905905905905906,
      "loss": 0.1726,
      "step": 48
    },
    {
      "epoch": 4.56,
      "grad_norm": 0.20846165716648102,
      "learning_rate": 0.001903903903903904,
      "loss": 0.1479,
      "step": 49
    },
    {
      "epoch": 4.65,
      "grad_norm": 0.24298708140850067,
      "learning_rate": 0.001901901901901902,
      "loss": 0.1616,
      "step": 50
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.2579248249530792,
      "learning_rate": 0.0018998998998999,
      "loss": 0.1445,
      "step": 51
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.19726793467998505,
      "learning_rate": 0.001897897897897898,
      "loss": 0.1425,
      "step": 52
    },
    {
      "epoch": 4.93,
      "grad_norm": 0.26564180850982666,
      "learning_rate": 0.0018958958958958958,
      "loss": 0.1565,
      "step": 53
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.19641900062561035,
      "learning_rate": 0.0018938938938938938,
      "loss": 0.1209,
      "step": 54
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.22598569095134735,
      "learning_rate": 0.0018918918918918919,
      "loss": 0.1164,
      "step": 55
    },
    {
      "epoch": 5.21,
      "grad_norm": 0.251605749130249,
      "learning_rate": 0.00188988988988989,
      "loss": 0.1108,
      "step": 56
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.23231488466262817,
      "learning_rate": 0.001887887887887888,
      "loss": 0.0952,
      "step": 57
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.29818660020828247,
      "learning_rate": 0.0018858858858858858,
      "loss": 0.1099,
      "step": 58
    },
    {
      "epoch": 5.49,
      "grad_norm": 0.24504368007183075,
      "learning_rate": 0.0018838838838838839,
      "loss": 0.101,
      "step": 59
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.21923938393592834,
      "learning_rate": 0.001881881881881882,
      "loss": 0.1039,
      "step": 60
    },
    {
      "epoch": 5.67,
      "grad_norm": 0.24487842619419098,
      "learning_rate": 0.00187987987987988,
      "loss": 0.1001,
      "step": 61
    },
    {
      "epoch": 5.77,
      "grad_norm": 0.261868417263031,
      "learning_rate": 0.001877877877877878,
      "loss": 0.0911,
      "step": 62
    },
    {
      "epoch": 5.86,
      "grad_norm": 0.26674044132232666,
      "learning_rate": 0.0018758758758758759,
      "loss": 0.1053,
      "step": 63
    },
    {
      "epoch": 5.95,
      "grad_norm": 0.1992938369512558,
      "learning_rate": 0.001873873873873874,
      "loss": 0.0883,
      "step": 64
    },
    {
      "epoch": 6.05,
      "grad_norm": 0.5339002013206482,
      "learning_rate": 0.001871871871871872,
      "loss": 0.0867,
      "step": 65
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.1707654446363449,
      "learning_rate": 0.00186986986986987,
      "loss": 0.0775,
      "step": 66
    },
    {
      "epoch": 6.23,
      "grad_norm": 0.8775418400764465,
      "learning_rate": 0.001867867867867868,
      "loss": 0.0801,
      "step": 67
    },
    {
      "epoch": 6.33,
      "grad_norm": 0.4073001444339752,
      "learning_rate": 0.0018658658658658657,
      "loss": 0.1034,
      "step": 68
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.29436206817626953,
      "learning_rate": 0.0018638638638638638,
      "loss": 0.1003,
      "step": 69
    },
    {
      "epoch": 6.51,
      "grad_norm": 0.290439635515213,
      "learning_rate": 0.0018618618618618619,
      "loss": 0.0956,
      "step": 70
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.21431350708007812,
      "learning_rate": 0.00185985985985986,
      "loss": 0.0852,
      "step": 71
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.24671588838100433,
      "learning_rate": 0.001857857857857858,
      "loss": 0.0848,
      "step": 72
    },
    {
      "epoch": 6.79,
      "grad_norm": 0.2750566601753235,
      "learning_rate": 0.0018558558558558558,
      "loss": 0.0784,
      "step": 73
    },
    {
      "epoch": 6.88,
      "grad_norm": 0.2785981297492981,
      "learning_rate": 0.0018538538538538539,
      "loss": 0.0853,
      "step": 74
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.2566787004470825,
      "learning_rate": 0.001851851851851852,
      "loss": 0.0802,
      "step": 75
    },
    {
      "epoch": 7.07,
      "grad_norm": 0.18839526176452637,
      "learning_rate": 0.00184984984984985,
      "loss": 0.0635,
      "step": 76
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.20859406888484955,
      "learning_rate": 0.001847847847847848,
      "loss": 0.0669,
      "step": 77
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.3039081394672394,
      "learning_rate": 0.0018458458458458459,
      "loss": 0.0644,
      "step": 78
    },
    {
      "epoch": 7.35,
      "grad_norm": 0.2052895873785019,
      "learning_rate": 0.001843843843843844,
      "loss": 0.0628,
      "step": 79
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.19869670271873474,
      "learning_rate": 0.001841841841841842,
      "loss": 0.0676,
      "step": 80
    },
    {
      "epoch": 7.53,
      "grad_norm": 0.22746023535728455,
      "learning_rate": 0.0018398398398398398,
      "loss": 0.0623,
      "step": 81
    },
    {
      "epoch": 7.63,
      "grad_norm": 0.19103148579597473,
      "learning_rate": 0.0018378378378378379,
      "loss": 0.0648,
      "step": 82
    },
    {
      "epoch": 7.72,
      "grad_norm": 0.18535712361335754,
      "learning_rate": 0.0018358358358358357,
      "loss": 0.0625,
      "step": 83
    },
    {
      "epoch": 7.81,
      "grad_norm": 0.21970614790916443,
      "learning_rate": 0.0018338338338338338,
      "loss": 0.0735,
      "step": 84
    },
    {
      "epoch": 7.91,
      "grad_norm": 0.21595558524131775,
      "learning_rate": 0.0018318318318318318,
      "loss": 0.0728,
      "step": 85
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.2423926591873169,
      "learning_rate": 0.00182982982982983,
      "loss": 0.0693,
      "step": 86
    },
    {
      "epoch": 8.09,
      "grad_norm": 0.17347559332847595,
      "learning_rate": 0.001827827827827828,
      "loss": 0.041,
      "step": 87
    },
    {
      "epoch": 8.19,
      "grad_norm": 0.19068543612957,
      "learning_rate": 0.0018258258258258258,
      "loss": 0.05,
      "step": 88
    },
    {
      "epoch": 8.28,
      "grad_norm": 0.15929880738258362,
      "learning_rate": 0.0018238238238238239,
      "loss": 0.0395,
      "step": 89
    },
    {
      "epoch": 8.37,
      "grad_norm": 0.2608363628387451,
      "learning_rate": 0.001821821821821822,
      "loss": 0.0446,
      "step": 90
    },
    {
      "epoch": 8.47,
      "grad_norm": 0.18586759269237518,
      "learning_rate": 0.00181981981981982,
      "loss": 0.0509,
      "step": 91
    },
    {
      "epoch": 8.56,
      "grad_norm": 0.16114765405654907,
      "learning_rate": 0.001817817817817818,
      "loss": 0.0421,
      "step": 92
    },
    {
      "epoch": 8.65,
      "grad_norm": 0.16853126883506775,
      "learning_rate": 0.0018158158158158159,
      "loss": 0.0462,
      "step": 93
    },
    {
      "epoch": 8.74,
      "grad_norm": 0.17938560247421265,
      "learning_rate": 0.001813813813813814,
      "loss": 0.0459,
      "step": 94
    },
    {
      "epoch": 8.84,
      "grad_norm": 0.19151407480239868,
      "learning_rate": 0.001811811811811812,
      "loss": 0.0494,
      "step": 95
    },
    {
      "epoch": 8.93,
      "grad_norm": 0.14819422364234924,
      "learning_rate": 0.0018098098098098098,
      "loss": 0.0421,
      "step": 96
    },
    {
      "epoch": 9.02,
      "grad_norm": 0.14905127882957458,
      "learning_rate": 0.0018078078078078077,
      "loss": 0.0381,
      "step": 97
    },
    {
      "epoch": 9.12,
      "grad_norm": 0.12670212984085083,
      "learning_rate": 0.0018058058058058057,
      "loss": 0.0287,
      "step": 98
    },
    {
      "epoch": 9.21,
      "grad_norm": 0.32756730914115906,
      "learning_rate": 0.0018038038038038038,
      "loss": 0.0306,
      "step": 99
    },
    {
      "epoch": 9.3,
      "grad_norm": 0.19094567000865936,
      "learning_rate": 0.0018018018018018018,
      "loss": 0.0413,
      "step": 100
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.18511539697647095,
      "learning_rate": 0.0017997997997997999,
      "loss": 0.0407,
      "step": 101
    },
    {
      "epoch": 9.49,
      "grad_norm": 0.16668114066123962,
      "learning_rate": 0.0017977977977977977,
      "loss": 0.0326,
      "step": 102
    },
    {
      "epoch": 9.58,
      "grad_norm": 0.15231497585773468,
      "learning_rate": 0.0017957957957957958,
      "loss": 0.0299,
      "step": 103
    },
    {
      "epoch": 9.67,
      "grad_norm": 0.19253884255886078,
      "learning_rate": 0.0017937937937937938,
      "loss": 0.0425,
      "step": 104
    },
    {
      "epoch": 9.77,
      "grad_norm": 0.161960169672966,
      "learning_rate": 0.001791791791791792,
      "loss": 0.0385,
      "step": 105
    },
    {
      "epoch": 9.86,
      "grad_norm": 0.15865224599838257,
      "learning_rate": 0.00178978978978979,
      "loss": 0.0358,
      "step": 106
    },
    {
      "epoch": 9.95,
      "grad_norm": 0.16319987177848816,
      "learning_rate": 0.0017877877877877878,
      "loss": 0.0367,
      "step": 107
    },
    {
      "epoch": 10.05,
      "grad_norm": 0.1199568435549736,
      "learning_rate": 0.0017857857857857859,
      "loss": 0.0279,
      "step": 108
    },
    {
      "epoch": 10.14,
      "grad_norm": 0.12916956841945648,
      "learning_rate": 0.001783783783783784,
      "loss": 0.0234,
      "step": 109
    },
    {
      "epoch": 10.23,
      "grad_norm": 0.14657966792583466,
      "learning_rate": 0.0017817817817817817,
      "loss": 0.0263,
      "step": 110
    },
    {
      "epoch": 10.33,
      "grad_norm": 0.1660681962966919,
      "learning_rate": 0.0017797797797797798,
      "loss": 0.0303,
      "step": 111
    },
    {
      "epoch": 10.42,
      "grad_norm": 0.1787068247795105,
      "learning_rate": 0.0017777777777777776,
      "loss": 0.0325,
      "step": 112
    },
    {
      "epoch": 10.51,
      "grad_norm": 0.36577415466308594,
      "learning_rate": 0.0017757757757757757,
      "loss": 0.0293,
      "step": 113
    },
    {
      "epoch": 10.6,
      "grad_norm": 0.15158431231975555,
      "learning_rate": 0.0017737737737737738,
      "loss": 0.035,
      "step": 114
    },
    {
      "epoch": 10.7,
      "grad_norm": 0.14038600027561188,
      "learning_rate": 0.0017717717717717718,
      "loss": 0.0359,
      "step": 115
    },
    {
      "epoch": 10.79,
      "grad_norm": 0.16171874105930328,
      "learning_rate": 0.0017697697697697699,
      "loss": 0.0309,
      "step": 116
    },
    {
      "epoch": 10.88,
      "grad_norm": 0.12541256844997406,
      "learning_rate": 0.0017677677677677677,
      "loss": 0.0272,
      "step": 117
    },
    {
      "epoch": 10.98,
      "grad_norm": 0.28332453966140747,
      "learning_rate": 0.0017657657657657658,
      "loss": 0.0356,
      "step": 118
    },
    {
      "epoch": 11.07,
      "grad_norm": 0.20303605496883392,
      "learning_rate": 0.0017637637637637638,
      "loss": 0.03,
      "step": 119
    },
    {
      "epoch": 11.16,
      "grad_norm": 0.1583126187324524,
      "learning_rate": 0.0017617617617617619,
      "loss": 0.0237,
      "step": 120
    },
    {
      "epoch": 11.26,
      "grad_norm": 0.1663227677345276,
      "learning_rate": 0.00175975975975976,
      "loss": 0.0242,
      "step": 121
    },
    {
      "epoch": 11.35,
      "grad_norm": 0.1336936503648758,
      "learning_rate": 0.0017577577577577578,
      "loss": 0.0203,
      "step": 122
    },
    {
      "epoch": 11.44,
      "grad_norm": 0.22252091765403748,
      "learning_rate": 0.0017557557557557558,
      "loss": 0.0375,
      "step": 123
    },
    {
      "epoch": 11.53,
      "grad_norm": 0.12601596117019653,
      "learning_rate": 0.001753753753753754,
      "loss": 0.0208,
      "step": 124
    },
    {
      "epoch": 11.63,
      "grad_norm": 0.19643957912921906,
      "learning_rate": 0.0017517517517517517,
      "loss": 0.0393,
      "step": 125
    },
    {
      "epoch": 11.72,
      "grad_norm": 0.1497407853603363,
      "learning_rate": 0.0017497497497497498,
      "loss": 0.0299,
      "step": 126
    },
    {
      "epoch": 11.81,
      "grad_norm": 0.1200670525431633,
      "learning_rate": 0.0017477477477477476,
      "loss": 0.027,
      "step": 127
    },
    {
      "epoch": 11.91,
      "grad_norm": 0.17628063261508942,
      "learning_rate": 0.0017457457457457457,
      "loss": 0.0414,
      "step": 128
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.1418675035238266,
      "learning_rate": 0.0017437437437437437,
      "loss": 0.0273,
      "step": 129
    },
    {
      "epoch": 12.09,
      "grad_norm": 0.1458865851163864,
      "learning_rate": 0.0017417417417417418,
      "loss": 0.022,
      "step": 130
    },
    {
      "epoch": 12.19,
      "grad_norm": 0.11567732691764832,
      "learning_rate": 0.0017397397397397399,
      "loss": 0.0167,
      "step": 131
    },
    {
      "epoch": 12.28,
      "grad_norm": 0.44207289814949036,
      "learning_rate": 0.0017377377377377377,
      "loss": 0.0262,
      "step": 132
    },
    {
      "epoch": 12.37,
      "grad_norm": 0.14352640509605408,
      "learning_rate": 0.0017357357357357358,
      "loss": 0.0236,
      "step": 133
    },
    {
      "epoch": 12.47,
      "grad_norm": 0.19560161232948303,
      "learning_rate": 0.0017337337337337338,
      "loss": 0.0339,
      "step": 134
    },
    {
      "epoch": 12.56,
      "grad_norm": 0.17569731175899506,
      "learning_rate": 0.0017317317317317319,
      "loss": 0.0332,
      "step": 135
    },
    {
      "epoch": 12.65,
      "grad_norm": 0.2208559513092041,
      "learning_rate": 0.00172972972972973,
      "loss": 0.036,
      "step": 136
    },
    {
      "epoch": 12.74,
      "grad_norm": 0.17576010525226593,
      "learning_rate": 0.0017277277277277278,
      "loss": 0.0293,
      "step": 137
    },
    {
      "epoch": 12.84,
      "grad_norm": 0.17175643146038055,
      "learning_rate": 0.0017257257257257258,
      "loss": 0.0305,
      "step": 138
    },
    {
      "epoch": 12.93,
      "grad_norm": 0.1681094914674759,
      "learning_rate": 0.0017237237237237237,
      "loss": 0.0311,
      "step": 139
    },
    {
      "epoch": 13.02,
      "grad_norm": 0.25889313220977783,
      "learning_rate": 0.0017217217217217217,
      "loss": 0.0332,
      "step": 140
    },
    {
      "epoch": 13.12,
      "grad_norm": 0.13084806501865387,
      "learning_rate": 0.0017197197197197198,
      "loss": 0.0215,
      "step": 141
    },
    {
      "epoch": 13.21,
      "grad_norm": 0.12187618762254715,
      "learning_rate": 0.0017177177177177176,
      "loss": 0.0192,
      "step": 142
    },
    {
      "epoch": 13.3,
      "grad_norm": 0.12164618819952011,
      "learning_rate": 0.0017157157157157157,
      "loss": 0.0195,
      "step": 143
    },
    {
      "epoch": 13.4,
      "grad_norm": 0.12924732267856598,
      "learning_rate": 0.0017137137137137137,
      "loss": 0.0199,
      "step": 144
    },
    {
      "epoch": 13.49,
      "grad_norm": 0.15081335604190826,
      "learning_rate": 0.0017117117117117118,
      "loss": 0.0244,
      "step": 145
    },
    {
      "epoch": 13.58,
      "grad_norm": 0.20312143862247467,
      "learning_rate": 0.0017097097097097098,
      "loss": 0.0299,
      "step": 146
    },
    {
      "epoch": 13.67,
      "grad_norm": 0.17748410999774933,
      "learning_rate": 0.0017077077077077077,
      "loss": 0.0275,
      "step": 147
    },
    {
      "epoch": 13.77,
      "grad_norm": 0.16115255653858185,
      "learning_rate": 0.0017057057057057057,
      "loss": 0.0289,
      "step": 148
    },
    {
      "epoch": 13.86,
      "grad_norm": 0.14800146222114563,
      "learning_rate": 0.0017037037037037038,
      "loss": 0.0255,
      "step": 149
    },
    {
      "epoch": 13.95,
      "grad_norm": 0.13397763669490814,
      "learning_rate": 0.0017017017017017019,
      "loss": 0.0222,
      "step": 150
    },
    {
      "epoch": 14.05,
      "grad_norm": 0.15193189680576324,
      "learning_rate": 0.0016996996996997,
      "loss": 0.0249,
      "step": 151
    },
    {
      "epoch": 14.14,
      "grad_norm": 0.11898165196180344,
      "learning_rate": 0.0016976976976976978,
      "loss": 0.0166,
      "step": 152
    },
    {
      "epoch": 14.23,
      "grad_norm": 0.13656771183013916,
      "learning_rate": 0.0016956956956956956,
      "loss": 0.0223,
      "step": 153
    },
    {
      "epoch": 14.33,
      "grad_norm": 0.15442809462547302,
      "learning_rate": 0.0016936936936936937,
      "loss": 0.0263,
      "step": 154
    },
    {
      "epoch": 14.42,
      "grad_norm": 0.35817015171051025,
      "learning_rate": 0.0016916916916916917,
      "loss": 0.0182,
      "step": 155
    },
    {
      "epoch": 14.51,
      "grad_norm": 0.14386148750782013,
      "learning_rate": 0.0016896896896896898,
      "loss": 0.0234,
      "step": 156
    },
    {
      "epoch": 14.6,
      "grad_norm": 0.15172500908374786,
      "learning_rate": 0.0016876876876876876,
      "loss": 0.0234,
      "step": 157
    },
    {
      "epoch": 14.7,
      "grad_norm": 0.1171698123216629,
      "learning_rate": 0.0016856856856856857,
      "loss": 0.0196,
      "step": 158
    },
    {
      "epoch": 14.79,
      "grad_norm": 0.14162251353263855,
      "learning_rate": 0.0016836836836836837,
      "loss": 0.0222,
      "step": 159
    },
    {
      "epoch": 14.88,
      "grad_norm": 0.16906990110874176,
      "learning_rate": 0.0016816816816816818,
      "loss": 0.0208,
      "step": 160
    },
    {
      "epoch": 14.98,
      "grad_norm": 0.13473807275295258,
      "learning_rate": 0.0016796796796796796,
      "loss": 0.0198,
      "step": 161
    },
    {
      "epoch": 15.07,
      "grad_norm": 0.7500114440917969,
      "learning_rate": 0.0016776776776776777,
      "loss": 0.0237,
      "step": 162
    },
    {
      "epoch": 15.16,
      "grad_norm": 0.17404961585998535,
      "learning_rate": 0.0016756756756756757,
      "loss": 0.022,
      "step": 163
    },
    {
      "epoch": 15.26,
      "grad_norm": 0.20667695999145508,
      "learning_rate": 0.0016736736736736738,
      "loss": 0.0317,
      "step": 164
    },
    {
      "epoch": 15.35,
      "grad_norm": 0.18450109660625458,
      "learning_rate": 0.0016716716716716718,
      "loss": 0.0324,
      "step": 165
    },
    {
      "epoch": 15.44,
      "grad_norm": 0.21405695378780365,
      "learning_rate": 0.0016696696696696697,
      "loss": 0.0354,
      "step": 166
    },
    {
      "epoch": 15.53,
      "grad_norm": 0.17413051426410675,
      "learning_rate": 0.0016676676676676677,
      "loss": 0.0282,
      "step": 167
    },
    {
      "epoch": 15.63,
      "grad_norm": 0.474658340215683,
      "learning_rate": 0.0016656656656656656,
      "loss": 0.0292,
      "step": 168
    },
    {
      "epoch": 15.72,
      "grad_norm": 0.216481015086174,
      "learning_rate": 0.0016636636636636636,
      "loss": 0.0324,
      "step": 169
    },
    {
      "epoch": 15.81,
      "grad_norm": 0.22553342580795288,
      "learning_rate": 0.0016616616616616617,
      "loss": 0.0319,
      "step": 170
    },
    {
      "epoch": 15.91,
      "grad_norm": 0.21478119492530823,
      "learning_rate": 0.0016596596596596595,
      "loss": 0.0325,
      "step": 171
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.2862038016319275,
      "learning_rate": 0.0016576576576576576,
      "loss": 0.0429,
      "step": 172
    },
    {
      "epoch": 16.09,
      "grad_norm": 0.15950839221477509,
      "learning_rate": 0.0016556556556556557,
      "loss": 0.0243,
      "step": 173
    },
    {
      "epoch": 16.19,
      "grad_norm": 0.31929463148117065,
      "learning_rate": 0.0016536536536536537,
      "loss": 0.0271,
      "step": 174
    },
    {
      "epoch": 16.28,
      "grad_norm": 0.1751406043767929,
      "learning_rate": 0.0016516516516516518,
      "loss": 0.0245,
      "step": 175
    },
    {
      "epoch": 16.37,
      "grad_norm": 0.16707609593868256,
      "learning_rate": 0.0016496496496496496,
      "loss": 0.0242,
      "step": 176
    },
    {
      "epoch": 16.47,
      "grad_norm": 0.7891533970832825,
      "learning_rate": 0.0016476476476476477,
      "loss": 0.0321,
      "step": 177
    },
    {
      "epoch": 16.56,
      "grad_norm": 0.18779712915420532,
      "learning_rate": 0.0016456456456456457,
      "loss": 0.0252,
      "step": 178
    },
    {
      "epoch": 16.65,
      "grad_norm": 0.1881711632013321,
      "learning_rate": 0.0016436436436436438,
      "loss": 0.0282,
      "step": 179
    },
    {
      "epoch": 16.74,
      "grad_norm": 0.203497052192688,
      "learning_rate": 0.0016416416416416418,
      "loss": 0.0269,
      "step": 180
    },
    {
      "epoch": 16.84,
      "grad_norm": 0.46536093950271606,
      "learning_rate": 0.0016396396396396397,
      "loss": 0.0327,
      "step": 181
    },
    {
      "epoch": 16.93,
      "grad_norm": 0.17711864411830902,
      "learning_rate": 0.0016376376376376375,
      "loss": 0.0267,
      "step": 182
    },
    {
      "epoch": 17.02,
      "grad_norm": 0.2085958570241928,
      "learning_rate": 0.0016356356356356356,
      "loss": 0.0351,
      "step": 183
    },
    {
      "epoch": 17.12,
      "grad_norm": 0.16959838569164276,
      "learning_rate": 0.0016336336336336336,
      "loss": 0.0245,
      "step": 184
    },
    {
      "epoch": 17.21,
      "grad_norm": 0.21090783178806305,
      "learning_rate": 0.0016316316316316317,
      "loss": 0.0317,
      "step": 185
    },
    {
      "epoch": 17.3,
      "grad_norm": 0.24964235723018646,
      "learning_rate": 0.0016296296296296295,
      "loss": 0.0237,
      "step": 186
    },
    {
      "epoch": 17.4,
      "grad_norm": 0.1871645301580429,
      "learning_rate": 0.0016276276276276276,
      "loss": 0.0279,
      "step": 187
    },
    {
      "epoch": 17.49,
      "grad_norm": 0.17427071928977966,
      "learning_rate": 0.0016256256256256256,
      "loss": 0.0246,
      "step": 188
    },
    {
      "epoch": 17.58,
      "grad_norm": 0.2249002605676651,
      "learning_rate": 0.0016236236236236237,
      "loss": 0.0342,
      "step": 189
    },
    {
      "epoch": 17.67,
      "grad_norm": 0.14843669533729553,
      "learning_rate": 0.0016216216216216218,
      "loss": 0.0237,
      "step": 190
    },
    {
      "epoch": 17.77,
      "grad_norm": 0.19408485293388367,
      "learning_rate": 0.0016196196196196196,
      "loss": 0.0312,
      "step": 191
    },
    {
      "epoch": 17.86,
      "grad_norm": 0.1466081440448761,
      "learning_rate": 0.0016176176176176177,
      "loss": 0.0232,
      "step": 192
    },
    {
      "epoch": 17.95,
      "grad_norm": 0.7054815888404846,
      "learning_rate": 0.0016156156156156157,
      "loss": 0.031,
      "step": 193
    },
    {
      "epoch": 18.05,
      "grad_norm": 0.23588448762893677,
      "learning_rate": 0.0016136136136136138,
      "loss": 0.032,
      "step": 194
    },
    {
      "epoch": 18.14,
      "grad_norm": 0.25636205077171326,
      "learning_rate": 0.0016116116116116118,
      "loss": 0.0304,
      "step": 195
    },
    {
      "epoch": 18.23,
      "grad_norm": 0.1628827303647995,
      "learning_rate": 0.0016096096096096097,
      "loss": 0.0183,
      "step": 196
    },
    {
      "epoch": 18.33,
      "grad_norm": 0.6799976229667664,
      "learning_rate": 0.0016076076076076075,
      "loss": 0.0383,
      "step": 197
    },
    {
      "epoch": 18.42,
      "grad_norm": 0.24623911082744598,
      "learning_rate": 0.0016056056056056056,
      "loss": 0.0301,
      "step": 198
    },
    {
      "epoch": 18.51,
      "grad_norm": 0.26263728737831116,
      "learning_rate": 0.0016036036036036036,
      "loss": 0.0384,
      "step": 199
    },
    {
      "epoch": 18.6,
      "grad_norm": 0.24216783046722412,
      "learning_rate": 0.0016016016016016017,
      "loss": 0.0377,
      "step": 200
    },
    {
      "epoch": 18.7,
      "grad_norm": 0.21968820691108704,
      "learning_rate": 0.0015995995995995995,
      "loss": 0.0388,
      "step": 201
    },
    {
      "epoch": 18.79,
      "grad_norm": 0.23277999460697174,
      "learning_rate": 0.0015975975975975976,
      "loss": 0.0335,
      "step": 202
    },
    {
      "epoch": 18.88,
      "grad_norm": 0.7189760804176331,
      "learning_rate": 0.0015955955955955956,
      "loss": 0.0448,
      "step": 203
    },
    {
      "epoch": 18.98,
      "grad_norm": 0.5283987522125244,
      "learning_rate": 0.0015935935935935937,
      "loss": 0.0479,
      "step": 204
    },
    {
      "epoch": 19.07,
      "grad_norm": 0.1992095708847046,
      "learning_rate": 0.0015915915915915917,
      "loss": 0.0259,
      "step": 205
    },
    {
      "epoch": 19.16,
      "grad_norm": 0.2722899317741394,
      "learning_rate": 0.0015895895895895896,
      "loss": 0.035,
      "step": 206
    },
    {
      "epoch": 19.26,
      "grad_norm": 0.16028214991092682,
      "learning_rate": 0.0015875875875875876,
      "loss": 0.0252,
      "step": 207
    },
    {
      "epoch": 19.35,
      "grad_norm": 0.1479153335094452,
      "learning_rate": 0.0015855855855855857,
      "loss": 0.0186,
      "step": 208
    },
    {
      "epoch": 19.44,
      "grad_norm": 0.18401648104190826,
      "learning_rate": 0.0015835835835835838,
      "loss": 0.0273,
      "step": 209
    },
    {
      "epoch": 19.53,
      "grad_norm": 0.45847856998443604,
      "learning_rate": 0.0015815815815815818,
      "loss": 0.0219,
      "step": 210
    },
    {
      "epoch": 19.63,
      "grad_norm": 0.172963485121727,
      "learning_rate": 0.0015795795795795794,
      "loss": 0.0212,
      "step": 211
    },
    {
      "epoch": 19.72,
      "grad_norm": 0.18817926943302155,
      "learning_rate": 0.0015775775775775775,
      "loss": 0.0292,
      "step": 212
    },
    {
      "epoch": 19.81,
      "grad_norm": 0.20419757068157196,
      "learning_rate": 0.0015755755755755755,
      "loss": 0.0303,
      "step": 213
    },
    {
      "epoch": 19.91,
      "grad_norm": 0.19756686687469482,
      "learning_rate": 0.0015735735735735736,
      "loss": 0.0271,
      "step": 214
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.14679427444934845,
      "learning_rate": 0.0015715715715715717,
      "loss": 0.0214,
      "step": 215
    },
    {
      "epoch": 20.09,
      "grad_norm": 0.14583350718021393,
      "learning_rate": 0.0015695695695695695,
      "loss": 0.0191,
      "step": 216
    },
    {
      "epoch": 20.19,
      "grad_norm": 0.15297341346740723,
      "learning_rate": 0.0015675675675675676,
      "loss": 0.0176,
      "step": 217
    },
    {
      "epoch": 20.28,
      "grad_norm": 0.18278124928474426,
      "learning_rate": 0.0015655655655655656,
      "loss": 0.0239,
      "step": 218
    },
    {
      "epoch": 20.37,
      "grad_norm": 0.1664392501115799,
      "learning_rate": 0.0015635635635635637,
      "loss": 0.0206,
      "step": 219
    },
    {
      "epoch": 20.47,
      "grad_norm": 0.49318164587020874,
      "learning_rate": 0.0015615615615615615,
      "loss": 0.0229,
      "step": 220
    },
    {
      "epoch": 20.56,
      "grad_norm": 0.13801375031471252,
      "learning_rate": 0.0015595595595595596,
      "loss": 0.0158,
      "step": 221
    },
    {
      "epoch": 20.65,
      "grad_norm": 0.154294952750206,
      "learning_rate": 0.0015575575575575576,
      "loss": 0.0205,
      "step": 222
    },
    {
      "epoch": 20.74,
      "grad_norm": 0.14349927008152008,
      "learning_rate": 0.0015555555555555557,
      "loss": 0.0218,
      "step": 223
    },
    {
      "epoch": 20.84,
      "grad_norm": 0.14842985570430756,
      "learning_rate": 0.0015535535535535537,
      "loss": 0.0228,
      "step": 224
    },
    {
      "epoch": 20.93,
      "grad_norm": 0.14195027947425842,
      "learning_rate": 0.0015515515515515516,
      "loss": 0.0218,
      "step": 225
    },
    {
      "epoch": 21.02,
      "grad_norm": 0.1998073309659958,
      "learning_rate": 0.0015495495495495494,
      "loss": 0.0323,
      "step": 226
    },
    {
      "epoch": 21.12,
      "grad_norm": 0.09599553048610687,
      "learning_rate": 0.0015475475475475475,
      "loss": 0.0128,
      "step": 227
    },
    {
      "epoch": 21.21,
      "grad_norm": 0.15941940248012543,
      "learning_rate": 0.0015455455455455455,
      "loss": 0.0193,
      "step": 228
    },
    {
      "epoch": 21.3,
      "grad_norm": 0.18800006806850433,
      "learning_rate": 0.0015435435435435436,
      "loss": 0.0202,
      "step": 229
    },
    {
      "epoch": 21.4,
      "grad_norm": 0.10920831561088562,
      "learning_rate": 0.0015415415415415414,
      "loss": 0.0111,
      "step": 230
    },
    {
      "epoch": 21.49,
      "grad_norm": 0.2068585604429245,
      "learning_rate": 0.0015395395395395395,
      "loss": 0.0272,
      "step": 231
    },
    {
      "epoch": 21.58,
      "grad_norm": 0.13729514181613922,
      "learning_rate": 0.0015375375375375375,
      "loss": 0.0188,
      "step": 232
    },
    {
      "epoch": 21.67,
      "grad_norm": 0.11570963263511658,
      "learning_rate": 0.0015355355355355356,
      "loss": 0.0162,
      "step": 233
    },
    {
      "epoch": 21.77,
      "grad_norm": 0.11246766149997711,
      "learning_rate": 0.0015335335335335337,
      "loss": 0.0158,
      "step": 234
    },
    {
      "epoch": 21.86,
      "grad_norm": 0.13074816763401031,
      "learning_rate": 0.0015315315315315315,
      "loss": 0.0146,
      "step": 235
    },
    {
      "epoch": 21.95,
      "grad_norm": 0.132390558719635,
      "learning_rate": 0.0015295295295295296,
      "loss": 0.0154,
      "step": 236
    },
    {
      "epoch": 22.05,
      "grad_norm": 0.09564812481403351,
      "learning_rate": 0.0015275275275275276,
      "loss": 0.0119,
      "step": 237
    },
    {
      "epoch": 22.14,
      "grad_norm": 0.10601690411567688,
      "learning_rate": 0.0015255255255255257,
      "loss": 0.0158,
      "step": 238
    },
    {
      "epoch": 22.23,
      "grad_norm": 0.11311355233192444,
      "learning_rate": 0.0015235235235235237,
      "loss": 0.0122,
      "step": 239
    },
    {
      "epoch": 22.33,
      "grad_norm": 0.12214581668376923,
      "learning_rate": 0.0015215215215215214,
      "loss": 0.0132,
      "step": 240
    },
    {
      "epoch": 22.42,
      "grad_norm": 0.1116672083735466,
      "learning_rate": 0.0015195195195195194,
      "loss": 0.0102,
      "step": 241
    },
    {
      "epoch": 22.51,
      "grad_norm": 0.08826189488172531,
      "learning_rate": 0.0015175175175175175,
      "loss": 0.0075,
      "step": 242
    },
    {
      "epoch": 22.6,
      "grad_norm": 0.10075847059488297,
      "learning_rate": 0.0015155155155155155,
      "loss": 0.0119,
      "step": 243
    },
    {
      "epoch": 22.7,
      "grad_norm": 0.11353474110364914,
      "learning_rate": 0.0015135135135135136,
      "loss": 0.0139,
      "step": 244
    },
    {
      "epoch": 22.79,
      "grad_norm": 0.10257426649332047,
      "learning_rate": 0.0015115115115115114,
      "loss": 0.0093,
      "step": 245
    },
    {
      "epoch": 22.88,
      "grad_norm": 0.09495458006858826,
      "learning_rate": 0.0015095095095095095,
      "loss": 0.0108,
      "step": 246
    },
    {
      "epoch": 22.98,
      "grad_norm": 0.5463971495628357,
      "learning_rate": 0.0015075075075075075,
      "loss": 0.0148,
      "step": 247
    },
    {
      "epoch": 23.07,
      "grad_norm": 0.10208092629909515,
      "learning_rate": 0.0015055055055055056,
      "loss": 0.0102,
      "step": 248
    },
    {
      "epoch": 23.16,
      "grad_norm": 0.18766802549362183,
      "learning_rate": 0.0015035035035035036,
      "loss": 0.019,
      "step": 249
    },
    {
      "epoch": 23.26,
      "grad_norm": 0.12946560978889465,
      "learning_rate": 0.0015015015015015015,
      "loss": 0.0138,
      "step": 250
    },
    {
      "epoch": 23.35,
      "grad_norm": 0.16246852278709412,
      "learning_rate": 0.0014994994994994995,
      "loss": 0.0158,
      "step": 251
    },
    {
      "epoch": 23.44,
      "grad_norm": 0.12050007283687592,
      "learning_rate": 0.0014974974974974976,
      "loss": 0.0094,
      "step": 252
    },
    {
      "epoch": 23.53,
      "grad_norm": 0.20695464313030243,
      "learning_rate": 0.0014954954954954957,
      "loss": 0.02,
      "step": 253
    },
    {
      "epoch": 23.63,
      "grad_norm": 0.12067891657352448,
      "learning_rate": 0.0014934934934934937,
      "loss": 0.0123,
      "step": 254
    },
    {
      "epoch": 23.72,
      "grad_norm": 0.1243496686220169,
      "learning_rate": 0.0014914914914914913,
      "loss": 0.016,
      "step": 255
    },
    {
      "epoch": 23.81,
      "grad_norm": 0.11034452170133591,
      "learning_rate": 0.0014894894894894894,
      "loss": 0.0141,
      "step": 256
    },
    {
      "epoch": 23.91,
      "grad_norm": 0.10018245875835419,
      "learning_rate": 0.0014874874874874875,
      "loss": 0.0119,
      "step": 257
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.10271686315536499,
      "learning_rate": 0.0014854854854854855,
      "loss": 0.0113,
      "step": 258
    },
    {
      "epoch": 24.09,
      "grad_norm": 0.0902242660522461,
      "learning_rate": 0.0014834834834834836,
      "loss": 0.0088,
      "step": 259
    },
    {
      "epoch": 24.19,
      "grad_norm": 0.0898832306265831,
      "learning_rate": 0.0014814814814814814,
      "loss": 0.0068,
      "step": 260
    },
    {
      "epoch": 24.28,
      "grad_norm": 0.10742070525884628,
      "learning_rate": 0.0014794794794794795,
      "loss": 0.008,
      "step": 261
    },
    {
      "epoch": 24.37,
      "grad_norm": 0.3394037187099457,
      "learning_rate": 0.0014774774774774775,
      "loss": 0.012,
      "step": 262
    },
    {
      "epoch": 24.47,
      "grad_norm": 0.1366187185049057,
      "learning_rate": 0.0014754754754754756,
      "loss": 0.0128,
      "step": 263
    },
    {
      "epoch": 24.56,
      "grad_norm": 0.16941972076892853,
      "learning_rate": 0.0014734734734734736,
      "loss": 0.0144,
      "step": 264
    },
    {
      "epoch": 24.65,
      "grad_norm": 0.13689333200454712,
      "learning_rate": 0.0014714714714714715,
      "loss": 0.0131,
      "step": 265
    },
    {
      "epoch": 24.74,
      "grad_norm": 0.12805940210819244,
      "learning_rate": 0.0014694694694694695,
      "loss": 0.0135,
      "step": 266
    },
    {
      "epoch": 24.84,
      "grad_norm": 0.15572786331176758,
      "learning_rate": 0.0014674674674674676,
      "loss": 0.0181,
      "step": 267
    },
    {
      "epoch": 24.93,
      "grad_norm": 0.11057024449110031,
      "learning_rate": 0.0014654654654654656,
      "loss": 0.0119,
      "step": 268
    },
    {
      "epoch": 25.02,
      "grad_norm": 0.15199409425258636,
      "learning_rate": 0.0014634634634634637,
      "loss": 0.017,
      "step": 269
    },
    {
      "epoch": 25.12,
      "grad_norm": 0.11110897362232208,
      "learning_rate": 0.0014614614614614613,
      "loss": 0.0099,
      "step": 270
    },
    {
      "epoch": 25.21,
      "grad_norm": 0.0694834515452385,
      "learning_rate": 0.0014594594594594594,
      "loss": 0.0065,
      "step": 271
    },
    {
      "epoch": 25.3,
      "grad_norm": 0.12897028028964996,
      "learning_rate": 0.0014574574574574574,
      "loss": 0.0126,
      "step": 272
    },
    {
      "epoch": 25.4,
      "grad_norm": 0.10043667256832123,
      "learning_rate": 0.0014554554554554555,
      "loss": 0.0112,
      "step": 273
    },
    {
      "epoch": 25.49,
      "grad_norm": 0.10153573751449585,
      "learning_rate": 0.0014534534534534536,
      "loss": 0.0099,
      "step": 274
    },
    {
      "epoch": 25.58,
      "grad_norm": 0.4327797591686249,
      "learning_rate": 0.0014514514514514514,
      "loss": 0.0086,
      "step": 275
    },
    {
      "epoch": 25.67,
      "grad_norm": 0.12818408012390137,
      "learning_rate": 0.0014494494494494495,
      "loss": 0.0142,
      "step": 276
    },
    {
      "epoch": 25.77,
      "grad_norm": 0.13305599987506866,
      "learning_rate": 0.0014474474474474475,
      "loss": 0.0144,
      "step": 277
    },
    {
      "epoch": 25.86,
      "grad_norm": 0.16424229741096497,
      "learning_rate": 0.0014454454454454456,
      "loss": 0.0168,
      "step": 278
    },
    {
      "epoch": 25.95,
      "grad_norm": 0.10903183370828629,
      "learning_rate": 0.0014434434434434436,
      "loss": 0.0105,
      "step": 279
    },
    {
      "epoch": 26.05,
      "grad_norm": 0.14578936994075775,
      "learning_rate": 0.0014414414414414415,
      "loss": 0.0126,
      "step": 280
    },
    {
      "epoch": 26.14,
      "grad_norm": 0.12450461089611053,
      "learning_rate": 0.0014394394394394395,
      "loss": 0.0116,
      "step": 281
    },
    {
      "epoch": 26.23,
      "grad_norm": 0.1154106929898262,
      "learning_rate": 0.0014374374374374376,
      "loss": 0.0094,
      "step": 282
    },
    {
      "epoch": 26.33,
      "grad_norm": 0.12836499512195587,
      "learning_rate": 0.0014354354354354356,
      "loss": 0.0105,
      "step": 283
    },
    {
      "epoch": 26.42,
      "grad_norm": 0.11856179684400558,
      "learning_rate": 0.0014334334334334333,
      "loss": 0.0091,
      "step": 284
    },
    {
      "epoch": 26.51,
      "grad_norm": 0.12042251974344254,
      "learning_rate": 0.0014314314314314313,
      "loss": 0.0134,
      "step": 285
    },
    {
      "epoch": 26.6,
      "grad_norm": 0.13722313940525055,
      "learning_rate": 0.0014294294294294294,
      "loss": 0.0162,
      "step": 286
    },
    {
      "epoch": 26.7,
      "grad_norm": 0.11666831374168396,
      "learning_rate": 0.0014274274274274274,
      "loss": 0.0101,
      "step": 287
    },
    {
      "epoch": 26.79,
      "grad_norm": 0.11098901182413101,
      "learning_rate": 0.0014254254254254255,
      "loss": 0.0107,
      "step": 288
    },
    {
      "epoch": 26.88,
      "grad_norm": 0.1570785492658615,
      "learning_rate": 0.0014234234234234233,
      "loss": 0.0182,
      "step": 289
    },
    {
      "epoch": 26.98,
      "grad_norm": 0.11240728944540024,
      "learning_rate": 0.0014214214214214214,
      "loss": 0.0102,
      "step": 290
    },
    {
      "epoch": 27.07,
      "grad_norm": 0.11979077011346817,
      "learning_rate": 0.0014194194194194194,
      "loss": 0.0107,
      "step": 291
    },
    {
      "epoch": 27.16,
      "grad_norm": 0.06227131932973862,
      "learning_rate": 0.0014174174174174175,
      "loss": 0.0058,
      "step": 292
    },
    {
      "epoch": 27.26,
      "grad_norm": 0.10084498673677444,
      "learning_rate": 0.0014154154154154156,
      "loss": 0.0082,
      "step": 293
    },
    {
      "epoch": 27.35,
      "grad_norm": 0.09121516346931458,
      "learning_rate": 0.0014134134134134134,
      "loss": 0.0086,
      "step": 294
    },
    {
      "epoch": 27.44,
      "grad_norm": 0.08678147196769714,
      "learning_rate": 0.0014114114114114115,
      "loss": 0.0076,
      "step": 295
    },
    {
      "epoch": 27.53,
      "grad_norm": 0.10272347927093506,
      "learning_rate": 0.0014094094094094095,
      "loss": 0.0106,
      "step": 296
    },
    {
      "epoch": 27.63,
      "grad_norm": 0.07840856909751892,
      "learning_rate": 0.0014074074074074076,
      "loss": 0.0071,
      "step": 297
    },
    {
      "epoch": 27.72,
      "grad_norm": 0.10716904699802399,
      "learning_rate": 0.0014054054054054054,
      "loss": 0.009,
      "step": 298
    },
    {
      "epoch": 27.81,
      "grad_norm": 0.08777103573083878,
      "learning_rate": 0.0014034034034034032,
      "loss": 0.0077,
      "step": 299
    },
    {
      "epoch": 27.91,
      "grad_norm": 0.1003369688987732,
      "learning_rate": 0.0014014014014014013,
      "loss": 0.0074,
      "step": 300
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.07700402289628983,
      "learning_rate": 0.0013993993993993994,
      "loss": 0.006,
      "step": 301
    },
    {
      "epoch": 28.09,
      "grad_norm": 0.05425987020134926,
      "learning_rate": 0.0013973973973973974,
      "loss": 0.0035,
      "step": 302
    },
    {
      "epoch": 28.19,
      "grad_norm": 0.08867619186639786,
      "learning_rate": 0.0013953953953953955,
      "loss": 0.0074,
      "step": 303
    },
    {
      "epoch": 28.28,
      "grad_norm": 0.06515760719776154,
      "learning_rate": 0.0013933933933933933,
      "loss": 0.0047,
      "step": 304
    },
    {
      "epoch": 28.37,
      "grad_norm": 0.08021624386310577,
      "learning_rate": 0.0013913913913913914,
      "loss": 0.0061,
      "step": 305
    },
    {
      "epoch": 28.47,
      "grad_norm": 0.1081114262342453,
      "learning_rate": 0.0013893893893893894,
      "loss": 0.0103,
      "step": 306
    },
    {
      "epoch": 28.56,
      "grad_norm": 0.0835670754313469,
      "learning_rate": 0.0013873873873873875,
      "loss": 0.0055,
      "step": 307
    },
    {
      "epoch": 28.65,
      "grad_norm": 0.08508322387933731,
      "learning_rate": 0.0013853853853853855,
      "loss": 0.0073,
      "step": 308
    },
    {
      "epoch": 28.74,
      "grad_norm": 0.07496494799852371,
      "learning_rate": 0.0013833833833833834,
      "loss": 0.0053,
      "step": 309
    },
    {
      "epoch": 28.84,
      "grad_norm": 0.07663434743881226,
      "learning_rate": 0.0013813813813813814,
      "loss": 0.0067,
      "step": 310
    },
    {
      "epoch": 28.93,
      "grad_norm": 0.08244332671165466,
      "learning_rate": 0.0013793793793793795,
      "loss": 0.0058,
      "step": 311
    },
    {
      "epoch": 29.02,
      "grad_norm": 0.0868007019162178,
      "learning_rate": 0.0013773773773773776,
      "loss": 0.0069,
      "step": 312
    },
    {
      "epoch": 29.12,
      "grad_norm": 0.4052487015724182,
      "learning_rate": 0.0013753753753753754,
      "loss": 0.0052,
      "step": 313
    },
    {
      "epoch": 29.21,
      "grad_norm": 0.06757152825593948,
      "learning_rate": 0.0013733733733733732,
      "loss": 0.0038,
      "step": 314
    },
    {
      "epoch": 29.3,
      "grad_norm": 0.07421130686998367,
      "learning_rate": 0.0013713713713713713,
      "loss": 0.0061,
      "step": 315
    },
    {
      "epoch": 29.4,
      "grad_norm": 0.12837092578411102,
      "learning_rate": 0.0013693693693693693,
      "loss": 0.0085,
      "step": 316
    },
    {
      "epoch": 29.49,
      "grad_norm": 0.09021154791116714,
      "learning_rate": 0.0013673673673673674,
      "loss": 0.0085,
      "step": 317
    },
    {
      "epoch": 29.58,
      "grad_norm": 0.07567004859447479,
      "learning_rate": 0.0013653653653653655,
      "loss": 0.007,
      "step": 318
    },
    {
      "epoch": 29.67,
      "grad_norm": 0.12329138815402985,
      "learning_rate": 0.0013633633633633633,
      "loss": 0.0077,
      "step": 319
    },
    {
      "epoch": 29.77,
      "grad_norm": 0.0948009118437767,
      "learning_rate": 0.0013613613613613614,
      "loss": 0.0061,
      "step": 320
    },
    {
      "epoch": 29.86,
      "grad_norm": 0.07781707495450974,
      "learning_rate": 0.0013593593593593594,
      "loss": 0.0051,
      "step": 321
    },
    {
      "epoch": 29.95,
      "grad_norm": 0.10603152215480804,
      "learning_rate": 0.0013573573573573575,
      "loss": 0.0083,
      "step": 322
    },
    {
      "epoch": 30.05,
      "grad_norm": 0.11571641266345978,
      "learning_rate": 0.0013553553553553555,
      "loss": 0.0064,
      "step": 323
    },
    {
      "epoch": 30.14,
      "grad_norm": 0.07329671829938889,
      "learning_rate": 0.0013533533533533534,
      "loss": 0.0045,
      "step": 324
    },
    {
      "epoch": 30.23,
      "grad_norm": 0.09423427283763885,
      "learning_rate": 0.0013513513513513514,
      "loss": 0.0067,
      "step": 325
    },
    {
      "epoch": 30.33,
      "grad_norm": 0.10144127160310745,
      "learning_rate": 0.0013493493493493495,
      "loss": 0.0067,
      "step": 326
    },
    {
      "epoch": 30.42,
      "grad_norm": 0.07445558905601501,
      "learning_rate": 0.0013473473473473473,
      "loss": 0.0051,
      "step": 327
    },
    {
      "epoch": 30.51,
      "grad_norm": 0.07344485819339752,
      "learning_rate": 0.0013453453453453454,
      "loss": 0.0058,
      "step": 328
    },
    {
      "epoch": 30.6,
      "grad_norm": 0.07438346743583679,
      "learning_rate": 0.0013433433433433432,
      "loss": 0.0061,
      "step": 329
    },
    {
      "epoch": 30.7,
      "grad_norm": 0.07676655054092407,
      "learning_rate": 0.0013413413413413413,
      "loss": 0.0052,
      "step": 330
    },
    {
      "epoch": 30.79,
      "grad_norm": 0.0767897292971611,
      "learning_rate": 0.0013393393393393393,
      "loss": 0.0064,
      "step": 331
    },
    {
      "epoch": 30.88,
      "grad_norm": 0.07645553350448608,
      "learning_rate": 0.0013373373373373374,
      "loss": 0.0062,
      "step": 332
    },
    {
      "epoch": 30.98,
      "grad_norm": 0.09462464600801468,
      "learning_rate": 0.0013353353353353354,
      "loss": 0.0077,
      "step": 333
    },
    {
      "epoch": 31.07,
      "grad_norm": 0.049924131482839584,
      "learning_rate": 0.0013333333333333333,
      "loss": 0.0036,
      "step": 334
    },
    {
      "epoch": 31.16,
      "grad_norm": 0.07813724875450134,
      "learning_rate": 0.0013313313313313313,
      "loss": 0.0047,
      "step": 335
    },
    {
      "epoch": 31.26,
      "grad_norm": 0.09082891792058945,
      "learning_rate": 0.0013293293293293294,
      "loss": 0.0046,
      "step": 336
    },
    {
      "epoch": 31.35,
      "grad_norm": 0.0659075453877449,
      "learning_rate": 0.0013273273273273275,
      "loss": 0.0047,
      "step": 337
    },
    {
      "epoch": 31.44,
      "grad_norm": 0.04118886962532997,
      "learning_rate": 0.0013253253253253255,
      "loss": 0.0027,
      "step": 338
    },
    {
      "epoch": 31.53,
      "grad_norm": 0.06225672736763954,
      "learning_rate": 0.0013233233233233234,
      "loss": 0.0039,
      "step": 339
    },
    {
      "epoch": 31.63,
      "grad_norm": 0.05968526005744934,
      "learning_rate": 0.0013213213213213214,
      "loss": 0.0043,
      "step": 340
    },
    {
      "epoch": 31.72,
      "grad_norm": 0.06615372747182846,
      "learning_rate": 0.0013193193193193195,
      "loss": 0.0047,
      "step": 341
    },
    {
      "epoch": 31.81,
      "grad_norm": 0.06672164797782898,
      "learning_rate": 0.0013173173173173173,
      "loss": 0.0053,
      "step": 342
    },
    {
      "epoch": 31.91,
      "grad_norm": 0.05841416120529175,
      "learning_rate": 0.0013153153153153154,
      "loss": 0.0035,
      "step": 343
    },
    {
      "epoch": 32.0,
      "grad_norm": 0.07656476646661758,
      "learning_rate": 0.0013133133133133132,
      "loss": 0.0058,
      "step": 344
    },
    {
      "epoch": 32.09,
      "grad_norm": 0.04063386842608452,
      "learning_rate": 0.0013113113113113113,
      "loss": 0.0024,
      "step": 345
    },
    {
      "epoch": 32.19,
      "grad_norm": 0.04993503540754318,
      "learning_rate": 0.0013093093093093093,
      "loss": 0.0034,
      "step": 346
    },
    {
      "epoch": 32.28,
      "grad_norm": 0.05899167060852051,
      "learning_rate": 0.0013073073073073074,
      "loss": 0.003,
      "step": 347
    },
    {
      "epoch": 32.37,
      "grad_norm": 0.03621702641248703,
      "learning_rate": 0.0013053053053053052,
      "loss": 0.0021,
      "step": 348
    },
    {
      "epoch": 32.47,
      "grad_norm": 0.04735443741083145,
      "learning_rate": 0.0013033033033033033,
      "loss": 0.0038,
      "step": 349
    },
    {
      "epoch": 32.56,
      "grad_norm": 0.047074075788259506,
      "learning_rate": 0.0013013013013013013,
      "loss": 0.0031,
      "step": 350
    },
    {
      "epoch": 32.65,
      "grad_norm": 0.04885280877351761,
      "learning_rate": 0.0012992992992992994,
      "loss": 0.0047,
      "step": 351
    },
    {
      "epoch": 32.74,
      "grad_norm": 0.06170407682657242,
      "learning_rate": 0.0012972972972972974,
      "loss": 0.0042,
      "step": 352
    },
    {
      "epoch": 32.84,
      "grad_norm": 0.06180822476744652,
      "learning_rate": 0.0012952952952952953,
      "loss": 0.0037,
      "step": 353
    },
    {
      "epoch": 32.93,
      "grad_norm": 0.04855296388268471,
      "learning_rate": 0.0012932932932932933,
      "loss": 0.0032,
      "step": 354
    },
    {
      "epoch": 33.02,
      "grad_norm": 0.05172926187515259,
      "learning_rate": 0.0012912912912912914,
      "loss": 0.0039,
      "step": 355
    },
    {
      "epoch": 33.12,
      "grad_norm": 0.03115912713110447,
      "learning_rate": 0.0012892892892892892,
      "loss": 0.0017,
      "step": 356
    },
    {
      "epoch": 33.21,
      "grad_norm": 0.03118099458515644,
      "learning_rate": 0.0012872872872872873,
      "loss": 0.0017,
      "step": 357
    },
    {
      "epoch": 33.3,
      "grad_norm": 0.09290407598018646,
      "learning_rate": 0.0012852852852852851,
      "loss": 0.0027,
      "step": 358
    },
    {
      "epoch": 33.4,
      "grad_norm": 0.062028996646404266,
      "learning_rate": 0.0012832832832832832,
      "loss": 0.003,
      "step": 359
    },
    {
      "epoch": 33.49,
      "grad_norm": 0.06242119520902634,
      "learning_rate": 0.0012812812812812813,
      "loss": 0.0037,
      "step": 360
    },
    {
      "epoch": 33.58,
      "grad_norm": 0.04311477020382881,
      "learning_rate": 0.0012792792792792793,
      "loss": 0.0024,
      "step": 361
    },
    {
      "epoch": 33.67,
      "grad_norm": 0.04930288344621658,
      "learning_rate": 0.0012772772772772774,
      "loss": 0.0025,
      "step": 362
    },
    {
      "epoch": 33.77,
      "grad_norm": 0.03412387892603874,
      "learning_rate": 0.0012752752752752752,
      "loss": 0.0026,
      "step": 363
    },
    {
      "epoch": 33.86,
      "grad_norm": 0.08664079010486603,
      "learning_rate": 0.0012732732732732733,
      "loss": 0.0066,
      "step": 364
    },
    {
      "epoch": 33.95,
      "grad_norm": 0.8719766736030579,
      "learning_rate": 0.0012712712712712713,
      "loss": 0.0064,
      "step": 365
    },
    {
      "epoch": 34.05,
      "grad_norm": 0.07285726815462112,
      "learning_rate": 0.0012692692692692694,
      "loss": 0.0034,
      "step": 366
    },
    {
      "epoch": 34.14,
      "grad_norm": 0.16661174595355988,
      "learning_rate": 0.0012672672672672674,
      "loss": 0.0131,
      "step": 367
    },
    {
      "epoch": 34.23,
      "grad_norm": 0.21597643196582794,
      "learning_rate": 0.0012652652652652653,
      "loss": 0.016,
      "step": 368
    },
    {
      "epoch": 34.33,
      "grad_norm": 0.19439759850502014,
      "learning_rate": 0.0012632632632632633,
      "loss": 0.0158,
      "step": 369
    },
    {
      "epoch": 34.42,
      "grad_norm": 0.11315418034791946,
      "learning_rate": 0.0012612612612612614,
      "loss": 0.0062,
      "step": 370
    },
    {
      "epoch": 34.51,
      "grad_norm": 0.10819867253303528,
      "learning_rate": 0.0012592592592592592,
      "loss": 0.0072,
      "step": 371
    },
    {
      "epoch": 34.6,
      "grad_norm": 0.11476534605026245,
      "learning_rate": 0.0012572572572572573,
      "loss": 0.008,
      "step": 372
    },
    {
      "epoch": 34.7,
      "grad_norm": 0.13075686991214752,
      "learning_rate": 0.0012552552552552551,
      "loss": 0.0091,
      "step": 373
    },
    {
      "epoch": 34.79,
      "grad_norm": 1.8338048458099365,
      "learning_rate": 0.0012532532532532532,
      "loss": 0.0338,
      "step": 374
    },
    {
      "epoch": 34.88,
      "grad_norm": 0.18288426101207733,
      "learning_rate": 0.0012512512512512512,
      "loss": 0.0177,
      "step": 375
    },
    {
      "epoch": 34.98,
      "grad_norm": 0.2959078252315521,
      "learning_rate": 0.0012492492492492493,
      "loss": 0.0319,
      "step": 376
    },
    {
      "epoch": 35.07,
      "grad_norm": 0.2056169956922531,
      "learning_rate": 0.0012472472472472474,
      "loss": 0.0208,
      "step": 377
    },
    {
      "epoch": 35.16,
      "grad_norm": 0.21074797213077545,
      "learning_rate": 0.0012452452452452452,
      "loss": 0.0233,
      "step": 378
    },
    {
      "epoch": 35.26,
      "grad_norm": 0.15737591683864594,
      "learning_rate": 0.0012432432432432433,
      "loss": 0.0172,
      "step": 379
    },
    {
      "epoch": 35.35,
      "grad_norm": 0.17381712794303894,
      "learning_rate": 0.0012412412412412413,
      "loss": 0.0199,
      "step": 380
    },
    {
      "epoch": 35.44,
      "grad_norm": 0.17823565006256104,
      "learning_rate": 0.0012392392392392394,
      "loss": 0.0206,
      "step": 381
    },
    {
      "epoch": 35.53,
      "grad_norm": 0.15753379464149475,
      "learning_rate": 0.0012372372372372374,
      "loss": 0.0186,
      "step": 382
    },
    {
      "epoch": 35.63,
      "grad_norm": 0.11811896413564682,
      "learning_rate": 0.0012352352352352353,
      "loss": 0.0135,
      "step": 383
    },
    {
      "epoch": 35.72,
      "grad_norm": 0.15179665386676788,
      "learning_rate": 0.0012332332332332333,
      "loss": 0.0156,
      "step": 384
    },
    {
      "epoch": 35.81,
      "grad_norm": 0.156512051820755,
      "learning_rate": 0.0012312312312312312,
      "loss": 0.0117,
      "step": 385
    },
    {
      "epoch": 35.91,
      "grad_norm": 0.18240876495838165,
      "learning_rate": 0.0012292292292292292,
      "loss": 0.0184,
      "step": 386
    },
    {
      "epoch": 36.0,
      "grad_norm": 0.12744146585464478,
      "learning_rate": 0.0012272272272272273,
      "loss": 0.0104,
      "step": 387
    },
    {
      "epoch": 36.09,
      "grad_norm": 0.11735134571790695,
      "learning_rate": 0.0012252252252252251,
      "loss": 0.0088,
      "step": 388
    },
    {
      "epoch": 36.19,
      "grad_norm": 0.10435677319765091,
      "learning_rate": 0.0012232232232232232,
      "loss": 0.0089,
      "step": 389
    },
    {
      "epoch": 36.28,
      "grad_norm": 0.1069398745894432,
      "learning_rate": 0.0012212212212212212,
      "loss": 0.0074,
      "step": 390
    },
    {
      "epoch": 36.37,
      "grad_norm": 0.20668792724609375,
      "learning_rate": 0.0012192192192192193,
      "loss": 0.0113,
      "step": 391
    },
    {
      "epoch": 36.47,
      "grad_norm": 0.12832938134670258,
      "learning_rate": 0.0012172172172172173,
      "loss": 0.013,
      "step": 392
    },
    {
      "epoch": 36.56,
      "grad_norm": 0.13163897395133972,
      "learning_rate": 0.0012152152152152152,
      "loss": 0.0102,
      "step": 393
    },
    {
      "epoch": 36.65,
      "grad_norm": 0.14289605617523193,
      "learning_rate": 0.0012132132132132132,
      "loss": 0.0135,
      "step": 394
    },
    {
      "epoch": 36.74,
      "grad_norm": 0.1352647840976715,
      "learning_rate": 0.0012112112112112113,
      "loss": 0.0131,
      "step": 395
    },
    {
      "epoch": 36.84,
      "grad_norm": 0.1026504635810852,
      "learning_rate": 0.0012092092092092094,
      "loss": 0.0091,
      "step": 396
    },
    {
      "epoch": 36.93,
      "grad_norm": 0.13999910652637482,
      "learning_rate": 0.0012072072072072074,
      "loss": 0.012,
      "step": 397
    },
    {
      "epoch": 37.02,
      "grad_norm": 0.13585957884788513,
      "learning_rate": 0.0012052052052052053,
      "loss": 0.0086,
      "step": 398
    },
    {
      "epoch": 37.12,
      "grad_norm": 0.06898730993270874,
      "learning_rate": 0.0012032032032032033,
      "loss": 0.0044,
      "step": 399
    },
    {
      "epoch": 37.21,
      "grad_norm": 0.06994560360908508,
      "learning_rate": 0.0012012012012012011,
      "loss": 0.0051,
      "step": 400
    },
    {
      "epoch": 37.3,
      "grad_norm": 0.1387774795293808,
      "learning_rate": 0.0011991991991991992,
      "loss": 0.0112,
      "step": 401
    },
    {
      "epoch": 37.4,
      "grad_norm": 0.08667416870594025,
      "learning_rate": 0.0011971971971971973,
      "loss": 0.0088,
      "step": 402
    },
    {
      "epoch": 37.49,
      "grad_norm": 0.08598697185516357,
      "learning_rate": 0.001195195195195195,
      "loss": 0.008,
      "step": 403
    },
    {
      "epoch": 37.58,
      "grad_norm": 0.0907205268740654,
      "learning_rate": 0.0011931931931931932,
      "loss": 0.0062,
      "step": 404
    },
    {
      "epoch": 37.67,
      "grad_norm": 0.08570162951946259,
      "learning_rate": 0.0011911911911911912,
      "loss": 0.0084,
      "step": 405
    },
    {
      "epoch": 37.77,
      "grad_norm": 0.10218426585197449,
      "learning_rate": 0.0011891891891891893,
      "loss": 0.0076,
      "step": 406
    },
    {
      "epoch": 37.86,
      "grad_norm": 0.10739883780479431,
      "learning_rate": 0.0011871871871871871,
      "loss": 0.0098,
      "step": 407
    },
    {
      "epoch": 37.95,
      "grad_norm": 0.11295542865991592,
      "learning_rate": 0.0011851851851851852,
      "loss": 0.0096,
      "step": 408
    },
    {
      "epoch": 38.05,
      "grad_norm": 0.0802343562245369,
      "learning_rate": 0.0011831831831831832,
      "loss": 0.0056,
      "step": 409
    },
    {
      "epoch": 38.14,
      "grad_norm": 0.09044903516769409,
      "learning_rate": 0.0011811811811811813,
      "loss": 0.0061,
      "step": 410
    },
    {
      "epoch": 38.23,
      "grad_norm": 0.08450707793235779,
      "learning_rate": 0.0011791791791791793,
      "loss": 0.006,
      "step": 411
    },
    {
      "epoch": 38.33,
      "grad_norm": 0.05670813471078873,
      "learning_rate": 0.0011771771771771772,
      "loss": 0.0041,
      "step": 412
    },
    {
      "epoch": 38.42,
      "grad_norm": 0.0767657682299614,
      "learning_rate": 0.0011751751751751752,
      "loss": 0.0059,
      "step": 413
    },
    {
      "epoch": 38.51,
      "grad_norm": 0.08444706350564957,
      "learning_rate": 0.001173173173173173,
      "loss": 0.006,
      "step": 414
    },
    {
      "epoch": 38.6,
      "grad_norm": 0.08652391284704208,
      "learning_rate": 0.0011711711711711711,
      "loss": 0.006,
      "step": 415
    },
    {
      "epoch": 38.7,
      "grad_norm": 0.09136863797903061,
      "learning_rate": 0.0011691691691691692,
      "loss": 0.0057,
      "step": 416
    },
    {
      "epoch": 38.79,
      "grad_norm": 0.083625428378582,
      "learning_rate": 0.001167167167167167,
      "loss": 0.0076,
      "step": 417
    },
    {
      "epoch": 38.88,
      "grad_norm": 0.07047466188669205,
      "learning_rate": 0.001165165165165165,
      "loss": 0.0042,
      "step": 418
    },
    {
      "epoch": 38.98,
      "grad_norm": 0.07656373828649521,
      "learning_rate": 0.0011631631631631631,
      "loss": 0.0057,
      "step": 419
    },
    {
      "epoch": 39.07,
      "grad_norm": 0.06967911869287491,
      "learning_rate": 0.0011611611611611612,
      "loss": 0.0036,
      "step": 420
    },
    {
      "epoch": 39.16,
      "grad_norm": 0.051445458084344864,
      "learning_rate": 0.0011591591591591593,
      "loss": 0.0028,
      "step": 421
    },
    {
      "epoch": 39.26,
      "grad_norm": 0.0559643991291523,
      "learning_rate": 0.001157157157157157,
      "loss": 0.0032,
      "step": 422
    },
    {
      "epoch": 39.35,
      "grad_norm": 0.03927305340766907,
      "learning_rate": 0.0011551551551551552,
      "loss": 0.0023,
      "step": 423
    },
    {
      "epoch": 39.44,
      "grad_norm": 0.057706110179424286,
      "learning_rate": 0.0011531531531531532,
      "loss": 0.0031,
      "step": 424
    },
    {
      "epoch": 39.53,
      "grad_norm": 0.06381837278604507,
      "learning_rate": 0.0011511511511511513,
      "loss": 0.0032,
      "step": 425
    },
    {
      "epoch": 39.63,
      "grad_norm": 0.05114300921559334,
      "learning_rate": 0.0011491491491491493,
      "loss": 0.0021,
      "step": 426
    },
    {
      "epoch": 39.72,
      "grad_norm": 0.07307816296815872,
      "learning_rate": 0.0011471471471471472,
      "loss": 0.0039,
      "step": 427
    },
    {
      "epoch": 39.81,
      "grad_norm": 0.05135943368077278,
      "learning_rate": 0.0011451451451451452,
      "loss": 0.0039,
      "step": 428
    },
    {
      "epoch": 39.91,
      "grad_norm": 0.0514879934489727,
      "learning_rate": 0.001143143143143143,
      "loss": 0.0026,
      "step": 429
    },
    {
      "epoch": 40.0,
      "grad_norm": 0.08439819514751434,
      "learning_rate": 0.0011411411411411411,
      "loss": 0.0038,
      "step": 430
    },
    {
      "epoch": 40.09,
      "grad_norm": 0.036230411380529404,
      "learning_rate": 0.0011391391391391392,
      "loss": 0.0016,
      "step": 431
    },
    {
      "epoch": 40.19,
      "grad_norm": 0.06861860305070877,
      "learning_rate": 0.001137137137137137,
      "loss": 0.0031,
      "step": 432
    },
    {
      "epoch": 40.28,
      "grad_norm": 0.03462895378470421,
      "learning_rate": 0.001135135135135135,
      "loss": 0.0015,
      "step": 433
    },
    {
      "epoch": 40.37,
      "grad_norm": 0.033459682017564774,
      "learning_rate": 0.0011331331331331331,
      "loss": 0.0016,
      "step": 434
    },
    {
      "epoch": 40.47,
      "grad_norm": 0.05087810382246971,
      "learning_rate": 0.0011311311311311312,
      "loss": 0.003,
      "step": 435
    },
    {
      "epoch": 40.56,
      "grad_norm": 0.06251133978366852,
      "learning_rate": 0.0011291291291291292,
      "loss": 0.0033,
      "step": 436
    },
    {
      "epoch": 40.65,
      "grad_norm": 0.057716287672519684,
      "learning_rate": 0.001127127127127127,
      "loss": 0.003,
      "step": 437
    },
    {
      "epoch": 40.74,
      "grad_norm": 0.04524217173457146,
      "learning_rate": 0.0011251251251251251,
      "loss": 0.0021,
      "step": 438
    },
    {
      "epoch": 40.84,
      "grad_norm": 0.05485425889492035,
      "learning_rate": 0.0011231231231231232,
      "loss": 0.0023,
      "step": 439
    },
    {
      "epoch": 40.93,
      "grad_norm": 0.04494103044271469,
      "learning_rate": 0.0011211211211211213,
      "loss": 0.002,
      "step": 440
    },
    {
      "epoch": 41.02,
      "grad_norm": 0.06474560499191284,
      "learning_rate": 0.0011191191191191193,
      "loss": 0.0031,
      "step": 441
    },
    {
      "epoch": 41.12,
      "grad_norm": 0.032112184911966324,
      "learning_rate": 0.0011171171171171172,
      "loss": 0.0015,
      "step": 442
    },
    {
      "epoch": 41.21,
      "grad_norm": 0.07855093479156494,
      "learning_rate": 0.001115115115115115,
      "loss": 0.003,
      "step": 443
    },
    {
      "epoch": 41.3,
      "grad_norm": 0.05911662057042122,
      "learning_rate": 0.001113113113113113,
      "loss": 0.0024,
      "step": 444
    },
    {
      "epoch": 41.4,
      "grad_norm": 0.07458988577127457,
      "learning_rate": 0.0011111111111111111,
      "loss": 0.0019,
      "step": 445
    },
    {
      "epoch": 41.49,
      "grad_norm": 0.06002940610051155,
      "learning_rate": 0.0011091091091091092,
      "loss": 0.0029,
      "step": 446
    },
    {
      "epoch": 41.58,
      "grad_norm": 0.04031607136130333,
      "learning_rate": 0.001107107107107107,
      "loss": 0.0016,
      "step": 447
    },
    {
      "epoch": 41.67,
      "grad_norm": 0.05871531367301941,
      "learning_rate": 0.001105105105105105,
      "loss": 0.0029,
      "step": 448
    },
    {
      "epoch": 41.77,
      "grad_norm": 0.06386204808950424,
      "learning_rate": 0.0011031031031031031,
      "loss": 0.0022,
      "step": 449
    },
    {
      "epoch": 41.86,
      "grad_norm": 0.031453412026166916,
      "learning_rate": 0.0011011011011011012,
      "loss": 0.0017,
      "step": 450
    },
    {
      "epoch": 41.95,
      "grad_norm": 0.0686318650841713,
      "learning_rate": 0.0010990990990990992,
      "loss": 0.005,
      "step": 451
    },
    {
      "epoch": 42.05,
      "grad_norm": 0.044364456087350845,
      "learning_rate": 0.001097097097097097,
      "loss": 0.0025,
      "step": 452
    },
    {
      "epoch": 42.14,
      "grad_norm": 0.07827119529247284,
      "learning_rate": 0.0010950950950950951,
      "loss": 0.004,
      "step": 453
    },
    {
      "epoch": 42.23,
      "grad_norm": 0.01674993522465229,
      "learning_rate": 0.0010930930930930932,
      "loss": 0.0013,
      "step": 454
    },
    {
      "epoch": 42.33,
      "grad_norm": 0.041024889796972275,
      "learning_rate": 0.0010910910910910912,
      "loss": 0.002,
      "step": 455
    },
    {
      "epoch": 42.42,
      "grad_norm": 0.04098690673708916,
      "learning_rate": 0.0010890890890890893,
      "loss": 0.0023,
      "step": 456
    },
    {
      "epoch": 42.51,
      "grad_norm": 0.05196992680430412,
      "learning_rate": 0.001087087087087087,
      "loss": 0.0024,
      "step": 457
    },
    {
      "epoch": 42.6,
      "grad_norm": 0.038588881492614746,
      "learning_rate": 0.001085085085085085,
      "loss": 0.0017,
      "step": 458
    },
    {
      "epoch": 42.7,
      "grad_norm": 0.044969286769628525,
      "learning_rate": 0.001083083083083083,
      "loss": 0.0023,
      "step": 459
    },
    {
      "epoch": 42.79,
      "grad_norm": 0.07375585287809372,
      "learning_rate": 0.001081081081081081,
      "loss": 0.0026,
      "step": 460
    },
    {
      "epoch": 42.88,
      "grad_norm": 0.0510486401617527,
      "learning_rate": 0.0010790790790790792,
      "loss": 0.003,
      "step": 461
    },
    {
      "epoch": 42.98,
      "grad_norm": 0.04110394045710564,
      "learning_rate": 0.001077077077077077,
      "loss": 0.0032,
      "step": 462
    },
    {
      "epoch": 43.07,
      "grad_norm": 0.024054836481809616,
      "learning_rate": 0.001075075075075075,
      "loss": 0.002,
      "step": 463
    },
    {
      "epoch": 43.16,
      "grad_norm": 0.03442402184009552,
      "learning_rate": 0.0010730730730730731,
      "loss": 0.0015,
      "step": 464
    },
    {
      "epoch": 43.26,
      "grad_norm": 0.03139084577560425,
      "learning_rate": 0.0010710710710710712,
      "loss": 0.0018,
      "step": 465
    },
    {
      "epoch": 43.35,
      "grad_norm": 0.04958639666438103,
      "learning_rate": 0.0010690690690690692,
      "loss": 0.0026,
      "step": 466
    },
    {
      "epoch": 43.44,
      "grad_norm": 0.03323910012841225,
      "learning_rate": 0.001067067067067067,
      "loss": 0.0016,
      "step": 467
    },
    {
      "epoch": 43.53,
      "grad_norm": 0.038213685154914856,
      "learning_rate": 0.0010650650650650651,
      "loss": 0.0019,
      "step": 468
    },
    {
      "epoch": 43.63,
      "grad_norm": 0.07056744396686554,
      "learning_rate": 0.0010630630630630632,
      "loss": 0.0022,
      "step": 469
    },
    {
      "epoch": 43.72,
      "grad_norm": 0.03680761158466339,
      "learning_rate": 0.0010610610610610612,
      "loss": 0.0014,
      "step": 470
    },
    {
      "epoch": 43.81,
      "grad_norm": 0.05797829478979111,
      "learning_rate": 0.001059059059059059,
      "loss": 0.0031,
      "step": 471
    },
    {
      "epoch": 43.91,
      "grad_norm": 0.03306601941585541,
      "learning_rate": 0.001057057057057057,
      "loss": 0.0023,
      "step": 472
    },
    {
      "epoch": 44.0,
      "grad_norm": 0.03531021997332573,
      "learning_rate": 0.001055055055055055,
      "loss": 0.0029,
      "step": 473
    },
    {
      "epoch": 44.09,
      "grad_norm": 0.03529014438390732,
      "learning_rate": 0.001053053053053053,
      "loss": 0.0018,
      "step": 474
    },
    {
      "epoch": 44.19,
      "grad_norm": 0.029707856476306915,
      "learning_rate": 0.001051051051051051,
      "loss": 0.0013,
      "step": 475
    },
    {
      "epoch": 44.28,
      "grad_norm": 0.038505978882312775,
      "learning_rate": 0.001049049049049049,
      "loss": 0.0019,
      "step": 476
    },
    {
      "epoch": 44.37,
      "grad_norm": 0.03348147124052048,
      "learning_rate": 0.001047047047047047,
      "loss": 0.0016,
      "step": 477
    },
    {
      "epoch": 44.47,
      "grad_norm": 0.03438611328601837,
      "learning_rate": 0.001045045045045045,
      "loss": 0.0018,
      "step": 478
    },
    {
      "epoch": 44.56,
      "grad_norm": 0.041374336928129196,
      "learning_rate": 0.001043043043043043,
      "loss": 0.0023,
      "step": 479
    },
    {
      "epoch": 44.65,
      "grad_norm": 0.03722222149372101,
      "learning_rate": 0.0010410410410410412,
      "loss": 0.0021,
      "step": 480
    },
    {
      "epoch": 44.74,
      "grad_norm": 0.027286631986498833,
      "learning_rate": 0.001039039039039039,
      "loss": 0.0016,
      "step": 481
    },
    {
      "epoch": 44.84,
      "grad_norm": 0.016527727246284485,
      "learning_rate": 0.001037037037037037,
      "loss": 0.0012,
      "step": 482
    },
    {
      "epoch": 44.93,
      "grad_norm": 0.06069435924291611,
      "learning_rate": 0.001035035035035035,
      "loss": 0.0033,
      "step": 483
    },
    {
      "epoch": 45.02,
      "grad_norm": 0.04517413303256035,
      "learning_rate": 0.0010330330330330332,
      "loss": 0.0024,
      "step": 484
    },
    {
      "epoch": 45.12,
      "grad_norm": 0.024984322488307953,
      "learning_rate": 0.0010310310310310312,
      "loss": 0.0013,
      "step": 485
    },
    {
      "epoch": 45.21,
      "grad_norm": 0.03412657603621483,
      "learning_rate": 0.0010290290290290288,
      "loss": 0.0015,
      "step": 486
    },
    {
      "epoch": 45.3,
      "grad_norm": 0.022605758160352707,
      "learning_rate": 0.001027027027027027,
      "loss": 0.0015,
      "step": 487
    },
    {
      "epoch": 45.4,
      "grad_norm": 0.034697774797677994,
      "learning_rate": 0.001025025025025025,
      "loss": 0.0019,
      "step": 488
    },
    {
      "epoch": 45.49,
      "grad_norm": 0.031802624464035034,
      "learning_rate": 0.001023023023023023,
      "loss": 0.0017,
      "step": 489
    },
    {
      "epoch": 45.58,
      "grad_norm": 0.04652908816933632,
      "learning_rate": 0.001021021021021021,
      "loss": 0.0015,
      "step": 490
    },
    {
      "epoch": 45.67,
      "grad_norm": 0.011993377469480038,
      "learning_rate": 0.001019019019019019,
      "loss": 0.0012,
      "step": 491
    },
    {
      "epoch": 45.77,
      "grad_norm": 0.062344640493392944,
      "learning_rate": 0.001017017017017017,
      "loss": 0.0019,
      "step": 492
    },
    {
      "epoch": 45.86,
      "grad_norm": 0.030106155201792717,
      "learning_rate": 0.001015015015015015,
      "loss": 0.0016,
      "step": 493
    },
    {
      "epoch": 45.95,
      "grad_norm": 0.04093213379383087,
      "learning_rate": 0.001013013013013013,
      "loss": 0.0018,
      "step": 494
    },
    {
      "epoch": 46.05,
      "grad_norm": 0.02282726764678955,
      "learning_rate": 0.0010110110110110111,
      "loss": 0.0015,
      "step": 495
    },
    {
      "epoch": 46.14,
      "grad_norm": 0.04156970977783203,
      "learning_rate": 0.001009009009009009,
      "loss": 0.0015,
      "step": 496
    },
    {
      "epoch": 46.23,
      "grad_norm": 0.030880684033036232,
      "learning_rate": 0.001007007007007007,
      "loss": 0.0027,
      "step": 497
    },
    {
      "epoch": 46.33,
      "grad_norm": 0.042682062834501266,
      "learning_rate": 0.001005005005005005,
      "loss": 0.0025,
      "step": 498
    },
    {
      "epoch": 46.42,
      "grad_norm": 0.009067803621292114,
      "learning_rate": 0.0010030030030030032,
      "loss": 0.0012,
      "step": 499
    },
    {
      "epoch": 46.51,
      "grad_norm": 0.014194684103131294,
      "learning_rate": 0.0010010010010010012,
      "loss": 0.0011,
      "step": 500
    },
    {
      "epoch": 46.6,
      "grad_norm": 0.030148006975650787,
      "learning_rate": 0.000998998998998999,
      "loss": 0.0014,
      "step": 501
    },
    {
      "epoch": 46.7,
      "grad_norm": 0.008643808774650097,
      "learning_rate": 0.0009969969969969969,
      "loss": 0.0011,
      "step": 502
    },
    {
      "epoch": 46.79,
      "grad_norm": 0.011928128078579903,
      "learning_rate": 0.000994994994994995,
      "loss": 0.0011,
      "step": 503
    },
    {
      "epoch": 46.88,
      "grad_norm": 0.031719814985990524,
      "learning_rate": 0.000992992992992993,
      "loss": 0.0022,
      "step": 504
    },
    {
      "epoch": 46.98,
      "grad_norm": 0.01134156621992588,
      "learning_rate": 0.000990990990990991,
      "loss": 0.0011,
      "step": 505
    },
    {
      "epoch": 47.07,
      "grad_norm": 0.011974232271313667,
      "learning_rate": 0.0009889889889889891,
      "loss": 0.0011,
      "step": 506
    },
    {
      "epoch": 47.16,
      "grad_norm": 0.009685841389000416,
      "learning_rate": 0.000986986986986987,
      "loss": 0.001,
      "step": 507
    },
    {
      "epoch": 47.26,
      "grad_norm": 0.008041341789066792,
      "learning_rate": 0.000984984984984985,
      "loss": 0.001,
      "step": 508
    },
    {
      "epoch": 47.35,
      "grad_norm": 0.010544364340603352,
      "learning_rate": 0.000982982982982983,
      "loss": 0.0012,
      "step": 509
    },
    {
      "epoch": 47.44,
      "grad_norm": 0.008778706192970276,
      "learning_rate": 0.000980980980980981,
      "loss": 0.0011,
      "step": 510
    },
    {
      "epoch": 47.53,
      "grad_norm": 0.007793562486767769,
      "learning_rate": 0.000978978978978979,
      "loss": 0.001,
      "step": 511
    },
    {
      "epoch": 47.63,
      "grad_norm": 0.010472383350133896,
      "learning_rate": 0.000976976976976977,
      "loss": 0.0011,
      "step": 512
    },
    {
      "epoch": 47.72,
      "grad_norm": 0.00800417922437191,
      "learning_rate": 0.000974974974974975,
      "loss": 0.001,
      "step": 513
    },
    {
      "epoch": 47.81,
      "grad_norm": 0.010395576246082783,
      "learning_rate": 0.000972972972972973,
      "loss": 0.0011,
      "step": 514
    },
    {
      "epoch": 47.91,
      "grad_norm": 0.031162628903985023,
      "learning_rate": 0.000970970970970971,
      "loss": 0.0013,
      "step": 515
    },
    {
      "epoch": 48.0,
      "grad_norm": 0.008229011669754982,
      "learning_rate": 0.000968968968968969,
      "loss": 0.0011,
      "step": 516
    },
    {
      "epoch": 48.09,
      "grad_norm": 0.005693014711141586,
      "learning_rate": 0.000966966966966967,
      "loss": 0.0009,
      "step": 517
    },
    {
      "epoch": 48.19,
      "grad_norm": 0.00472146924585104,
      "learning_rate": 0.000964964964964965,
      "loss": 0.0009,
      "step": 518
    },
    {
      "epoch": 48.28,
      "grad_norm": 0.006739554926753044,
      "learning_rate": 0.0009629629629629629,
      "loss": 0.0009,
      "step": 519
    },
    {
      "epoch": 48.37,
      "grad_norm": 0.006326691247522831,
      "learning_rate": 0.0009609609609609609,
      "loss": 0.0009,
      "step": 520
    },
    {
      "epoch": 48.47,
      "grad_norm": 0.004322423599660397,
      "learning_rate": 0.000958958958958959,
      "loss": 0.0009,
      "step": 521
    },
    {
      "epoch": 48.56,
      "grad_norm": 0.005275717470794916,
      "learning_rate": 0.0009569569569569569,
      "loss": 0.001,
      "step": 522
    },
    {
      "epoch": 48.65,
      "grad_norm": 0.007286928128451109,
      "learning_rate": 0.000954954954954955,
      "loss": 0.0011,
      "step": 523
    },
    {
      "epoch": 48.74,
      "grad_norm": 0.009518984705209732,
      "learning_rate": 0.000952952952952953,
      "loss": 0.001,
      "step": 524
    },
    {
      "epoch": 48.84,
      "grad_norm": 0.0076581742614507675,
      "learning_rate": 0.000950950950950951,
      "loss": 0.0011,
      "step": 525
    },
    {
      "epoch": 48.93,
      "grad_norm": 0.008818943053483963,
      "learning_rate": 0.000948948948948949,
      "loss": 0.0012,
      "step": 526
    },
    {
      "epoch": 49.02,
      "grad_norm": 0.005841933190822601,
      "learning_rate": 0.0009469469469469469,
      "loss": 0.001,
      "step": 527
    },
    {
      "epoch": 49.12,
      "grad_norm": 0.007067856378853321,
      "learning_rate": 0.000944944944944945,
      "loss": 0.001,
      "step": 528
    },
    {
      "epoch": 49.21,
      "grad_norm": 0.022261235862970352,
      "learning_rate": 0.0009429429429429429,
      "loss": 0.0009,
      "step": 529
    },
    {
      "epoch": 49.3,
      "grad_norm": 0.005122487433254719,
      "learning_rate": 0.000940940940940941,
      "loss": 0.0009,
      "step": 530
    },
    {
      "epoch": 49.4,
      "grad_norm": 0.005736043211072683,
      "learning_rate": 0.000938938938938939,
      "loss": 0.001,
      "step": 531
    },
    {
      "epoch": 49.49,
      "grad_norm": 0.005500317085534334,
      "learning_rate": 0.000936936936936937,
      "loss": 0.0009,
      "step": 532
    },
    {
      "epoch": 49.58,
      "grad_norm": 0.005690107587724924,
      "learning_rate": 0.000934934934934935,
      "loss": 0.001,
      "step": 533
    },
    {
      "epoch": 49.67,
      "grad_norm": 0.01583140902221203,
      "learning_rate": 0.0009329329329329329,
      "loss": 0.0011,
      "step": 534
    },
    {
      "epoch": 49.77,
      "grad_norm": 0.006315015256404877,
      "learning_rate": 0.0009309309309309309,
      "loss": 0.001,
      "step": 535
    },
    {
      "epoch": 49.86,
      "grad_norm": 0.007522172760218382,
      "learning_rate": 0.000928928928928929,
      "loss": 0.001,
      "step": 536
    },
    {
      "epoch": 49.95,
      "grad_norm": 0.005803990643471479,
      "learning_rate": 0.0009269269269269269,
      "loss": 0.001,
      "step": 537
    },
    {
      "epoch": 50.05,
      "grad_norm": 0.006560762412846088,
      "learning_rate": 0.000924924924924925,
      "loss": 0.001,
      "step": 538
    },
    {
      "epoch": 50.14,
      "grad_norm": 0.005123397801071405,
      "learning_rate": 0.0009229229229229229,
      "loss": 0.0009,
      "step": 539
    },
    {
      "epoch": 50.23,
      "grad_norm": 0.005609103478491306,
      "learning_rate": 0.000920920920920921,
      "loss": 0.0009,
      "step": 540
    },
    {
      "epoch": 50.33,
      "grad_norm": 0.004306567832827568,
      "learning_rate": 0.0009189189189189189,
      "loss": 0.0009,
      "step": 541
    },
    {
      "epoch": 50.42,
      "grad_norm": 0.018686862662434578,
      "learning_rate": 0.0009169169169169169,
      "loss": 0.0011,
      "step": 542
    },
    {
      "epoch": 50.51,
      "grad_norm": 0.00628719013184309,
      "learning_rate": 0.000914914914914915,
      "loss": 0.0009,
      "step": 543
    },
    {
      "epoch": 50.6,
      "grad_norm": 0.005820378195494413,
      "learning_rate": 0.0009129129129129129,
      "loss": 0.001,
      "step": 544
    },
    {
      "epoch": 50.7,
      "grad_norm": 0.007535787299275398,
      "learning_rate": 0.000910910910910911,
      "loss": 0.001,
      "step": 545
    },
    {
      "epoch": 50.79,
      "grad_norm": 0.006216641515493393,
      "learning_rate": 0.000908908908908909,
      "loss": 0.001,
      "step": 546
    },
    {
      "epoch": 50.88,
      "grad_norm": 0.0056008230894804,
      "learning_rate": 0.000906906906906907,
      "loss": 0.001,
      "step": 547
    },
    {
      "epoch": 50.98,
      "grad_norm": 0.006009987089782953,
      "learning_rate": 0.0009049049049049049,
      "loss": 0.001,
      "step": 548
    },
    {
      "epoch": 51.07,
      "grad_norm": 0.00538256112486124,
      "learning_rate": 0.0009029029029029029,
      "loss": 0.0009,
      "step": 549
    },
    {
      "epoch": 51.16,
      "grad_norm": 0.004297400824725628,
      "learning_rate": 0.0009009009009009009,
      "loss": 0.0009,
      "step": 550
    },
    {
      "epoch": 51.26,
      "grad_norm": 0.00530287204310298,
      "learning_rate": 0.0008988988988988989,
      "loss": 0.0009,
      "step": 551
    },
    {
      "epoch": 51.35,
      "grad_norm": 0.005018638446927071,
      "learning_rate": 0.0008968968968968969,
      "loss": 0.0009,
      "step": 552
    },
    {
      "epoch": 51.44,
      "grad_norm": 0.0048153214156627655,
      "learning_rate": 0.000894894894894895,
      "loss": 0.0009,
      "step": 553
    },
    {
      "epoch": 51.53,
      "grad_norm": 0.005478543229401112,
      "learning_rate": 0.0008928928928928929,
      "loss": 0.001,
      "step": 554
    },
    {
      "epoch": 51.63,
      "grad_norm": 0.006168559659272432,
      "learning_rate": 0.0008908908908908909,
      "loss": 0.0009,
      "step": 555
    },
    {
      "epoch": 51.72,
      "grad_norm": 0.004636559169739485,
      "learning_rate": 0.0008888888888888888,
      "loss": 0.001,
      "step": 556
    },
    {
      "epoch": 51.81,
      "grad_norm": 0.006312352139502764,
      "learning_rate": 0.0008868868868868869,
      "loss": 0.001,
      "step": 557
    },
    {
      "epoch": 51.91,
      "grad_norm": 0.005704349838197231,
      "learning_rate": 0.0008848848848848849,
      "loss": 0.001,
      "step": 558
    },
    {
      "epoch": 52.0,
      "grad_norm": 0.008049027994275093,
      "learning_rate": 0.0008828828828828829,
      "loss": 0.0011,
      "step": 559
    },
    {
      "epoch": 52.09,
      "grad_norm": 0.005900105927139521,
      "learning_rate": 0.0008808808808808809,
      "loss": 0.001,
      "step": 560
    },
    {
      "epoch": 52.19,
      "grad_norm": 0.006347515154629946,
      "learning_rate": 0.0008788788788788789,
      "loss": 0.0009,
      "step": 561
    },
    {
      "epoch": 52.28,
      "grad_norm": 0.006557863671332598,
      "learning_rate": 0.000876876876876877,
      "loss": 0.0009,
      "step": 562
    },
    {
      "epoch": 52.37,
      "grad_norm": 0.00601592194288969,
      "learning_rate": 0.0008748748748748749,
      "loss": 0.0009,
      "step": 563
    },
    {
      "epoch": 52.47,
      "grad_norm": 0.016323665156960487,
      "learning_rate": 0.0008728728728728728,
      "loss": 0.001,
      "step": 564
    },
    {
      "epoch": 52.56,
      "grad_norm": 0.004659431055188179,
      "learning_rate": 0.0008708708708708709,
      "loss": 0.001,
      "step": 565
    },
    {
      "epoch": 52.65,
      "grad_norm": 0.00462191179394722,
      "learning_rate": 0.0008688688688688689,
      "loss": 0.0009,
      "step": 566
    },
    {
      "epoch": 52.74,
      "grad_norm": 0.005063162650913,
      "learning_rate": 0.0008668668668668669,
      "loss": 0.001,
      "step": 567
    },
    {
      "epoch": 52.84,
      "grad_norm": 0.007546721026301384,
      "learning_rate": 0.000864864864864865,
      "loss": 0.001,
      "step": 568
    },
    {
      "epoch": 52.93,
      "grad_norm": 0.0063664885237813,
      "learning_rate": 0.0008628628628628629,
      "loss": 0.001,
      "step": 569
    },
    {
      "epoch": 53.02,
      "grad_norm": 0.007820897735655308,
      "learning_rate": 0.0008608608608608609,
      "loss": 0.0011,
      "step": 570
    },
    {
      "epoch": 53.12,
      "grad_norm": 0.0044373441487550735,
      "learning_rate": 0.0008588588588588588,
      "loss": 0.0009,
      "step": 571
    },
    {
      "epoch": 53.21,
      "grad_norm": 0.005179597996175289,
      "learning_rate": 0.0008568568568568569,
      "loss": 0.0009,
      "step": 572
    },
    {
      "epoch": 53.3,
      "grad_norm": 0.004271842539310455,
      "learning_rate": 0.0008548548548548549,
      "loss": 0.0009,
      "step": 573
    },
    {
      "epoch": 53.4,
      "grad_norm": 0.005934496410191059,
      "learning_rate": 0.0008528528528528529,
      "loss": 0.0009,
      "step": 574
    },
    {
      "epoch": 53.49,
      "grad_norm": 0.005639166571199894,
      "learning_rate": 0.0008508508508508509,
      "loss": 0.001,
      "step": 575
    },
    {
      "epoch": 53.58,
      "grad_norm": 0.00574887078255415,
      "learning_rate": 0.0008488488488488489,
      "loss": 0.001,
      "step": 576
    },
    {
      "epoch": 53.67,
      "grad_norm": 0.0070459553971886635,
      "learning_rate": 0.0008468468468468468,
      "loss": 0.001,
      "step": 577
    },
    {
      "epoch": 53.77,
      "grad_norm": 0.0053420052863657475,
      "learning_rate": 0.0008448448448448449,
      "loss": 0.001,
      "step": 578
    },
    {
      "epoch": 53.86,
      "grad_norm": 0.005432996898889542,
      "learning_rate": 0.0008428428428428428,
      "loss": 0.001,
      "step": 579
    },
    {
      "epoch": 53.95,
      "grad_norm": 0.005831349175423384,
      "learning_rate": 0.0008408408408408409,
      "loss": 0.001,
      "step": 580
    },
    {
      "epoch": 54.05,
      "grad_norm": 0.00669609010219574,
      "learning_rate": 0.0008388388388388388,
      "loss": 0.001,
      "step": 581
    },
    {
      "epoch": 54.14,
      "grad_norm": 0.005738838575780392,
      "learning_rate": 0.0008368368368368369,
      "loss": 0.0009,
      "step": 582
    },
    {
      "epoch": 54.23,
      "grad_norm": 0.005119386129081249,
      "learning_rate": 0.0008348348348348348,
      "loss": 0.0009,
      "step": 583
    },
    {
      "epoch": 54.33,
      "grad_norm": 0.004580301232635975,
      "learning_rate": 0.0008328328328328328,
      "loss": 0.0009,
      "step": 584
    },
    {
      "epoch": 54.42,
      "grad_norm": 0.0041924030520021915,
      "learning_rate": 0.0008308308308308308,
      "loss": 0.0009,
      "step": 585
    },
    {
      "epoch": 54.51,
      "grad_norm": 0.005346381571143866,
      "learning_rate": 0.0008288288288288288,
      "loss": 0.001,
      "step": 586
    },
    {
      "epoch": 54.6,
      "grad_norm": 0.005660634953528643,
      "learning_rate": 0.0008268268268268269,
      "loss": 0.001,
      "step": 587
    },
    {
      "epoch": 54.7,
      "grad_norm": 0.005552636459469795,
      "learning_rate": 0.0008248248248248248,
      "loss": 0.0009,
      "step": 588
    },
    {
      "epoch": 54.79,
      "grad_norm": 0.005862879566848278,
      "learning_rate": 0.0008228228228228229,
      "loss": 0.0009,
      "step": 589
    },
    {
      "epoch": 54.88,
      "grad_norm": 0.007424304727464914,
      "learning_rate": 0.0008208208208208209,
      "loss": 0.001,
      "step": 590
    },
    {
      "epoch": 54.98,
      "grad_norm": 0.00554997380822897,
      "learning_rate": 0.0008188188188188188,
      "loss": 0.001,
      "step": 591
    },
    {
      "epoch": 55.07,
      "grad_norm": 0.003520325059071183,
      "learning_rate": 0.0008168168168168168,
      "loss": 0.0009,
      "step": 592
    },
    {
      "epoch": 55.16,
      "grad_norm": 0.004904530942440033,
      "learning_rate": 0.0008148148148148148,
      "loss": 0.0009,
      "step": 593
    },
    {
      "epoch": 55.26,
      "grad_norm": 0.0051420219242572784,
      "learning_rate": 0.0008128128128128128,
      "loss": 0.0009,
      "step": 594
    },
    {
      "epoch": 55.35,
      "grad_norm": 0.004829346667975187,
      "learning_rate": 0.0008108108108108109,
      "loss": 0.0009,
      "step": 595
    },
    {
      "epoch": 55.44,
      "grad_norm": 0.004913767799735069,
      "learning_rate": 0.0008088088088088088,
      "loss": 0.0009,
      "step": 596
    },
    {
      "epoch": 55.53,
      "grad_norm": 0.006039491388946772,
      "learning_rate": 0.0008068068068068069,
      "loss": 0.001,
      "step": 597
    },
    {
      "epoch": 55.63,
      "grad_norm": 0.005799802485853434,
      "learning_rate": 0.0008048048048048048,
      "loss": 0.0009,
      "step": 598
    },
    {
      "epoch": 55.72,
      "grad_norm": 0.007055402733385563,
      "learning_rate": 0.0008028028028028028,
      "loss": 0.001,
      "step": 599
    },
    {
      "epoch": 55.81,
      "grad_norm": 0.004408523440361023,
      "learning_rate": 0.0008008008008008008,
      "loss": 0.0009,
      "step": 600
    },
    {
      "epoch": 55.91,
      "grad_norm": 0.006718651857227087,
      "learning_rate": 0.0007987987987987988,
      "loss": 0.001,
      "step": 601
    },
    {
      "epoch": 56.0,
      "grad_norm": 0.005820451304316521,
      "learning_rate": 0.0007967967967967968,
      "loss": 0.001,
      "step": 602
    },
    {
      "epoch": 56.09,
      "grad_norm": 0.005731355398893356,
      "learning_rate": 0.0007947947947947948,
      "loss": 0.0008,
      "step": 603
    },
    {
      "epoch": 56.19,
      "grad_norm": 0.005305266939103603,
      "learning_rate": 0.0007927927927927928,
      "loss": 0.0009,
      "step": 604
    },
    {
      "epoch": 56.28,
      "grad_norm": 0.006083925720304251,
      "learning_rate": 0.0007907907907907909,
      "loss": 0.0009,
      "step": 605
    },
    {
      "epoch": 56.37,
      "grad_norm": 0.004002666100859642,
      "learning_rate": 0.0007887887887887887,
      "loss": 0.0009,
      "step": 606
    },
    {
      "epoch": 56.47,
      "grad_norm": 0.006040920503437519,
      "learning_rate": 0.0007867867867867868,
      "loss": 0.0009,
      "step": 607
    },
    {
      "epoch": 56.56,
      "grad_norm": 0.004592646844685078,
      "learning_rate": 0.0007847847847847848,
      "loss": 0.0009,
      "step": 608
    },
    {
      "epoch": 56.65,
      "grad_norm": 0.00515854824334383,
      "learning_rate": 0.0007827827827827828,
      "loss": 0.0009,
      "step": 609
    },
    {
      "epoch": 56.74,
      "grad_norm": 0.005443588364869356,
      "learning_rate": 0.0007807807807807808,
      "loss": 0.001,
      "step": 610
    },
    {
      "epoch": 56.84,
      "grad_norm": 0.005207471549510956,
      "learning_rate": 0.0007787787787787788,
      "loss": 0.0009,
      "step": 611
    },
    {
      "epoch": 56.93,
      "grad_norm": 0.0059023452922701836,
      "learning_rate": 0.0007767767767767769,
      "loss": 0.001,
      "step": 612
    },
    {
      "epoch": 57.02,
      "grad_norm": 0.005412122700363398,
      "learning_rate": 0.0007747747747747747,
      "loss": 0.001,
      "step": 613
    },
    {
      "epoch": 57.12,
      "grad_norm": 0.004014482256025076,
      "learning_rate": 0.0007727727727727728,
      "loss": 0.0009,
      "step": 614
    },
    {
      "epoch": 57.21,
      "grad_norm": 0.0038894633762538433,
      "learning_rate": 0.0007707707707707707,
      "loss": 0.0009,
      "step": 615
    },
    {
      "epoch": 57.3,
      "grad_norm": 0.005240234546363354,
      "learning_rate": 0.0007687687687687688,
      "loss": 0.0009,
      "step": 616
    },
    {
      "epoch": 57.4,
      "grad_norm": 0.004040597938001156,
      "learning_rate": 0.0007667667667667668,
      "loss": 0.0009,
      "step": 617
    },
    {
      "epoch": 57.49,
      "grad_norm": 0.005286985542625189,
      "learning_rate": 0.0007647647647647648,
      "loss": 0.0009,
      "step": 618
    },
    {
      "epoch": 57.58,
      "grad_norm": 0.004892811644822359,
      "learning_rate": 0.0007627627627627628,
      "loss": 0.0009,
      "step": 619
    },
    {
      "epoch": 57.67,
      "grad_norm": 0.005646155681461096,
      "learning_rate": 0.0007607607607607607,
      "loss": 0.001,
      "step": 620
    },
    {
      "epoch": 57.77,
      "grad_norm": 0.004856550600379705,
      "learning_rate": 0.0007587587587587587,
      "loss": 0.0009,
      "step": 621
    },
    {
      "epoch": 57.86,
      "grad_norm": 0.005464382469654083,
      "learning_rate": 0.0007567567567567568,
      "loss": 0.001,
      "step": 622
    },
    {
      "epoch": 57.95,
      "grad_norm": 0.004736950621008873,
      "learning_rate": 0.0007547547547547547,
      "loss": 0.001,
      "step": 623
    },
    {
      "epoch": 58.05,
      "grad_norm": 0.0054096076637506485,
      "learning_rate": 0.0007527527527527528,
      "loss": 0.001,
      "step": 624
    },
    {
      "epoch": 58.14,
      "grad_norm": 0.0046325973235070705,
      "learning_rate": 0.0007507507507507507,
      "loss": 0.0009,
      "step": 625
    },
    {
      "epoch": 58.23,
      "grad_norm": 0.00454937806352973,
      "learning_rate": 0.0007487487487487488,
      "loss": 0.0009,
      "step": 626
    },
    {
      "epoch": 58.33,
      "grad_norm": 0.004584938753396273,
      "learning_rate": 0.0007467467467467469,
      "loss": 0.0009,
      "step": 627
    },
    {
      "epoch": 58.42,
      "grad_norm": 0.005642315838485956,
      "learning_rate": 0.0007447447447447447,
      "loss": 0.0009,
      "step": 628
    },
    {
      "epoch": 58.51,
      "grad_norm": 0.0049066124483942986,
      "learning_rate": 0.0007427427427427428,
      "loss": 0.0009,
      "step": 629
    },
    {
      "epoch": 58.6,
      "grad_norm": 0.004979234654456377,
      "learning_rate": 0.0007407407407407407,
      "loss": 0.0009,
      "step": 630
    },
    {
      "epoch": 58.7,
      "grad_norm": 0.004991329275071621,
      "learning_rate": 0.0007387387387387388,
      "loss": 0.0009,
      "step": 631
    },
    {
      "epoch": 58.79,
      "grad_norm": 0.00438812468200922,
      "learning_rate": 0.0007367367367367368,
      "loss": 0.0009,
      "step": 632
    },
    {
      "epoch": 58.88,
      "grad_norm": 0.006005723960697651,
      "learning_rate": 0.0007347347347347348,
      "loss": 0.001,
      "step": 633
    },
    {
      "epoch": 58.98,
      "grad_norm": 0.004877123050391674,
      "learning_rate": 0.0007327327327327328,
      "loss": 0.001,
      "step": 634
    },
    {
      "epoch": 59.07,
      "grad_norm": 0.007415796164423227,
      "learning_rate": 0.0007307307307307307,
      "loss": 0.0009,
      "step": 635
    },
    {
      "epoch": 59.16,
      "grad_norm": 0.004727993626147509,
      "learning_rate": 0.0007287287287287287,
      "loss": 0.0009,
      "step": 636
    },
    {
      "epoch": 59.26,
      "grad_norm": 0.004820824600756168,
      "learning_rate": 0.0007267267267267268,
      "loss": 0.0009,
      "step": 637
    },
    {
      "epoch": 59.35,
      "grad_norm": 0.005232499446719885,
      "learning_rate": 0.0007247247247247247,
      "loss": 0.0009,
      "step": 638
    },
    {
      "epoch": 59.44,
      "grad_norm": 0.004785620607435703,
      "learning_rate": 0.0007227227227227228,
      "loss": 0.0009,
      "step": 639
    },
    {
      "epoch": 59.53,
      "grad_norm": 0.005055941641330719,
      "learning_rate": 0.0007207207207207207,
      "loss": 0.0009,
      "step": 640
    },
    {
      "epoch": 59.63,
      "grad_norm": 0.004847476724535227,
      "learning_rate": 0.0007187187187187188,
      "loss": 0.0009,
      "step": 641
    },
    {
      "epoch": 59.72,
      "grad_norm": 0.004750678315758705,
      "learning_rate": 0.0007167167167167166,
      "loss": 0.0009,
      "step": 642
    },
    {
      "epoch": 59.81,
      "grad_norm": 0.004703650716692209,
      "learning_rate": 0.0007147147147147147,
      "loss": 0.0009,
      "step": 643
    },
    {
      "epoch": 59.91,
      "grad_norm": 0.006315199192613363,
      "learning_rate": 0.0007127127127127127,
      "loss": 0.001,
      "step": 644
    },
    {
      "epoch": 60.0,
      "grad_norm": 0.005466056987643242,
      "learning_rate": 0.0007107107107107107,
      "loss": 0.001,
      "step": 645
    },
    {
      "epoch": 60.09,
      "grad_norm": 0.004128921311348677,
      "learning_rate": 0.0007087087087087087,
      "loss": 0.0009,
      "step": 646
    },
    {
      "epoch": 60.19,
      "grad_norm": 0.00519982585683465,
      "learning_rate": 0.0007067067067067067,
      "loss": 0.0009,
      "step": 647
    },
    {
      "epoch": 60.28,
      "grad_norm": 0.005540397949516773,
      "learning_rate": 0.0007047047047047048,
      "loss": 0.0009,
      "step": 648
    },
    {
      "epoch": 60.37,
      "grad_norm": 0.003583627287298441,
      "learning_rate": 0.0007027027027027027,
      "loss": 0.0009,
      "step": 649
    },
    {
      "epoch": 60.47,
      "grad_norm": 0.0053488630801439285,
      "learning_rate": 0.0007007007007007007,
      "loss": 0.0009,
      "step": 650
    },
    {
      "epoch": 60.56,
      "grad_norm": 0.005517234094440937,
      "learning_rate": 0.0006986986986986987,
      "loss": 0.0009,
      "step": 651
    },
    {
      "epoch": 60.65,
      "grad_norm": 0.006610073149204254,
      "learning_rate": 0.0006966966966966967,
      "loss": 0.0009,
      "step": 652
    },
    {
      "epoch": 60.74,
      "grad_norm": 0.006281854584813118,
      "learning_rate": 0.0006946946946946947,
      "loss": 0.0009,
      "step": 653
    },
    {
      "epoch": 60.84,
      "grad_norm": 0.0055662780068814754,
      "learning_rate": 0.0006926926926926928,
      "loss": 0.001,
      "step": 654
    },
    {
      "epoch": 60.93,
      "grad_norm": 0.0051424275152385235,
      "learning_rate": 0.0006906906906906907,
      "loss": 0.001,
      "step": 655
    },
    {
      "epoch": 61.02,
      "grad_norm": 0.0067566693760454655,
      "learning_rate": 0.0006886886886886888,
      "loss": 0.001,
      "step": 656
    },
    {
      "epoch": 61.12,
      "grad_norm": 0.0065893204882740974,
      "learning_rate": 0.0006866866866866866,
      "loss": 0.0009,
      "step": 657
    },
    {
      "epoch": 61.21,
      "grad_norm": 0.005402968265116215,
      "learning_rate": 0.0006846846846846847,
      "loss": 0.0009,
      "step": 658
    },
    {
      "epoch": 61.3,
      "grad_norm": 0.004769261460751295,
      "learning_rate": 0.0006826826826826827,
      "loss": 0.0009,
      "step": 659
    },
    {
      "epoch": 61.4,
      "grad_norm": 0.005315279122442007,
      "learning_rate": 0.0006806806806806807,
      "loss": 0.0009,
      "step": 660
    },
    {
      "epoch": 61.49,
      "grad_norm": 0.005348887760192156,
      "learning_rate": 0.0006786786786786787,
      "loss": 0.0009,
      "step": 661
    },
    {
      "epoch": 61.58,
      "grad_norm": 0.003994453232735395,
      "learning_rate": 0.0006766766766766767,
      "loss": 0.0009,
      "step": 662
    },
    {
      "epoch": 61.67,
      "grad_norm": 0.006423485931009054,
      "learning_rate": 0.0006746746746746747,
      "loss": 0.001,
      "step": 663
    },
    {
      "epoch": 61.77,
      "grad_norm": 0.005487724207341671,
      "learning_rate": 0.0006726726726726727,
      "loss": 0.0009,
      "step": 664
    },
    {
      "epoch": 61.86,
      "grad_norm": 0.00510474992915988,
      "learning_rate": 0.0006706706706706706,
      "loss": 0.0009,
      "step": 665
    },
    {
      "epoch": 61.95,
      "grad_norm": 0.004575695376843214,
      "learning_rate": 0.0006686686686686687,
      "loss": 0.001,
      "step": 666
    },
    {
      "epoch": 62.05,
      "grad_norm": 0.006318713538348675,
      "learning_rate": 0.0006666666666666666,
      "loss": 0.001,
      "step": 667
    },
    {
      "epoch": 62.14,
      "grad_norm": 0.004629228729754686,
      "learning_rate": 0.0006646646646646647,
      "loss": 0.0009,
      "step": 668
    },
    {
      "epoch": 62.23,
      "grad_norm": 0.00417210441082716,
      "learning_rate": 0.0006626626626626628,
      "loss": 0.0009,
      "step": 669
    },
    {
      "epoch": 62.33,
      "grad_norm": 0.00515393353998661,
      "learning_rate": 0.0006606606606606607,
      "loss": 0.0009,
      "step": 670
    },
    {
      "epoch": 62.42,
      "grad_norm": 0.005272227339446545,
      "learning_rate": 0.0006586586586586587,
      "loss": 0.0009,
      "step": 671
    },
    {
      "epoch": 62.51,
      "grad_norm": 0.004666326567530632,
      "learning_rate": 0.0006566566566566566,
      "loss": 0.0009,
      "step": 672
    },
    {
      "epoch": 62.6,
      "grad_norm": 0.004507464822381735,
      "learning_rate": 0.0006546546546546547,
      "loss": 0.0009,
      "step": 673
    },
    {
      "epoch": 62.7,
      "grad_norm": 0.004493135958909988,
      "learning_rate": 0.0006526526526526526,
      "loss": 0.0009,
      "step": 674
    },
    {
      "epoch": 62.79,
      "grad_norm": 0.004387193825095892,
      "learning_rate": 0.0006506506506506507,
      "loss": 0.001,
      "step": 675
    },
    {
      "epoch": 62.88,
      "grad_norm": 0.00477193295955658,
      "learning_rate": 0.0006486486486486487,
      "loss": 0.001,
      "step": 676
    },
    {
      "epoch": 62.98,
      "grad_norm": 0.00627317326143384,
      "learning_rate": 0.0006466466466466467,
      "loss": 0.001,
      "step": 677
    },
    {
      "epoch": 63.07,
      "grad_norm": 0.005958099849522114,
      "learning_rate": 0.0006446446446446446,
      "loss": 0.0009,
      "step": 678
    },
    {
      "epoch": 63.16,
      "grad_norm": 0.005122070200741291,
      "learning_rate": 0.0006426426426426426,
      "loss": 0.0009,
      "step": 679
    },
    {
      "epoch": 63.26,
      "grad_norm": 0.004745810758322477,
      "learning_rate": 0.0006406406406406406,
      "loss": 0.0009,
      "step": 680
    },
    {
      "epoch": 63.35,
      "grad_norm": 0.005352159030735493,
      "learning_rate": 0.0006386386386386387,
      "loss": 0.0009,
      "step": 681
    },
    {
      "epoch": 63.44,
      "grad_norm": 0.005828471854329109,
      "learning_rate": 0.0006366366366366366,
      "loss": 0.0009,
      "step": 682
    },
    {
      "epoch": 63.53,
      "grad_norm": 0.005063640419393778,
      "learning_rate": 0.0006346346346346347,
      "loss": 0.0009,
      "step": 683
    },
    {
      "epoch": 63.63,
      "grad_norm": 0.005305576603859663,
      "learning_rate": 0.0006326326326326326,
      "loss": 0.0009,
      "step": 684
    },
    {
      "epoch": 63.72,
      "grad_norm": 0.004934265278279781,
      "learning_rate": 0.0006306306306306307,
      "loss": 0.0009,
      "step": 685
    },
    {
      "epoch": 63.81,
      "grad_norm": 0.0063949464820325375,
      "learning_rate": 0.0006286286286286286,
      "loss": 0.0009,
      "step": 686
    },
    {
      "epoch": 63.91,
      "grad_norm": 0.005123211536556482,
      "learning_rate": 0.0006266266266266266,
      "loss": 0.0009,
      "step": 687
    },
    {
      "epoch": 64.0,
      "grad_norm": 0.005168468225747347,
      "learning_rate": 0.0006246246246246246,
      "loss": 0.001,
      "step": 688
    },
    {
      "epoch": 64.09,
      "grad_norm": 0.004274832550436258,
      "learning_rate": 0.0006226226226226226,
      "loss": 0.0009,
      "step": 689
    },
    {
      "epoch": 64.19,
      "grad_norm": 0.005837561096996069,
      "learning_rate": 0.0006206206206206207,
      "loss": 0.0009,
      "step": 690
    },
    {
      "epoch": 64.28,
      "grad_norm": 0.0039881994016468525,
      "learning_rate": 0.0006186186186186187,
      "loss": 0.0009,
      "step": 691
    },
    {
      "epoch": 64.37,
      "grad_norm": 0.004921737592667341,
      "learning_rate": 0.0006166166166166167,
      "loss": 0.0009,
      "step": 692
    },
    {
      "epoch": 64.47,
      "grad_norm": 0.006080364342778921,
      "learning_rate": 0.0006146146146146146,
      "loss": 0.0009,
      "step": 693
    },
    {
      "epoch": 64.56,
      "grad_norm": 0.004030365962535143,
      "learning_rate": 0.0006126126126126126,
      "loss": 0.0009,
      "step": 694
    },
    {
      "epoch": 64.65,
      "grad_norm": 0.004934939555823803,
      "learning_rate": 0.0006106106106106106,
      "loss": 0.0009,
      "step": 695
    },
    {
      "epoch": 64.74,
      "grad_norm": 0.004940544255077839,
      "learning_rate": 0.0006086086086086087,
      "loss": 0.001,
      "step": 696
    },
    {
      "epoch": 64.84,
      "grad_norm": 0.004450839478522539,
      "learning_rate": 0.0006066066066066066,
      "loss": 0.0009,
      "step": 697
    },
    {
      "epoch": 64.93,
      "grad_norm": 0.004888167139142752,
      "learning_rate": 0.0006046046046046047,
      "loss": 0.001,
      "step": 698
    },
    {
      "epoch": 65.02,
      "grad_norm": 0.005614493042230606,
      "learning_rate": 0.0006026026026026026,
      "loss": 0.001,
      "step": 699
    },
    {
      "epoch": 65.12,
      "grad_norm": 0.004568004980683327,
      "learning_rate": 0.0006006006006006006,
      "loss": 0.0009,
      "step": 700
    },
    {
      "epoch": 65.21,
      "grad_norm": 0.00491096219047904,
      "learning_rate": 0.0005985985985985986,
      "loss": 0.0009,
      "step": 701
    },
    {
      "epoch": 65.3,
      "grad_norm": 0.004417866468429565,
      "learning_rate": 0.0005965965965965966,
      "loss": 0.0009,
      "step": 702
    },
    {
      "epoch": 65.4,
      "grad_norm": 0.005960181355476379,
      "learning_rate": 0.0005945945945945946,
      "loss": 0.0009,
      "step": 703
    },
    {
      "epoch": 65.49,
      "grad_norm": 0.006144127808511257,
      "learning_rate": 0.0005925925925925926,
      "loss": 0.0009,
      "step": 704
    },
    {
      "epoch": 65.58,
      "grad_norm": 0.00465022400021553,
      "learning_rate": 0.0005905905905905906,
      "loss": 0.0009,
      "step": 705
    },
    {
      "epoch": 65.67,
      "grad_norm": 0.005546695552766323,
      "learning_rate": 0.0005885885885885886,
      "loss": 0.0009,
      "step": 706
    },
    {
      "epoch": 65.77,
      "grad_norm": 0.005240352358669043,
      "learning_rate": 0.0005865865865865865,
      "loss": 0.0009,
      "step": 707
    },
    {
      "epoch": 65.86,
      "grad_norm": 0.005172856617718935,
      "learning_rate": 0.0005845845845845846,
      "loss": 0.0009,
      "step": 708
    },
    {
      "epoch": 65.95,
      "grad_norm": 0.006124091800302267,
      "learning_rate": 0.0005825825825825825,
      "loss": 0.001,
      "step": 709
    },
    {
      "epoch": 66.05,
      "grad_norm": 0.005465738940984011,
      "learning_rate": 0.0005805805805805806,
      "loss": 0.0009,
      "step": 710
    },
    {
      "epoch": 66.14,
      "grad_norm": 0.005223135929554701,
      "learning_rate": 0.0005785785785785786,
      "loss": 0.0009,
      "step": 711
    },
    {
      "epoch": 66.23,
      "grad_norm": 0.005092476028949022,
      "learning_rate": 0.0005765765765765766,
      "loss": 0.0009,
      "step": 712
    },
    {
      "epoch": 66.33,
      "grad_norm": 0.005102634429931641,
      "learning_rate": 0.0005745745745745747,
      "loss": 0.0009,
      "step": 713
    },
    {
      "epoch": 66.42,
      "grad_norm": 0.005702032241970301,
      "learning_rate": 0.0005725725725725726,
      "loss": 0.0009,
      "step": 714
    },
    {
      "epoch": 66.51,
      "grad_norm": 0.005094748921692371,
      "learning_rate": 0.0005705705705705706,
      "loss": 0.0009,
      "step": 715
    },
    {
      "epoch": 66.6,
      "grad_norm": 0.003098653396591544,
      "learning_rate": 0.0005685685685685685,
      "loss": 0.0009,
      "step": 716
    },
    {
      "epoch": 66.7,
      "grad_norm": 0.005292132962495089,
      "learning_rate": 0.0005665665665665666,
      "loss": 0.0009,
      "step": 717
    },
    {
      "epoch": 66.79,
      "grad_norm": 0.004768182523548603,
      "learning_rate": 0.0005645645645645646,
      "loss": 0.0009,
      "step": 718
    },
    {
      "epoch": 66.88,
      "grad_norm": 0.005750990007072687,
      "learning_rate": 0.0005625625625625626,
      "loss": 0.001,
      "step": 719
    },
    {
      "epoch": 66.98,
      "grad_norm": 0.006446843966841698,
      "learning_rate": 0.0005605605605605606,
      "loss": 0.001,
      "step": 720
    },
    {
      "epoch": 67.07,
      "grad_norm": 0.00464686518535018,
      "learning_rate": 0.0005585585585585586,
      "loss": 0.0009,
      "step": 721
    },
    {
      "epoch": 67.16,
      "grad_norm": 0.004099560435861349,
      "learning_rate": 0.0005565565565565565,
      "loss": 0.0009,
      "step": 722
    },
    {
      "epoch": 67.26,
      "grad_norm": 0.004498236812651157,
      "learning_rate": 0.0005545545545545546,
      "loss": 0.0009,
      "step": 723
    },
    {
      "epoch": 67.35,
      "grad_norm": 0.005328867118805647,
      "learning_rate": 0.0005525525525525525,
      "loss": 0.0009,
      "step": 724
    },
    {
      "epoch": 67.44,
      "grad_norm": 0.006489933934062719,
      "learning_rate": 0.0005505505505505506,
      "loss": 0.0009,
      "step": 725
    },
    {
      "epoch": 67.53,
      "grad_norm": 0.006325933616608381,
      "learning_rate": 0.0005485485485485485,
      "loss": 0.0009,
      "step": 726
    },
    {
      "epoch": 67.63,
      "grad_norm": 0.005391648970544338,
      "learning_rate": 0.0005465465465465466,
      "loss": 0.0009,
      "step": 727
    },
    {
      "epoch": 67.72,
      "grad_norm": 0.004351357463747263,
      "learning_rate": 0.0005445445445445447,
      "loss": 0.0009,
      "step": 728
    },
    {
      "epoch": 67.81,
      "grad_norm": 0.004834192339330912,
      "learning_rate": 0.0005425425425425425,
      "loss": 0.0009,
      "step": 729
    },
    {
      "epoch": 67.91,
      "grad_norm": 0.0048619648441672325,
      "learning_rate": 0.0005405405405405405,
      "loss": 0.0009,
      "step": 730
    },
    {
      "epoch": 68.0,
      "grad_norm": 0.004727635532617569,
      "learning_rate": 0.0005385385385385385,
      "loss": 0.0009,
      "step": 731
    },
    {
      "epoch": 68.09,
      "grad_norm": 0.0052162036299705505,
      "learning_rate": 0.0005365365365365366,
      "loss": 0.0009,
      "step": 732
    },
    {
      "epoch": 68.19,
      "grad_norm": 0.00380492745898664,
      "learning_rate": 0.0005345345345345346,
      "loss": 0.0009,
      "step": 733
    },
    {
      "epoch": 68.28,
      "grad_norm": 0.004356956109404564,
      "learning_rate": 0.0005325325325325326,
      "loss": 0.0009,
      "step": 734
    },
    {
      "epoch": 68.37,
      "grad_norm": 0.0050609237514436245,
      "learning_rate": 0.0005305305305305306,
      "loss": 0.0009,
      "step": 735
    },
    {
      "epoch": 68.47,
      "grad_norm": 0.004486294463276863,
      "learning_rate": 0.0005285285285285285,
      "loss": 0.0009,
      "step": 736
    },
    {
      "epoch": 68.56,
      "grad_norm": 0.004462026990950108,
      "learning_rate": 0.0005265265265265265,
      "loss": 0.0009,
      "step": 737
    },
    {
      "epoch": 68.65,
      "grad_norm": 0.005147531162947416,
      "learning_rate": 0.0005245245245245245,
      "loss": 0.0009,
      "step": 738
    },
    {
      "epoch": 68.74,
      "grad_norm": 0.0050637489184737206,
      "learning_rate": 0.0005225225225225225,
      "loss": 0.0009,
      "step": 739
    },
    {
      "epoch": 68.84,
      "grad_norm": 0.0046780104748904705,
      "learning_rate": 0.0005205205205205206,
      "loss": 0.0009,
      "step": 740
    },
    {
      "epoch": 68.93,
      "grad_norm": 0.00649573840200901,
      "learning_rate": 0.0005185185185185185,
      "loss": 0.0009,
      "step": 741
    },
    {
      "epoch": 69.02,
      "grad_norm": 0.004121126141399145,
      "learning_rate": 0.0005165165165165166,
      "loss": 0.0009,
      "step": 742
    },
    {
      "epoch": 69.12,
      "grad_norm": 0.005139260087162256,
      "learning_rate": 0.0005145145145145144,
      "loss": 0.0009,
      "step": 743
    },
    {
      "epoch": 69.21,
      "grad_norm": 0.004405038896948099,
      "learning_rate": 0.0005125125125125125,
      "loss": 0.0009,
      "step": 744
    },
    {
      "epoch": 69.3,
      "grad_norm": 0.004359386395663023,
      "learning_rate": 0.0005105105105105105,
      "loss": 0.0009,
      "step": 745
    },
    {
      "epoch": 69.4,
      "grad_norm": 0.006022099405527115,
      "learning_rate": 0.0005085085085085085,
      "loss": 0.0009,
      "step": 746
    },
    {
      "epoch": 69.49,
      "grad_norm": 0.0061242845840752125,
      "learning_rate": 0.0005065065065065065,
      "loss": 0.0009,
      "step": 747
    },
    {
      "epoch": 69.58,
      "grad_norm": 0.0056119151413440704,
      "learning_rate": 0.0005045045045045045,
      "loss": 0.0009,
      "step": 748
    },
    {
      "epoch": 69.67,
      "grad_norm": 0.0056916349567472935,
      "learning_rate": 0.0005025025025025025,
      "loss": 0.001,
      "step": 749
    },
    {
      "epoch": 69.77,
      "grad_norm": 0.005073244217783213,
      "learning_rate": 0.0005005005005005006,
      "loss": 0.0009,
      "step": 750
    },
    {
      "epoch": 69.86,
      "grad_norm": 0.005603079218417406,
      "learning_rate": 0.0004984984984984984,
      "loss": 0.0009,
      "step": 751
    },
    {
      "epoch": 69.95,
      "grad_norm": 0.004498012363910675,
      "learning_rate": 0.0004964964964964965,
      "loss": 0.001,
      "step": 752
    },
    {
      "epoch": 70.05,
      "grad_norm": 0.003957652952522039,
      "learning_rate": 0.0004944944944944946,
      "loss": 0.0009,
      "step": 753
    },
    {
      "epoch": 70.14,
      "grad_norm": 0.005899989977478981,
      "learning_rate": 0.0004924924924924925,
      "loss": 0.0009,
      "step": 754
    },
    {
      "epoch": 70.23,
      "grad_norm": 0.00578395975753665,
      "learning_rate": 0.0004904904904904905,
      "loss": 0.0009,
      "step": 755
    },
    {
      "epoch": 70.33,
      "grad_norm": 0.00512510072439909,
      "learning_rate": 0.0004884884884884885,
      "loss": 0.0009,
      "step": 756
    },
    {
      "epoch": 70.42,
      "grad_norm": 0.00464695505797863,
      "learning_rate": 0.0004864864864864865,
      "loss": 0.0009,
      "step": 757
    },
    {
      "epoch": 70.51,
      "grad_norm": 0.0049385204911231995,
      "learning_rate": 0.0004844844844844845,
      "loss": 0.0009,
      "step": 758
    },
    {
      "epoch": 70.6,
      "grad_norm": 0.006543036084622145,
      "learning_rate": 0.0004824824824824825,
      "loss": 0.0009,
      "step": 759
    },
    {
      "epoch": 70.7,
      "grad_norm": 0.004690464586019516,
      "learning_rate": 0.00048048048048048047,
      "loss": 0.0009,
      "step": 760
    },
    {
      "epoch": 70.79,
      "grad_norm": 0.004293113946914673,
      "learning_rate": 0.00047847847847847847,
      "loss": 0.0009,
      "step": 761
    },
    {
      "epoch": 70.88,
      "grad_norm": 0.005701511632651091,
      "learning_rate": 0.0004764764764764765,
      "loss": 0.001,
      "step": 762
    },
    {
      "epoch": 70.98,
      "grad_norm": 0.005801317747682333,
      "learning_rate": 0.0004744744744744745,
      "loss": 0.0009,
      "step": 763
    },
    {
      "epoch": 71.07,
      "grad_norm": 0.0050879609771072865,
      "learning_rate": 0.0004724724724724725,
      "loss": 0.0009,
      "step": 764
    },
    {
      "epoch": 71.16,
      "grad_norm": 0.003959728870540857,
      "learning_rate": 0.0004704704704704705,
      "loss": 0.0009,
      "step": 765
    },
    {
      "epoch": 71.26,
      "grad_norm": 0.003927950747311115,
      "learning_rate": 0.0004684684684684685,
      "loss": 0.0009,
      "step": 766
    },
    {
      "epoch": 71.35,
      "grad_norm": 0.004778233356773853,
      "learning_rate": 0.00046646646646646644,
      "loss": 0.0009,
      "step": 767
    },
    {
      "epoch": 71.44,
      "grad_norm": 0.005506793037056923,
      "learning_rate": 0.0004644644644644645,
      "loss": 0.0009,
      "step": 768
    },
    {
      "epoch": 71.53,
      "grad_norm": 0.005333637818694115,
      "learning_rate": 0.0004624624624624625,
      "loss": 0.0009,
      "step": 769
    },
    {
      "epoch": 71.63,
      "grad_norm": 0.004676744807511568,
      "learning_rate": 0.0004604604604604605,
      "loss": 0.0009,
      "step": 770
    },
    {
      "epoch": 71.72,
      "grad_norm": 0.004362679086625576,
      "learning_rate": 0.00045845845845845845,
      "loss": 0.0009,
      "step": 771
    },
    {
      "epoch": 71.81,
      "grad_norm": 0.004646714776754379,
      "learning_rate": 0.00045645645645645645,
      "loss": 0.0009,
      "step": 772
    },
    {
      "epoch": 71.91,
      "grad_norm": 0.005374584347009659,
      "learning_rate": 0.0004544544544544545,
      "loss": 0.0009,
      "step": 773
    },
    {
      "epoch": 72.0,
      "grad_norm": 0.005148720927536488,
      "learning_rate": 0.00045245245245245245,
      "loss": 0.001,
      "step": 774
    },
    {
      "epoch": 72.09,
      "grad_norm": 0.004247575532644987,
      "learning_rate": 0.00045045045045045046,
      "loss": 0.0009,
      "step": 775
    },
    {
      "epoch": 72.19,
      "grad_norm": 0.0048266686499118805,
      "learning_rate": 0.00044844844844844846,
      "loss": 0.0009,
      "step": 776
    },
    {
      "epoch": 72.28,
      "grad_norm": 0.005021723918616772,
      "learning_rate": 0.00044644644644644646,
      "loss": 0.0009,
      "step": 777
    },
    {
      "epoch": 72.37,
      "grad_norm": 0.004453569650650024,
      "learning_rate": 0.0004444444444444444,
      "loss": 0.0009,
      "step": 778
    },
    {
      "epoch": 72.47,
      "grad_norm": 0.005982222966849804,
      "learning_rate": 0.00044244244244244247,
      "loss": 0.0009,
      "step": 779
    },
    {
      "epoch": 72.56,
      "grad_norm": 0.005484255496412516,
      "learning_rate": 0.00044044044044044047,
      "loss": 0.0009,
      "step": 780
    },
    {
      "epoch": 72.65,
      "grad_norm": 0.004797578323632479,
      "learning_rate": 0.0004384384384384385,
      "loss": 0.0009,
      "step": 781
    },
    {
      "epoch": 72.74,
      "grad_norm": 0.006132503971457481,
      "learning_rate": 0.0004364364364364364,
      "loss": 0.0009,
      "step": 782
    },
    {
      "epoch": 72.84,
      "grad_norm": 0.004848809912800789,
      "learning_rate": 0.0004344344344344344,
      "loss": 0.0009,
      "step": 783
    },
    {
      "epoch": 72.93,
      "grad_norm": 0.004503164906054735,
      "learning_rate": 0.0004324324324324325,
      "loss": 0.0009,
      "step": 784
    },
    {
      "epoch": 73.02,
      "grad_norm": 0.0051297759637236595,
      "learning_rate": 0.00043043043043043043,
      "loss": 0.0009,
      "step": 785
    },
    {
      "epoch": 73.12,
      "grad_norm": 0.0043725077994167805,
      "learning_rate": 0.00042842842842842843,
      "loss": 0.0009,
      "step": 786
    },
    {
      "epoch": 73.21,
      "grad_norm": 0.005198729690164328,
      "learning_rate": 0.00042642642642642644,
      "loss": 0.0009,
      "step": 787
    },
    {
      "epoch": 73.3,
      "grad_norm": 0.006160935387015343,
      "learning_rate": 0.00042442442442442444,
      "loss": 0.0009,
      "step": 788
    },
    {
      "epoch": 73.4,
      "grad_norm": 0.004737855866551399,
      "learning_rate": 0.00042242242242242244,
      "loss": 0.0009,
      "step": 789
    },
    {
      "epoch": 73.49,
      "grad_norm": 0.0053480821661651134,
      "learning_rate": 0.00042042042042042044,
      "loss": 0.0009,
      "step": 790
    },
    {
      "epoch": 73.58,
      "grad_norm": 0.004748126026242971,
      "learning_rate": 0.00041841841841841845,
      "loss": 0.0009,
      "step": 791
    },
    {
      "epoch": 73.67,
      "grad_norm": 0.004372806288301945,
      "learning_rate": 0.0004164164164164164,
      "loss": 0.0009,
      "step": 792
    },
    {
      "epoch": 73.77,
      "grad_norm": 0.004583439324051142,
      "learning_rate": 0.0004144144144144144,
      "loss": 0.0009,
      "step": 793
    },
    {
      "epoch": 73.86,
      "grad_norm": 0.004807659890502691,
      "learning_rate": 0.0004124124124124124,
      "loss": 0.0009,
      "step": 794
    },
    {
      "epoch": 73.95,
      "grad_norm": 0.005351128056645393,
      "learning_rate": 0.00041041041041041046,
      "loss": 0.0009,
      "step": 795
    },
    {
      "epoch": 74.05,
      "grad_norm": 0.007114302832633257,
      "learning_rate": 0.0004084084084084084,
      "loss": 0.0009,
      "step": 796
    },
    {
      "epoch": 74.14,
      "grad_norm": 0.004424531944096088,
      "learning_rate": 0.0004064064064064064,
      "loss": 0.0009,
      "step": 797
    },
    {
      "epoch": 74.23,
      "grad_norm": 0.005046687554568052,
      "learning_rate": 0.0004044044044044044,
      "loss": 0.0009,
      "step": 798
    },
    {
      "epoch": 74.33,
      "grad_norm": 0.004553201142698526,
      "learning_rate": 0.0004024024024024024,
      "loss": 0.0009,
      "step": 799
    },
    {
      "epoch": 74.42,
      "grad_norm": 0.005300381686538458,
      "learning_rate": 0.0004004004004004004,
      "loss": 0.0009,
      "step": 800
    },
    {
      "epoch": 74.51,
      "grad_norm": 0.004393191542476416,
      "learning_rate": 0.0003983983983983984,
      "loss": 0.0009,
      "step": 801
    },
    {
      "epoch": 74.6,
      "grad_norm": 0.0052809640765190125,
      "learning_rate": 0.0003963963963963964,
      "loss": 0.0009,
      "step": 802
    },
    {
      "epoch": 74.7,
      "grad_norm": 0.0055501414462924,
      "learning_rate": 0.00039439439439439437,
      "loss": 0.0009,
      "step": 803
    },
    {
      "epoch": 74.79,
      "grad_norm": 0.005796784069389105,
      "learning_rate": 0.0003923923923923924,
      "loss": 0.001,
      "step": 804
    },
    {
      "epoch": 74.88,
      "grad_norm": 0.005209118127822876,
      "learning_rate": 0.0003903903903903904,
      "loss": 0.0009,
      "step": 805
    },
    {
      "epoch": 74.98,
      "grad_norm": 0.005717949476093054,
      "learning_rate": 0.00038838838838838844,
      "loss": 0.0009,
      "step": 806
    },
    {
      "epoch": 75.07,
      "grad_norm": 0.005229995120316744,
      "learning_rate": 0.0003863863863863864,
      "loss": 0.0009,
      "step": 807
    },
    {
      "epoch": 75.16,
      "grad_norm": 0.004944903776049614,
      "learning_rate": 0.0003843843843843844,
      "loss": 0.0009,
      "step": 808
    },
    {
      "epoch": 75.26,
      "grad_norm": 0.006028860807418823,
      "learning_rate": 0.0003823823823823824,
      "loss": 0.0009,
      "step": 809
    },
    {
      "epoch": 75.35,
      "grad_norm": 0.005048729944974184,
      "learning_rate": 0.00038038038038038034,
      "loss": 0.0009,
      "step": 810
    },
    {
      "epoch": 75.44,
      "grad_norm": 0.004992008674889803,
      "learning_rate": 0.0003783783783783784,
      "loss": 0.0009,
      "step": 811
    },
    {
      "epoch": 75.53,
      "grad_norm": 0.005278703756630421,
      "learning_rate": 0.0003763763763763764,
      "loss": 0.0009,
      "step": 812
    },
    {
      "epoch": 75.63,
      "grad_norm": 0.004506049677729607,
      "learning_rate": 0.0003743743743743744,
      "loss": 0.0009,
      "step": 813
    },
    {
      "epoch": 75.72,
      "grad_norm": 0.004034656099975109,
      "learning_rate": 0.00037237237237237235,
      "loss": 0.0009,
      "step": 814
    },
    {
      "epoch": 75.81,
      "grad_norm": 0.005864436272531748,
      "learning_rate": 0.00037037037037037035,
      "loss": 0.0009,
      "step": 815
    },
    {
      "epoch": 75.91,
      "grad_norm": 0.005160591099411249,
      "learning_rate": 0.0003683683683683684,
      "loss": 0.0009,
      "step": 816
    },
    {
      "epoch": 76.0,
      "grad_norm": 0.004996043164283037,
      "learning_rate": 0.0003663663663663664,
      "loss": 0.0009,
      "step": 817
    },
    {
      "epoch": 76.09,
      "grad_norm": 0.004918511491268873,
      "learning_rate": 0.00036436436436436436,
      "loss": 0.0009,
      "step": 818
    },
    {
      "epoch": 76.19,
      "grad_norm": 0.005201864056289196,
      "learning_rate": 0.00036236236236236236,
      "loss": 0.0009,
      "step": 819
    },
    {
      "epoch": 76.28,
      "grad_norm": 0.004492416977882385,
      "learning_rate": 0.00036036036036036037,
      "loss": 0.0009,
      "step": 820
    },
    {
      "epoch": 76.37,
      "grad_norm": 0.004455370828509331,
      "learning_rate": 0.0003583583583583583,
      "loss": 0.0009,
      "step": 821
    },
    {
      "epoch": 76.47,
      "grad_norm": 0.004748644307255745,
      "learning_rate": 0.00035635635635635637,
      "loss": 0.0009,
      "step": 822
    },
    {
      "epoch": 76.56,
      "grad_norm": 0.005465961527079344,
      "learning_rate": 0.0003543543543543544,
      "loss": 0.0009,
      "step": 823
    },
    {
      "epoch": 76.65,
      "grad_norm": 0.004653491545468569,
      "learning_rate": 0.0003523523523523524,
      "loss": 0.0009,
      "step": 824
    },
    {
      "epoch": 76.74,
      "grad_norm": 0.0050563388504087925,
      "learning_rate": 0.0003503503503503503,
      "loss": 0.0009,
      "step": 825
    },
    {
      "epoch": 76.84,
      "grad_norm": 0.0048650517128407955,
      "learning_rate": 0.00034834834834834833,
      "loss": 0.0009,
      "step": 826
    },
    {
      "epoch": 76.93,
      "grad_norm": 0.006074638105928898,
      "learning_rate": 0.0003463463463463464,
      "loss": 0.0009,
      "step": 827
    },
    {
      "epoch": 77.02,
      "grad_norm": 0.006220606621354818,
      "learning_rate": 0.0003443443443443444,
      "loss": 0.0009,
      "step": 828
    },
    {
      "epoch": 77.12,
      "grad_norm": 0.003972330596297979,
      "learning_rate": 0.00034234234234234234,
      "loss": 0.0009,
      "step": 829
    },
    {
      "epoch": 77.21,
      "grad_norm": 0.004148792941123247,
      "learning_rate": 0.00034034034034034034,
      "loss": 0.0009,
      "step": 830
    },
    {
      "epoch": 77.3,
      "grad_norm": 0.004665898159146309,
      "learning_rate": 0.00033833833833833834,
      "loss": 0.0009,
      "step": 831
    },
    {
      "epoch": 77.4,
      "grad_norm": 0.00474063353613019,
      "learning_rate": 0.00033633633633633635,
      "loss": 0.0009,
      "step": 832
    },
    {
      "epoch": 77.49,
      "grad_norm": 0.004362752195447683,
      "learning_rate": 0.00033433433433433435,
      "loss": 0.0009,
      "step": 833
    },
    {
      "epoch": 77.58,
      "grad_norm": 0.005252499599009752,
      "learning_rate": 0.00033233233233233235,
      "loss": 0.0009,
      "step": 834
    },
    {
      "epoch": 77.67,
      "grad_norm": 0.005096133332699537,
      "learning_rate": 0.00033033033033033035,
      "loss": 0.0009,
      "step": 835
    },
    {
      "epoch": 77.77,
      "grad_norm": 0.005595281720161438,
      "learning_rate": 0.0003283283283283283,
      "loss": 0.0009,
      "step": 836
    },
    {
      "epoch": 77.86,
      "grad_norm": 0.005261037964373827,
      "learning_rate": 0.0003263263263263263,
      "loss": 0.0009,
      "step": 837
    },
    {
      "epoch": 77.95,
      "grad_norm": 0.004542606417089701,
      "learning_rate": 0.00032432432432432436,
      "loss": 0.0009,
      "step": 838
    },
    {
      "epoch": 78.05,
      "grad_norm": 0.006030418444424868,
      "learning_rate": 0.0003223223223223223,
      "loss": 0.0009,
      "step": 839
    },
    {
      "epoch": 78.14,
      "grad_norm": 0.006518964190036058,
      "learning_rate": 0.0003203203203203203,
      "loss": 0.0009,
      "step": 840
    },
    {
      "epoch": 78.23,
      "grad_norm": 0.004217600915580988,
      "learning_rate": 0.0003183183183183183,
      "loss": 0.0009,
      "step": 841
    },
    {
      "epoch": 78.33,
      "grad_norm": 0.006001576315611601,
      "learning_rate": 0.0003163163163163163,
      "loss": 0.0009,
      "step": 842
    },
    {
      "epoch": 78.42,
      "grad_norm": 0.0051309033297002316,
      "learning_rate": 0.0003143143143143143,
      "loss": 0.0009,
      "step": 843
    },
    {
      "epoch": 78.51,
      "grad_norm": 0.005750332027673721,
      "learning_rate": 0.0003123123123123123,
      "loss": 0.0009,
      "step": 844
    },
    {
      "epoch": 78.6,
      "grad_norm": 0.006327745039016008,
      "learning_rate": 0.00031031031031031033,
      "loss": 0.0009,
      "step": 845
    },
    {
      "epoch": 78.7,
      "grad_norm": 0.004373130854219198,
      "learning_rate": 0.00030830830830830833,
      "loss": 0.0009,
      "step": 846
    },
    {
      "epoch": 78.79,
      "grad_norm": 0.006739124655723572,
      "learning_rate": 0.0003063063063063063,
      "loss": 0.0009,
      "step": 847
    },
    {
      "epoch": 78.88,
      "grad_norm": 0.004565860144793987,
      "learning_rate": 0.00030430430430430434,
      "loss": 0.0009,
      "step": 848
    },
    {
      "epoch": 78.98,
      "grad_norm": 0.0058022006414830685,
      "learning_rate": 0.00030230230230230234,
      "loss": 0.001,
      "step": 849
    },
    {
      "epoch": 79.07,
      "grad_norm": 0.005292449612170458,
      "learning_rate": 0.0003003003003003003,
      "loss": 0.0009,
      "step": 850
    },
    {
      "epoch": 79.16,
      "grad_norm": 0.004624104592949152,
      "learning_rate": 0.0002982982982982983,
      "loss": 0.0009,
      "step": 851
    },
    {
      "epoch": 79.26,
      "grad_norm": 0.0044622598215937614,
      "learning_rate": 0.0002962962962962963,
      "loss": 0.0009,
      "step": 852
    },
    {
      "epoch": 79.35,
      "grad_norm": 0.00514860637485981,
      "learning_rate": 0.0002942942942942943,
      "loss": 0.0009,
      "step": 853
    },
    {
      "epoch": 79.44,
      "grad_norm": 0.005344431847333908,
      "learning_rate": 0.0002922922922922923,
      "loss": 0.0009,
      "step": 854
    },
    {
      "epoch": 79.53,
      "grad_norm": 0.00455870758742094,
      "learning_rate": 0.0002902902902902903,
      "loss": 0.0009,
      "step": 855
    },
    {
      "epoch": 79.63,
      "grad_norm": 0.005169988144189119,
      "learning_rate": 0.0002882882882882883,
      "loss": 0.0009,
      "step": 856
    },
    {
      "epoch": 79.72,
      "grad_norm": 0.005183654837310314,
      "learning_rate": 0.0002862862862862863,
      "loss": 0.0009,
      "step": 857
    },
    {
      "epoch": 79.81,
      "grad_norm": 0.005088067147880793,
      "learning_rate": 0.00028428428428428425,
      "loss": 0.0009,
      "step": 858
    },
    {
      "epoch": 79.91,
      "grad_norm": 0.005213799886405468,
      "learning_rate": 0.0002822822822822823,
      "loss": 0.0009,
      "step": 859
    },
    {
      "epoch": 80.0,
      "grad_norm": 0.004457606468349695,
      "learning_rate": 0.0002802802802802803,
      "loss": 0.0009,
      "step": 860
    },
    {
      "epoch": 80.09,
      "grad_norm": 0.003913497552275658,
      "learning_rate": 0.00027827827827827826,
      "loss": 0.0009,
      "step": 861
    },
    {
      "epoch": 80.19,
      "grad_norm": 0.005697009153664112,
      "learning_rate": 0.00027627627627627627,
      "loss": 0.0009,
      "step": 862
    },
    {
      "epoch": 80.28,
      "grad_norm": 0.004843274597078562,
      "learning_rate": 0.00027427427427427427,
      "loss": 0.0009,
      "step": 863
    },
    {
      "epoch": 80.37,
      "grad_norm": 0.005225572735071182,
      "learning_rate": 0.0002722722722722723,
      "loss": 0.0009,
      "step": 864
    },
    {
      "epoch": 80.47,
      "grad_norm": 0.004732534755021334,
      "learning_rate": 0.0002702702702702703,
      "loss": 0.0009,
      "step": 865
    },
    {
      "epoch": 80.56,
      "grad_norm": 0.004842433612793684,
      "learning_rate": 0.0002682682682682683,
      "loss": 0.0009,
      "step": 866
    },
    {
      "epoch": 80.65,
      "grad_norm": 0.004786440636962652,
      "learning_rate": 0.0002662662662662663,
      "loss": 0.0009,
      "step": 867
    },
    {
      "epoch": 80.74,
      "grad_norm": 0.005430594086647034,
      "learning_rate": 0.00026426426426426423,
      "loss": 0.0009,
      "step": 868
    },
    {
      "epoch": 80.84,
      "grad_norm": 0.0050706504844129086,
      "learning_rate": 0.00026226226226226223,
      "loss": 0.0009,
      "step": 869
    },
    {
      "epoch": 80.93,
      "grad_norm": 0.005583117250353098,
      "learning_rate": 0.0002602602602602603,
      "loss": 0.0009,
      "step": 870
    },
    {
      "epoch": 81.02,
      "grad_norm": 0.005822559352964163,
      "learning_rate": 0.0002582582582582583,
      "loss": 0.0009,
      "step": 871
    },
    {
      "epoch": 81.12,
      "grad_norm": 0.006199766416102648,
      "learning_rate": 0.00025625625625625624,
      "loss": 0.0009,
      "step": 872
    },
    {
      "epoch": 81.21,
      "grad_norm": 0.005196480546146631,
      "learning_rate": 0.00025425425425425424,
      "loss": 0.0009,
      "step": 873
    },
    {
      "epoch": 81.3,
      "grad_norm": 0.006123671308159828,
      "learning_rate": 0.00025225225225225225,
      "loss": 0.0009,
      "step": 874
    },
    {
      "epoch": 81.4,
      "grad_norm": 0.004678761120885611,
      "learning_rate": 0.0002502502502502503,
      "loss": 0.0009,
      "step": 875
    },
    {
      "epoch": 81.49,
      "grad_norm": 0.005526341963559389,
      "learning_rate": 0.00024824824824824825,
      "loss": 0.0009,
      "step": 876
    },
    {
      "epoch": 81.58,
      "grad_norm": 0.004942110739648342,
      "learning_rate": 0.00024624624624624625,
      "loss": 0.0009,
      "step": 877
    },
    {
      "epoch": 81.67,
      "grad_norm": 0.0053625814616680145,
      "learning_rate": 0.00024424424424424426,
      "loss": 0.0009,
      "step": 878
    },
    {
      "epoch": 81.77,
      "grad_norm": 0.0047049871645867825,
      "learning_rate": 0.00024224224224224226,
      "loss": 0.0009,
      "step": 879
    },
    {
      "epoch": 81.86,
      "grad_norm": 0.005122072994709015,
      "learning_rate": 0.00024024024024024023,
      "loss": 0.0009,
      "step": 880
    },
    {
      "epoch": 81.95,
      "grad_norm": 0.005123209673911333,
      "learning_rate": 0.00023823823823823824,
      "loss": 0.0009,
      "step": 881
    },
    {
      "epoch": 82.05,
      "grad_norm": 0.00432162918150425,
      "learning_rate": 0.00023623623623623624,
      "loss": 0.0009,
      "step": 882
    },
    {
      "epoch": 82.14,
      "grad_norm": 0.004765363875776529,
      "learning_rate": 0.00023423423423423424,
      "loss": 0.0009,
      "step": 883
    },
    {
      "epoch": 82.23,
      "grad_norm": 0.004959010053426027,
      "learning_rate": 0.00023223223223223225,
      "loss": 0.0009,
      "step": 884
    },
    {
      "epoch": 82.33,
      "grad_norm": 0.005064112599939108,
      "learning_rate": 0.00023023023023023025,
      "loss": 0.0009,
      "step": 885
    },
    {
      "epoch": 82.42,
      "grad_norm": 0.006142341997474432,
      "learning_rate": 0.00022822822822822822,
      "loss": 0.0009,
      "step": 886
    },
    {
      "epoch": 82.51,
      "grad_norm": 0.0046863132156431675,
      "learning_rate": 0.00022622622622622623,
      "loss": 0.0009,
      "step": 887
    },
    {
      "epoch": 82.6,
      "grad_norm": 0.00540094543248415,
      "learning_rate": 0.00022422422422422423,
      "loss": 0.0009,
      "step": 888
    },
    {
      "epoch": 82.7,
      "grad_norm": 0.004909657873213291,
      "learning_rate": 0.0002222222222222222,
      "loss": 0.0009,
      "step": 889
    },
    {
      "epoch": 82.79,
      "grad_norm": 0.004822991788387299,
      "learning_rate": 0.00022022022022022024,
      "loss": 0.0009,
      "step": 890
    },
    {
      "epoch": 82.88,
      "grad_norm": 0.004753682296723127,
      "learning_rate": 0.0002182182182182182,
      "loss": 0.0009,
      "step": 891
    },
    {
      "epoch": 82.98,
      "grad_norm": 0.005664428696036339,
      "learning_rate": 0.00021621621621621624,
      "loss": 0.0009,
      "step": 892
    },
    {
      "epoch": 83.07,
      "grad_norm": 0.005224739201366901,
      "learning_rate": 0.00021421421421421422,
      "loss": 0.0009,
      "step": 893
    },
    {
      "epoch": 83.16,
      "grad_norm": 0.005606709513813257,
      "learning_rate": 0.00021221221221221222,
      "loss": 0.0009,
      "step": 894
    },
    {
      "epoch": 83.26,
      "grad_norm": 0.0056505161337554455,
      "learning_rate": 0.00021021021021021022,
      "loss": 0.0009,
      "step": 895
    },
    {
      "epoch": 83.35,
      "grad_norm": 0.004067895468324423,
      "learning_rate": 0.0002082082082082082,
      "loss": 0.0009,
      "step": 896
    },
    {
      "epoch": 83.44,
      "grad_norm": 0.005673372186720371,
      "learning_rate": 0.0002062062062062062,
      "loss": 0.0009,
      "step": 897
    },
    {
      "epoch": 83.53,
      "grad_norm": 0.004777818918228149,
      "learning_rate": 0.0002042042042042042,
      "loss": 0.0009,
      "step": 898
    },
    {
      "epoch": 83.63,
      "grad_norm": 0.0042342375963926315,
      "learning_rate": 0.0002022022022022022,
      "loss": 0.0009,
      "step": 899
    },
    {
      "epoch": 83.72,
      "grad_norm": 0.005134632810950279,
      "learning_rate": 0.0002002002002002002,
      "loss": 0.0009,
      "step": 900
    },
    {
      "epoch": 83.81,
      "grad_norm": 0.0050768135115504265,
      "learning_rate": 0.0001981981981981982,
      "loss": 0.0009,
      "step": 901
    },
    {
      "epoch": 83.91,
      "grad_norm": 0.004603811539709568,
      "learning_rate": 0.0001961961961961962,
      "loss": 0.0009,
      "step": 902
    },
    {
      "epoch": 84.0,
      "grad_norm": 0.006010351236909628,
      "learning_rate": 0.00019419419419419422,
      "loss": 0.0009,
      "step": 903
    },
    {
      "epoch": 84.09,
      "grad_norm": 0.004712512716650963,
      "learning_rate": 0.0001921921921921922,
      "loss": 0.0009,
      "step": 904
    },
    {
      "epoch": 84.19,
      "grad_norm": 0.0047902921214699745,
      "learning_rate": 0.00019019019019019017,
      "loss": 0.0009,
      "step": 905
    },
    {
      "epoch": 84.28,
      "grad_norm": 0.006162314210087061,
      "learning_rate": 0.0001881881881881882,
      "loss": 0.0009,
      "step": 906
    },
    {
      "epoch": 84.37,
      "grad_norm": 0.0053647770546376705,
      "learning_rate": 0.00018618618618618617,
      "loss": 0.0009,
      "step": 907
    },
    {
      "epoch": 84.47,
      "grad_norm": 0.004498952999711037,
      "learning_rate": 0.0001841841841841842,
      "loss": 0.0009,
      "step": 908
    },
    {
      "epoch": 84.56,
      "grad_norm": 0.004967319779098034,
      "learning_rate": 0.00018218218218218218,
      "loss": 0.0009,
      "step": 909
    },
    {
      "epoch": 84.65,
      "grad_norm": 0.005319822113960981,
      "learning_rate": 0.00018018018018018018,
      "loss": 0.0009,
      "step": 910
    },
    {
      "epoch": 84.74,
      "grad_norm": 0.005924291908740997,
      "learning_rate": 0.00017817817817817819,
      "loss": 0.0009,
      "step": 911
    },
    {
      "epoch": 84.84,
      "grad_norm": 0.006012198980897665,
      "learning_rate": 0.0001761761761761762,
      "loss": 0.0009,
      "step": 912
    },
    {
      "epoch": 84.93,
      "grad_norm": 0.005654493346810341,
      "learning_rate": 0.00017417417417417416,
      "loss": 0.0009,
      "step": 913
    },
    {
      "epoch": 85.02,
      "grad_norm": 0.005291327368468046,
      "learning_rate": 0.0001721721721721722,
      "loss": 0.0009,
      "step": 914
    },
    {
      "epoch": 85.12,
      "grad_norm": 0.005491611547768116,
      "learning_rate": 0.00017017017017017017,
      "loss": 0.0009,
      "step": 915
    },
    {
      "epoch": 85.21,
      "grad_norm": 0.005493619944900274,
      "learning_rate": 0.00016816816816816817,
      "loss": 0.0009,
      "step": 916
    },
    {
      "epoch": 85.3,
      "grad_norm": 0.005633119493722916,
      "learning_rate": 0.00016616616616616618,
      "loss": 0.0009,
      "step": 917
    },
    {
      "epoch": 85.4,
      "grad_norm": 0.005285446532070637,
      "learning_rate": 0.00016416416416416415,
      "loss": 0.0009,
      "step": 918
    },
    {
      "epoch": 85.49,
      "grad_norm": 0.005487255286425352,
      "learning_rate": 0.00016216216216216218,
      "loss": 0.0009,
      "step": 919
    },
    {
      "epoch": 85.58,
      "grad_norm": 0.005247251596301794,
      "learning_rate": 0.00016016016016016016,
      "loss": 0.0009,
      "step": 920
    },
    {
      "epoch": 85.67,
      "grad_norm": 0.005201181396842003,
      "learning_rate": 0.00015815815815815816,
      "loss": 0.0009,
      "step": 921
    },
    {
      "epoch": 85.77,
      "grad_norm": 0.006415599025785923,
      "learning_rate": 0.00015615615615615616,
      "loss": 0.0009,
      "step": 922
    },
    {
      "epoch": 85.86,
      "grad_norm": 0.004794733133167028,
      "learning_rate": 0.00015415415415415416,
      "loss": 0.0009,
      "step": 923
    },
    {
      "epoch": 85.95,
      "grad_norm": 0.005826341453939676,
      "learning_rate": 0.00015215215215215217,
      "loss": 0.0009,
      "step": 924
    },
    {
      "epoch": 86.05,
      "grad_norm": 0.006257457658648491,
      "learning_rate": 0.00015015015015015014,
      "loss": 0.0009,
      "step": 925
    },
    {
      "epoch": 86.14,
      "grad_norm": 0.005907529965043068,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.0009,
      "step": 926
    },
    {
      "epoch": 86.23,
      "grad_norm": 0.004965157713741064,
      "learning_rate": 0.00014614614614614615,
      "loss": 0.0009,
      "step": 927
    },
    {
      "epoch": 86.33,
      "grad_norm": 0.004294954240322113,
      "learning_rate": 0.00014414414414414415,
      "loss": 0.0009,
      "step": 928
    },
    {
      "epoch": 86.42,
      "grad_norm": 0.005463412031531334,
      "learning_rate": 0.00014214214214214213,
      "loss": 0.0009,
      "step": 929
    },
    {
      "epoch": 86.51,
      "grad_norm": 0.0046057021245360374,
      "learning_rate": 0.00014014014014014016,
      "loss": 0.0009,
      "step": 930
    },
    {
      "epoch": 86.6,
      "grad_norm": 0.006264116149395704,
      "learning_rate": 0.00013813813813813813,
      "loss": 0.0009,
      "step": 931
    },
    {
      "epoch": 86.7,
      "grad_norm": 0.004772479180246592,
      "learning_rate": 0.00013613613613613616,
      "loss": 0.0009,
      "step": 932
    },
    {
      "epoch": 86.79,
      "grad_norm": 0.005987710319459438,
      "learning_rate": 0.00013413413413413414,
      "loss": 0.0009,
      "step": 933
    },
    {
      "epoch": 86.88,
      "grad_norm": 0.006432294379919767,
      "learning_rate": 0.00013213213213213211,
      "loss": 0.0009,
      "step": 934
    },
    {
      "epoch": 86.98,
      "grad_norm": 0.004743521101772785,
      "learning_rate": 0.00013013013013013014,
      "loss": 0.0009,
      "step": 935
    },
    {
      "epoch": 87.07,
      "grad_norm": 0.005615281872451305,
      "learning_rate": 0.00012812812812812812,
      "loss": 0.0009,
      "step": 936
    },
    {
      "epoch": 87.16,
      "grad_norm": 0.005134435836225748,
      "learning_rate": 0.00012612612612612612,
      "loss": 0.0009,
      "step": 937
    },
    {
      "epoch": 87.26,
      "grad_norm": 0.0045804292894899845,
      "learning_rate": 0.00012412412412412413,
      "loss": 0.0009,
      "step": 938
    },
    {
      "epoch": 87.35,
      "grad_norm": 0.005011300556361675,
      "learning_rate": 0.00012212212212212213,
      "loss": 0.0009,
      "step": 939
    },
    {
      "epoch": 87.44,
      "grad_norm": 0.005737390369176865,
      "learning_rate": 0.00012012012012012012,
      "loss": 0.0009,
      "step": 940
    },
    {
      "epoch": 87.53,
      "grad_norm": 0.004861002787947655,
      "learning_rate": 0.00011811811811811812,
      "loss": 0.0009,
      "step": 941
    },
    {
      "epoch": 87.63,
      "grad_norm": 0.005671943537890911,
      "learning_rate": 0.00011611611611611612,
      "loss": 0.0009,
      "step": 942
    },
    {
      "epoch": 87.72,
      "grad_norm": 0.00483910134062171,
      "learning_rate": 0.00011411411411411411,
      "loss": 0.0009,
      "step": 943
    },
    {
      "epoch": 87.81,
      "grad_norm": 0.00649560522288084,
      "learning_rate": 0.00011211211211211212,
      "loss": 0.0009,
      "step": 944
    },
    {
      "epoch": 87.91,
      "grad_norm": 0.004792740568518639,
      "learning_rate": 0.00011011011011011012,
      "loss": 0.0009,
      "step": 945
    },
    {
      "epoch": 88.0,
      "grad_norm": 0.004884125664830208,
      "learning_rate": 0.00010810810810810812,
      "loss": 0.0009,
      "step": 946
    },
    {
      "epoch": 88.09,
      "grad_norm": 0.006317455787211657,
      "learning_rate": 0.00010610610610610611,
      "loss": 0.0009,
      "step": 947
    },
    {
      "epoch": 88.19,
      "grad_norm": 0.006164257880300283,
      "learning_rate": 0.0001041041041041041,
      "loss": 0.0009,
      "step": 948
    },
    {
      "epoch": 88.28,
      "grad_norm": 0.004466003272682428,
      "learning_rate": 0.0001021021021021021,
      "loss": 0.0009,
      "step": 949
    },
    {
      "epoch": 88.37,
      "grad_norm": 0.004721886478364468,
      "learning_rate": 0.0001001001001001001,
      "loss": 0.0009,
      "step": 950
    },
    {
      "epoch": 88.47,
      "grad_norm": 0.005947052966803312,
      "learning_rate": 9.80980980980981e-05,
      "loss": 0.0009,
      "step": 951
    },
    {
      "epoch": 88.56,
      "grad_norm": 0.00461878115311265,
      "learning_rate": 9.60960960960961e-05,
      "loss": 0.0009,
      "step": 952
    },
    {
      "epoch": 88.65,
      "grad_norm": 0.005542756989598274,
      "learning_rate": 9.40940940940941e-05,
      "loss": 0.0009,
      "step": 953
    },
    {
      "epoch": 88.74,
      "grad_norm": 0.005270383786410093,
      "learning_rate": 9.20920920920921e-05,
      "loss": 0.0009,
      "step": 954
    },
    {
      "epoch": 88.84,
      "grad_norm": 0.004892627242952585,
      "learning_rate": 9.009009009009009e-05,
      "loss": 0.0009,
      "step": 955
    },
    {
      "epoch": 88.93,
      "grad_norm": 0.004252477549016476,
      "learning_rate": 8.80880880880881e-05,
      "loss": 0.0009,
      "step": 956
    },
    {
      "epoch": 89.02,
      "grad_norm": 0.005125406198203564,
      "learning_rate": 8.60860860860861e-05,
      "loss": 0.0009,
      "step": 957
    },
    {
      "epoch": 89.12,
      "grad_norm": 0.006224116776138544,
      "learning_rate": 8.408408408408409e-05,
      "loss": 0.0009,
      "step": 958
    },
    {
      "epoch": 89.21,
      "grad_norm": 0.004906150978058577,
      "learning_rate": 8.208208208208208e-05,
      "loss": 0.0009,
      "step": 959
    },
    {
      "epoch": 89.3,
      "grad_norm": 0.004579299129545689,
      "learning_rate": 8.008008008008008e-05,
      "loss": 0.0009,
      "step": 960
    },
    {
      "epoch": 89.4,
      "grad_norm": 0.005291109438985586,
      "learning_rate": 7.807807807807808e-05,
      "loss": 0.0009,
      "step": 961
    },
    {
      "epoch": 89.49,
      "grad_norm": 0.005488655064254999,
      "learning_rate": 7.607607607607608e-05,
      "loss": 0.0009,
      "step": 962
    },
    {
      "epoch": 89.58,
      "grad_norm": 0.004119394347071648,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.0009,
      "step": 963
    },
    {
      "epoch": 89.67,
      "grad_norm": 0.005245835054665804,
      "learning_rate": 7.207207207207208e-05,
      "loss": 0.0009,
      "step": 964
    },
    {
      "epoch": 89.77,
      "grad_norm": 0.005400402937084436,
      "learning_rate": 7.007007007007008e-05,
      "loss": 0.0009,
      "step": 965
    },
    {
      "epoch": 89.86,
      "grad_norm": 0.005284850485622883,
      "learning_rate": 6.806806806806808e-05,
      "loss": 0.0009,
      "step": 966
    },
    {
      "epoch": 89.95,
      "grad_norm": 0.005430346820503473,
      "learning_rate": 6.606606606606606e-05,
      "loss": 0.0009,
      "step": 967
    },
    {
      "epoch": 90.05,
      "grad_norm": 0.006896638777107,
      "learning_rate": 6.406406406406406e-05,
      "loss": 0.0009,
      "step": 968
    },
    {
      "epoch": 90.14,
      "grad_norm": 0.005804664455354214,
      "learning_rate": 6.206206206206206e-05,
      "loss": 0.0009,
      "step": 969
    },
    {
      "epoch": 90.23,
      "grad_norm": 0.00458492711186409,
      "learning_rate": 6.006006006006006e-05,
      "loss": 0.0009,
      "step": 970
    },
    {
      "epoch": 90.33,
      "grad_norm": 0.00411634286865592,
      "learning_rate": 5.805805805805806e-05,
      "loss": 0.0009,
      "step": 971
    },
    {
      "epoch": 90.42,
      "grad_norm": 0.004750021733343601,
      "learning_rate": 5.605605605605606e-05,
      "loss": 0.0009,
      "step": 972
    },
    {
      "epoch": 90.51,
      "grad_norm": 0.005801167339086533,
      "learning_rate": 5.405405405405406e-05,
      "loss": 0.0009,
      "step": 973
    },
    {
      "epoch": 90.6,
      "grad_norm": 0.004878166131675243,
      "learning_rate": 5.205205205205205e-05,
      "loss": 0.0009,
      "step": 974
    },
    {
      "epoch": 90.7,
      "grad_norm": 0.005457410588860512,
      "learning_rate": 5.005005005005005e-05,
      "loss": 0.0009,
      "step": 975
    },
    {
      "epoch": 90.79,
      "grad_norm": 0.00535906758159399,
      "learning_rate": 4.804804804804805e-05,
      "loss": 0.0009,
      "step": 976
    },
    {
      "epoch": 90.88,
      "grad_norm": 0.005218366626650095,
      "learning_rate": 4.604604604604605e-05,
      "loss": 0.0009,
      "step": 977
    },
    {
      "epoch": 90.98,
      "grad_norm": 0.005457831546664238,
      "learning_rate": 4.404404404404405e-05,
      "loss": 0.0009,
      "step": 978
    },
    {
      "epoch": 91.07,
      "grad_norm": 0.0045090666972100735,
      "learning_rate": 4.204204204204204e-05,
      "loss": 0.0009,
      "step": 979
    },
    {
      "epoch": 91.16,
      "grad_norm": 0.004378278274089098,
      "learning_rate": 4.004004004004004e-05,
      "loss": 0.0009,
      "step": 980
    },
    {
      "epoch": 91.26,
      "grad_norm": 0.006145849823951721,
      "learning_rate": 3.803803803803804e-05,
      "loss": 0.0009,
      "step": 981
    },
    {
      "epoch": 91.35,
      "grad_norm": 0.005175464786589146,
      "learning_rate": 3.603603603603604e-05,
      "loss": 0.0009,
      "step": 982
    },
    {
      "epoch": 91.44,
      "grad_norm": 0.0053061991930007935,
      "learning_rate": 3.403403403403404e-05,
      "loss": 0.0009,
      "step": 983
    },
    {
      "epoch": 91.53,
      "grad_norm": 0.004651503637433052,
      "learning_rate": 3.203203203203203e-05,
      "loss": 0.0009,
      "step": 984
    },
    {
      "epoch": 91.63,
      "grad_norm": 0.005121590569615364,
      "learning_rate": 3.003003003003003e-05,
      "loss": 0.0009,
      "step": 985
    },
    {
      "epoch": 91.72,
      "grad_norm": 0.005293362773954868,
      "learning_rate": 2.802802802802803e-05,
      "loss": 0.0009,
      "step": 986
    },
    {
      "epoch": 91.81,
      "grad_norm": 0.005477495025843382,
      "learning_rate": 2.6026026026026025e-05,
      "loss": 0.0009,
      "step": 987
    },
    {
      "epoch": 91.91,
      "grad_norm": 0.004816056694835424,
      "learning_rate": 2.4024024024024024e-05,
      "loss": 0.0009,
      "step": 988
    },
    {
      "epoch": 92.0,
      "grad_norm": 0.004626130219548941,
      "learning_rate": 2.2022022022022024e-05,
      "loss": 0.0009,
      "step": 989
    },
    {
      "epoch": 92.09,
      "grad_norm": 0.0056447554379701614,
      "learning_rate": 2.002002002002002e-05,
      "loss": 0.0009,
      "step": 990
    },
    {
      "epoch": 92.19,
      "grad_norm": 0.004338358994573355,
      "learning_rate": 1.801801801801802e-05,
      "loss": 0.0009,
      "step": 991
    },
    {
      "epoch": 92.28,
      "grad_norm": 0.004959416575729847,
      "learning_rate": 1.6016016016016015e-05,
      "loss": 0.0009,
      "step": 992
    },
    {
      "epoch": 92.37,
      "grad_norm": 0.004483124241232872,
      "learning_rate": 1.4014014014014014e-05,
      "loss": 0.0009,
      "step": 993
    },
    {
      "epoch": 92.47,
      "grad_norm": 0.005554167553782463,
      "learning_rate": 1.2012012012012012e-05,
      "loss": 0.0009,
      "step": 994
    },
    {
      "epoch": 92.56,
      "grad_norm": 0.004885959438979626,
      "learning_rate": 1.001001001001001e-05,
      "loss": 0.0009,
      "step": 995
    },
    {
      "epoch": 92.65,
      "grad_norm": 0.0045693181455135345,
      "learning_rate": 8.008008008008007e-06,
      "loss": 0.0009,
      "step": 996
    },
    {
      "epoch": 92.74,
      "grad_norm": 0.005001838784664869,
      "learning_rate": 6.006006006006006e-06,
      "loss": 0.0009,
      "step": 997
    },
    {
      "epoch": 92.84,
      "grad_norm": 0.0038486875128000975,
      "learning_rate": 4.004004004004004e-06,
      "loss": 0.0009,
      "step": 998
    },
    {
      "epoch": 92.93,
      "grad_norm": 0.005863032303750515,
      "learning_rate": 2.002002002002002e-06,
      "loss": 0.0009,
      "step": 999
    },
    {
      "epoch": 93.02,
      "grad_norm": 0.005194799043238163,
      "learning_rate": 0.0,
      "loss": 0.0009,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 100,
  "save_steps": 500,
  "total_flos": 2.605287815372759e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
